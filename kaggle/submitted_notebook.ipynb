{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de03ec7",
   "metadata": {},
   "source": [
    "# Mattia Sarti's Notebook\n",
    "### The following source code is illustrated [here](https://github.com/MattiaSarti/yolo-to-help-protect-the-great-barrier-reef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eff60b",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ac694",
   "metadata": {},
   "outputs": [],
   "source": [
    "! if [ -d \"/kaggle/working/cache\" ]; then rm -r /kaggle/working/cache; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fad6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir /kaggle/working/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee991028",
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__ = 'main_by_mattia'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494bd615",
   "metadata": {},
   "source": [
    "#### Common constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c70c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convenient definitions of common constants.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from numpy import arange, meshgrid, ndarray, stack\n",
    "# pylint: disable=import-error,no-name-in-module\n",
    "from tensorflow import float32 as tf_float32, uint8 as tf_uint8\n",
    "# pylint: enable=import-error,no-name-in-module\n",
    "\n",
    "\n",
    "def compute_grid_cell_centers_xy_coords() -> Tuple[ndarray, ndarray]:\n",
    "    \"\"\"\n",
    "    Return two 3D arrays respectively representing the output grid cell\n",
    "    centers' (x, y) coordinates and top-left corners' (x, y) coordinates,\n",
    "    indexed along the first two dimensions as rows and columns of cells in the\n",
    "    output grid.\n",
    "    ---\n",
    "        Outputs' Shapes:\n",
    "            - (OUTPUT_GRID_N_ROWS, OUTPUT_GRID_N_COLUMNS, 2)\n",
    "            - (OUTPUT_GRID_N_ROWS, OUTPUT_GRID_N_COLUMNS, 2)\n",
    "    ---\n",
    "        Outputs' Meanings:\n",
    "            - the first dimension is the row index of the grid cell and the\n",
    "            second dimension is the column index of the grid cell, while the\n",
    "            third dimension represents the tuple of center (x, y) coordinates\n",
    "            of the considered grid cell\n",
    "            - the first dimension is the row index of the grid cell and the\n",
    "            second dimension is the column index of the grid cell, while the\n",
    "            third dimension represents the tuple of top-left corner (x, y)\n",
    "            coordinates of the considered grid cell\n",
    "    \"\"\"\n",
    "    # x and y possible values spanned by grid cell centers:\n",
    "    centers_x_coords_values = arange(\n",
    "        start=int(OUTPUT_GRID_CELL_N_COLUMNS / 2),\n",
    "        stop=IMAGE_N_COLUMNS,\n",
    "        step=OUTPUT_GRID_CELL_N_COLUMNS\n",
    "    )\n",
    "    assert centers_x_coords_values.shape == (OUTPUT_GRID_N_COLUMNS,)\n",
    "    centers_y_coords_values = arange(\n",
    "        start=int(OUTPUT_GRID_CELL_N_ROWS / 2),\n",
    "        stop=IMAGE_N_ROWS,\n",
    "        step=OUTPUT_GRID_CELL_N_ROWS\n",
    "    )\n",
    "    assert centers_y_coords_values.shape == (OUTPUT_GRID_N_ROWS,)\n",
    "\n",
    "    # x and y possible values spanned by grid cell top-left corners:\n",
    "    corners_x_coords_values = arange(\n",
    "        start=0,\n",
    "        stop=IMAGE_N_COLUMNS,\n",
    "        step=OUTPUT_GRID_CELL_N_COLUMNS\n",
    "    )\n",
    "    assert corners_x_coords_values.shape == (OUTPUT_GRID_N_COLUMNS,)\n",
    "    corners_y_coords_values = arange(\n",
    "        start=0,\n",
    "        stop=IMAGE_N_ROWS,\n",
    "        step=OUTPUT_GRID_CELL_N_ROWS\n",
    "    )\n",
    "    assert corners_y_coords_values.shape == (OUTPUT_GRID_N_ROWS,)\n",
    "\n",
    "    # grid of cells containing the respective center x and y coordinates each:\n",
    "    centers_xy_coords = stack(\n",
    "        arrays=meshgrid(centers_x_coords_values, centers_y_coords_values),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    # grid of cells containing the respective top-left corner x and y\n",
    "    # coordinates each:\n",
    "    corners_xy_coords = stack(\n",
    "        arrays=meshgrid(corners_x_coords_values, corners_y_coords_values),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        centers_xy_coords,\n",
    "        corners_xy_coords\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_weights_to_balance_anchors_emptiness() -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Return the weights, for the loss function terms, that balance full vs\n",
    "    empty anchors.\n",
    "    \"\"\"\n",
    "    average_n_full_anchors_per_image = (\n",
    "        AVERAGE_N_BOUNDING_BOXES_PER_IMAGE / N_ANCHORS_PER_IMAGE\n",
    "    )\n",
    "    average_n_empty_anchors_per_image = (\n",
    "        N_ANCHORS_PER_IMAGE - average_n_full_anchors_per_image\n",
    "    )\n",
    "\n",
    "    full_anchors_weight = 1 / average_n_full_anchors_per_image\n",
    "    empty_anchors_weight = 1 / average_n_empty_anchors_per_image\n",
    "\n",
    "    weights_sum = full_anchors_weight + empty_anchors_weight\n",
    "\n",
    "    normalized_full_anchors_weight = full_anchors_weight / weights_sum\n",
    "    normalized_empty_anchors_weight = empty_anchors_weight / weights_sum\n",
    "\n",
    "    return normalized_full_anchors_weight, normalized_empty_anchors_weight\n",
    "\n",
    "\n",
    "AVERAGE_N_BOUNDING_BOXES_PER_IMAGE = 0.51\n",
    "\n",
    "DATA_TYPE_FOR_INPUTS = tf_uint8\n",
    "DATA_TYPE_FOR_OUTPUTS = tf_float32\n",
    "\n",
    "DOWNSAMPLING_STEPS = 4\n",
    "\n",
    "IMAGE_N_CHANNELS = 3\n",
    "IMAGE_N_COLUMNS = 1280\n",
    "IMAGE_N_ROWS = 720\n",
    "\n",
    "# MINIMUM_BOUNDING_BOX_HEIGHT = 13  # [pixels]\n",
    "# MINIMUM_BOUNDING_BOX_WIDTH = 17  # [pixels]\n",
    "\n",
    "N_OUTPUTS_PER_ANCHOR = 5\n",
    "\n",
    "ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS = (\n",
    "    (0.6, 0.4),\n",
    "    (0.5, 0.5),\n",
    "    # (0.4, 0.6)  # NOTE: empirically observed: this anchor is less relevant\n",
    ")\n",
    "assert all(\n",
    "    [\n",
    "        (weight[0] + weight[1] == 1) for weight in\n",
    "        ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS\n",
    "    ]\n",
    ")\n",
    "N_ANCHORS_PER_CELL = len(\n",
    "    ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS\n",
    ")\n",
    "\n",
    "OUTPUT_GRID_CELL_N_COLUMNS = 16  # NOTE: this may vary with the architecture\n",
    "OUTPUT_GRID_CELL_N_ROWS = 16  # NOTE: this may vary with the architecture\n",
    "# NOTE: common divisors of 1280 and 720: {1, 2, 4, 5, 8, 10, 16, 20, 40, 80},\n",
    "# and the ones that respect the training-plus-validation set bounding boxes'\n",
    "# distinction when using a single anchor are: {1, 2, 4, 5, 8, 10, 16}\n",
    "\n",
    "OUTPUT_GRID_N_COLUMNS = int(IMAGE_N_COLUMNS / OUTPUT_GRID_CELL_N_COLUMNS)\n",
    "OUTPUT_GRID_N_ROWS = int(IMAGE_N_ROWS / OUTPUT_GRID_CELL_N_ROWS)\n",
    "\n",
    "N_ANCHORS_PER_IMAGE = (\n",
    "    OUTPUT_GRID_N_COLUMNS * OUTPUT_GRID_N_ROWS * N_ANCHORS_PER_CELL\n",
    ")\n",
    "\n",
    "(\n",
    "    OUTPUT_GRID_CELL_CENTERS_XY_COORDS,\n",
    "    OUTPUT_GRID_CELL_CORNERS_XY_COORDS\n",
    ") = compute_grid_cell_centers_xy_coords()\n",
    "\n",
    "(\n",
    "    LOSS_CONTRIBUTE_IMPORTANCE_OF_FULL_ANCHORS,\n",
    "    LOSS_CONTRIBUTE_IMPORTANCE_OF_EMPTY_ANCHORS\n",
    ") = compute_weights_to_balance_anchors_emptiness()\n",
    "# FIXME: is this balancing reasonable?  with 0.999999990162037 vs\n",
    "# 9.837962962962963e-09, using float32 will truncate the second term to 0!!\n",
    "(\n",
    "    LOSS_CONTRIBUTE_IMPORTANCE_OF_FULL_ANCHORS,\n",
    "    LOSS_CONTRIBUTE_IMPORTANCE_OF_EMPTY_ANCHORS\n",
    ") = (0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6ed1a",
   "metadata": {},
   "source": [
    "#### Samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pylint: disable=too-many-lines\n",
    "\"\"\"\n",
    "Sample and label extraction from the raw dataset files, inspection and\n",
    "preprocessing for feeding the model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from csv import reader as csv_reader\n",
    "from itertools import combinations\n",
    "from json import loads as json_loads\n",
    "from math import sqrt\n",
    "from os import getcwd, pardir\n",
    "from os.path import join as path_join\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.pyplot import (\n",
    "    clf as plt_clf,\n",
    "    close as plt_close,\n",
    "    figure as plt_figure,\n",
    "    hist as plt_hist,\n",
    "    get_current_fig_manager,\n",
    "    pause as plt_pause,\n",
    "    savefig as plt_savefig,\n",
    "    show as plt_show,\n",
    "    subplots,\n",
    "    title as plt_title,\n",
    "    xticks as plt_xticks\n",
    ")\n",
    "from numpy import argmin, sum as np_sum, unravel_index, zeros\n",
    "# pylint: disable=import-error,no-name-in-module\n",
    "from tensorflow import convert_to_tensor, py_function, Tensor\n",
    "from tensorflow.data import AUTOTUNE, Dataset\n",
    "from tensorflow.io import decode_jpeg, read_file\n",
    "# pylint: enable=import-error,no-name-in-module\n",
    "\n",
    "# only when running everything in a unified notebook on Kaggle's servers:\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import (\n",
    "        ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS,\n",
    "        DATA_TYPE_FOR_INPUTS,\n",
    "        DATA_TYPE_FOR_OUTPUTS,\n",
    "        IMAGE_N_COLUMNS,\n",
    "        IMAGE_N_ROWS,\n",
    "        N_ANCHORS_PER_CELL,\n",
    "        N_OUTPUTS_PER_ANCHOR,\n",
    "        OUTPUT_GRID_CELL_CENTERS_XY_COORDS,\n",
    "        OUTPUT_GRID_CELL_CORNERS_XY_COORDS,\n",
    "        OUTPUT_GRID_CELL_N_COLUMNS,\n",
    "        OUTPUT_GRID_CELL_N_ROWS,\n",
    "        OUTPUT_GRID_N_COLUMNS,\n",
    "        OUTPUT_GRID_N_ROWS\n",
    "    )\n",
    "\n",
    "\n",
    "MINI_BATCH_SIZE = 8  # TODO\n",
    "VALIDATION_SET_PORTION_OF_DATA = 0.1  # 0.3\n",
    "\n",
    "# only when running everything in a unified notebook on Kaggle's servers:\n",
    "if __name__ != 'main_by_mattia':\n",
    "    DATASET_DIR = path_join(\n",
    "        getcwd(),\n",
    "        pardir,\n",
    "        'tensorflow-great-barrier-reef'\n",
    "    )\n",
    "else:\n",
    "    DATASET_DIR = path_join(\n",
    "        getcwd(),\n",
    "        pardir,\n",
    "        'input',\n",
    "        'tensorflow-great-barrier-reef'\n",
    "    )\n",
    "CACHE_DIR = path_join(\n",
    "    getcwd(),\n",
    "    'cache'\n",
    ")\n",
    "CACHE_FILE_PATH_FOR_STATISTICS_SET = path_join(\n",
    "    CACHE_DIR,\n",
    "    'statistics.tmp'\n",
    ")\n",
    "CACHE_FILE_PATH_FOR_TRAINING_SET = path_join(\n",
    "    CACHE_DIR,\n",
    "    'training.tmp'\n",
    ")\n",
    "CACHE_FILE_PATH_FOR_VALIDATION_SET = path_join(\n",
    "    CACHE_DIR,\n",
    "    'validation.tmp'\n",
    ")\n",
    "LABELS_FILE_PATH = path_join(\n",
    "    DATASET_DIR,\n",
    "    'train.csv'\n",
    ")\n",
    "PICTURES_DIR = path_join(\n",
    "    getcwd(),\n",
    "    pardir,\n",
    "    'docs',\n",
    "    'pictures'\n",
    ")\n",
    "\n",
    "SHOW_BOUNDING_BOXES_STATISTICS = False\n",
    "SHOW_DATASET_MOVIES = False\n",
    "\n",
    "\n",
    "def get_cell_containing_bounding_box_center(\n",
    "        center_absolute_x_and_y_coords: Tuple[float, float]\n",
    ") -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Find the output grid cell whose center is closest to the bounding box one\n",
    "    (the input one), returning the grid cell's row and column indexes and its\n",
    "    top-left corner x and y coordinates.\n",
    "    ---\n",
    "        Output Shape:\n",
    "            - (4,)\n",
    "    ---\n",
    "        Output Meaning:\n",
    "            - [\n",
    "                grid cell row index,\n",
    "                grid cell column index,\n",
    "                x coordindate of cell top-left corner,\n",
    "                y coordindate of cell top-left corner\n",
    "            ]\n",
    "    \"\"\"\n",
    "    (  # pylint: disable=unbalanced-tuple-unpacking\n",
    "        grid_cell_enclosing_bounding_box_center_row_index,\n",
    "        grid_cell_enclosing_bounding_box_center_column_index\n",
    "    ) = unravel_index(\n",
    "        indices=argmin(\n",
    "            # NOTE: in case of equivalent minima, the first one is picked grid\n",
    "            # of squared element-wise center pairs' distances representing the\n",
    "            # minimized objective to find the closest grid cell center:\n",
    "            a=np_sum(\n",
    "                a=(\n",
    "                    (OUTPUT_GRID_CELL_CENTERS_XY_COORDS -\n",
    "                     center_absolute_x_and_y_coords) ** 2\n",
    "                ),\n",
    "                axis=-1\n",
    "            )\n",
    "        ),\n",
    "        shape=(OUTPUT_GRID_N_ROWS, OUTPUT_GRID_N_COLUMNS),\n",
    "        order='C'\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        # [grid cell row index, grid cell column index]:\n",
    "        [\n",
    "            grid_cell_enclosing_bounding_box_center_row_index,\n",
    "            grid_cell_enclosing_bounding_box_center_column_index\n",
    "        ] +\n",
    "        # [x coordindate of cell corner, y coordindate of cell corner]:\n",
    "        OUTPUT_GRID_CELL_CORNERS_XY_COORDS[\n",
    "            grid_cell_enclosing_bounding_box_center_row_index,\n",
    "            grid_cell_enclosing_bounding_box_center_column_index,\n",
    "            :\n",
    "        ].tolist()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_index_of_anchor_with_closest_aspect_ratio(\n",
    "        absolute_width: float,\n",
    "        absolute_height: float\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Return the index of the anchor whose aspect ratio is close to the\n",
    "    considered bounding box represented by the input relative width and\n",
    "    relative height.\n",
    "    \"\"\"\n",
    "    width_weight = absolute_width / (absolute_width + absolute_height)\n",
    "    height_weight = absolute_height / (absolute_width + absolute_height)\n",
    "\n",
    "    return (\n",
    "        ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS.index(\n",
    "            sorted(\n",
    "                ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS,\n",
    "                key=lambda width_vs_height_weights: (\n",
    "                    abs(width_vs_height_weights[0] - width_weight) +\n",
    "                    abs(width_vs_height_weights[1] - height_weight)\n",
    "                ),\n",
    "                reverse=False\n",
    "            )[0]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def dataset_of_samples_and_bounding_boxes() -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a TensorFlow dataset that can iterate over all the dataset samples\n",
    "    and the respective labels containing bounding boxes.\n",
    "    \"\"\"\n",
    "    image_paths_dataset = Dataset.from_tensor_slices(\n",
    "        tensors=[*IMAGE_PATHS_TO_BOUNDING_BOXES]  # only keys included\n",
    "    )\n",
    "\n",
    "    image_paths_dataset = image_paths_dataset.map(\n",
    "        map_func=lambda image_path: py_function(\n",
    "            func=load_sample_and_get_bounding_boxes,\n",
    "            inp=[image_path],\n",
    "            Tout=(DATA_TYPE_FOR_INPUTS, DATA_TYPE_FOR_OUTPUTS)\n",
    "        ),\n",
    "        num_parallel_calls=AUTOTUNE,\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "    if __name__ == 'main_by_mattia':\n",
    "        # optimizing performances by caching end-results:\n",
    "        image_paths_dataset = image_paths_dataset.cache(\n",
    "            filename=CACHE_FILE_PATH_FOR_STATISTICS_SET\n",
    "        )\n",
    "\n",
    "    # optimizing performances by pre-fetching final elements:\n",
    "    image_paths_dataset = image_paths_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return image_paths_dataset\n",
    "\n",
    "\n",
    "def dataset_of_samples_and_model_outputs(shuffle: bool = True) -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a TensorFlow dataset that can iterate over all the dataset samples\n",
    "    and the respective labels containing model outputs, in a shuffled order.\n",
    "    \"\"\"\n",
    "    image_paths_dataset = Dataset.from_tensor_slices(\n",
    "        tensors=[*IMAGE_PATHS_TO_MODEL_OUTPUTS]  # only keys included\n",
    "    )\n",
    "\n",
    "    # NOTE: shuffling is carried out here to have acceptable performance with\n",
    "    # a shuffling buffer size that allows to take the whole set into memory\n",
    "    # in case shuffling is desired:\n",
    "    if shuffle:\n",
    "        image_paths_dataset = image_paths_dataset.shuffle(\n",
    "            buffer_size=N_TRAINING_PLUS_VALIDATION_SAMPLES,\n",
    "            seed=0,\n",
    "            reshuffle_each_iteration=False  # NOTE: relevant when splitting\n",
    "        )\n",
    "\n",
    "    # NOTE: further optimizations on this dataset - that is the one employed\n",
    "    # for training/validation - are carried out later, after\n",
    "    # training/validation splitting and batching, to optimize performances\n",
    "\n",
    "    return image_paths_dataset.map(\n",
    "        map_func=lambda image_path: py_function(\n",
    "            func=load_sample_and_get_model_outputs,\n",
    "            inp=[image_path],\n",
    "            Tout=(DATA_TYPE_FOR_INPUTS, DATA_TYPE_FOR_OUTPUTS)\n",
    "        ),\n",
    "        num_parallel_calls=AUTOTUNE,\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "\n",
    "def inspect_bounding_boxes_statistics_on_training_n_validation_set() -> None:  # noqa: E501 pylint: disable=too-many-locals,too-many-branches,too-many-statements\n",
    "    \"\"\"\n",
    "    Inspect and print the following statistics of bounding boxes in the\n",
    "    training-plus-validation set:\n",
    "        - total number of bounding boxes\n",
    "        - total number of images\n",
    "        - average number of bounding boxes per image\n",
    "        - minimum number of bounding boxes per image\n",
    "        - maximum number of bounding boxes per image\n",
    "        - total number of empty images\n",
    "        - average bounding box height [pixels]\n",
    "        - average bounding box width [pixels]\n",
    "        - average bounding boxes' centers distance [pixels]\n",
    "        - average bounding boxes' centers x-coord distance [pixels]\n",
    "        - average bounding boxes' centers y-coord distance [pixels]\n",
    "        - minimum bounding box height [pixels]\n",
    "        - minimum bounding box width [pixels]\n",
    "        - minimum bounding boxes' centers distance [pixels]\n",
    "        - minimum bounding boxes' centers x-coord distance [pixels]\n",
    "        - minimum bounding boxes' centers y-coord distance [pixels]\n",
    "        - maximum bounding box height [pixels]\n",
    "        - maximum bounding box width [pixels]\n",
    "        - maximum bounding boxes' centers distance [pixels]\n",
    "        - maximum bounding boxes' centers x-coord distance [pixels]\n",
    "        - maximum bounding boxes' centers y-coord distance [pixels]\n",
    "        - histogram of number of bounding boxes per image\n",
    "        - histogram of bounding boxes' centers distance [pixels]\n",
    "        - histogram of bounding boxes' centers x-coord distance [pixels]\n",
    "        - histogram of bounding boxes' centers y-coord distance [pixels]\n",
    "    \"\"\"\n",
    "    total_n_images = len(IMAGE_PATHS_TO_BOUNDING_BOXES)\n",
    "\n",
    "    bounding_boxes_centers_distances_for_histogram = []\n",
    "    bounding_boxes_centers_x_coord_distances_for_histogram = []\n",
    "    bounding_boxes_centers_y_coord_distances_for_histogram = []\n",
    "    cumulative_bounding_box_height = 0\n",
    "    cumulative_bounding_box_width = 0\n",
    "    cumulative_bounding_boxes_centers_distance = 0\n",
    "    cumulative_bounding_boxes_centers_x_coord_distance = 0\n",
    "    cumulative_bounding_boxes_centers_y_coord_distance = 0\n",
    "    minimum_bounding_box_height = 99999\n",
    "    minimum_bounding_box_width = 99999\n",
    "    minimum_bounding_boxes_centers_distance = 99999\n",
    "    minimum_bounding_boxes_centers_x_coord_distance = 99999\n",
    "    minimum_bounding_boxes_centers_y_coord_distance = 99999\n",
    "    minimum_n_bounding_boxes_per_image = 99999\n",
    "    maximum_bounding_box_height = 0\n",
    "    maximum_bounding_box_width = 0\n",
    "    maximum_bounding_boxes_centers_distance = 0\n",
    "    maximum_bounding_boxes_centers_x_coord_distance = 0\n",
    "    maximum_bounding_boxes_centers_y_coord_distance = 0\n",
    "    maximum_n_bounding_boxes_per_image = 0\n",
    "    n_bounding_boxes_per_image_for_histogram = []\n",
    "    total_n_bounding_boxes = 0\n",
    "    total_n_bounding_boxes_center_distances_cumulated = 0\n",
    "    total_n_empty_images = 0\n",
    "\n",
    "    for image_bounding_boxes in IMAGE_PATHS_TO_BOUNDING_BOXES.values():\n",
    "        n_bounding_boxes = len(image_bounding_boxes)\n",
    "        n_bounding_boxes_per_image_for_histogram.append(\n",
    "            n_bounding_boxes\n",
    "        )\n",
    "\n",
    "        total_n_bounding_boxes += n_bounding_boxes\n",
    "        if n_bounding_boxes < minimum_n_bounding_boxes_per_image:\n",
    "            minimum_n_bounding_boxes_per_image = n_bounding_boxes\n",
    "        if n_bounding_boxes > maximum_n_bounding_boxes_per_image:\n",
    "            maximum_n_bounding_boxes_per_image = n_bounding_boxes\n",
    "        if n_bounding_boxes == 0:\n",
    "            total_n_empty_images += 1\n",
    "\n",
    "        bounding_boxes_centers_x_and_y_coords = []\n",
    "        for bounding_box in image_bounding_boxes:\n",
    "            cumulative_bounding_box_height += bounding_box['height']\n",
    "            cumulative_bounding_box_width += bounding_box['width']\n",
    "\n",
    "            bounding_boxes_centers_x_and_y_coords.append(\n",
    "                {\n",
    "                    'x': (bounding_box['x'] + bounding_box['width']) / 2,\n",
    "                    'y': (bounding_box['y'] + bounding_box['height']) / 2\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if bounding_box['height'] < minimum_bounding_box_height:\n",
    "                minimum_bounding_box_height = bounding_box['height']\n",
    "            if bounding_box['width'] < minimum_bounding_box_width:\n",
    "                minimum_bounding_box_width = bounding_box['width']\n",
    "\n",
    "            if bounding_box['height'] > maximum_bounding_box_height:\n",
    "                maximum_bounding_box_height = bounding_box['height']\n",
    "            if bounding_box['width'] > maximum_bounding_box_width:\n",
    "                maximum_bounding_box_width = bounding_box['width']\n",
    "\n",
    "        if n_bounding_boxes > 1:\n",
    "            for centers_coords_pair in combinations(\n",
    "                    iterable=bounding_boxes_centers_x_and_y_coords,\n",
    "                    r=2\n",
    "            ):\n",
    "                total_n_bounding_boxes_center_distances_cumulated += 1\n",
    "\n",
    "                x_coord_difference = abs(\n",
    "                    centers_coords_pair[0]['x'] - centers_coords_pair[1]['x']\n",
    "                )\n",
    "                y_coord_difference = abs(\n",
    "                    centers_coords_pair[0]['y'] - centers_coords_pair[1]['y']\n",
    "                )\n",
    "                distance = sqrt(\n",
    "                    x_coord_difference**2 + y_coord_difference**2\n",
    "                )\n",
    "\n",
    "                bounding_boxes_centers_distances_for_histogram.append(\n",
    "                    distance\n",
    "                )\n",
    "                bounding_boxes_centers_x_coord_distances_for_histogram.append(\n",
    "                    x_coord_difference\n",
    "                )\n",
    "                bounding_boxes_centers_y_coord_distances_for_histogram.append(\n",
    "                    y_coord_difference\n",
    "                )\n",
    "\n",
    "                cumulative_bounding_boxes_centers_distance += (\n",
    "                    distance\n",
    "                )\n",
    "                cumulative_bounding_boxes_centers_x_coord_distance += (\n",
    "                    x_coord_difference\n",
    "                )\n",
    "                cumulative_bounding_boxes_centers_y_coord_distance += (\n",
    "                    y_coord_difference\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                        distance <\n",
    "                        minimum_bounding_boxes_centers_distance\n",
    "                ):\n",
    "                    minimum_bounding_boxes_centers_distance = (\n",
    "                        distance\n",
    "                    )\n",
    "                if (\n",
    "                        x_coord_difference <\n",
    "                        minimum_bounding_boxes_centers_x_coord_distance\n",
    "                ):\n",
    "                    minimum_bounding_boxes_centers_x_coord_distance = (\n",
    "                        x_coord_difference\n",
    "                    )\n",
    "                if (\n",
    "                        y_coord_difference <\n",
    "                        minimum_bounding_boxes_centers_y_coord_distance\n",
    "                ):\n",
    "                    minimum_bounding_boxes_centers_y_coord_distance = (\n",
    "                        y_coord_difference\n",
    "                    )\n",
    "\n",
    "                if (\n",
    "                        distance >\n",
    "                        maximum_bounding_boxes_centers_distance\n",
    "                ):\n",
    "                    maximum_bounding_boxes_centers_distance = (\n",
    "                        distance\n",
    "                    )\n",
    "                if (\n",
    "                        x_coord_difference >\n",
    "                        maximum_bounding_boxes_centers_x_coord_distance\n",
    "                ):\n",
    "                    maximum_bounding_boxes_centers_x_coord_distance = (\n",
    "                        x_coord_difference\n",
    "                    )\n",
    "                if (\n",
    "                        y_coord_difference > (\n",
    "                            maximum_bounding_boxes_centers_y_coord_distance\n",
    "                        )\n",
    "                ):\n",
    "                    maximum_bounding_boxes_centers_y_coord_distance = (\n",
    "                        y_coord_difference\n",
    "                    )\n",
    "\n",
    "    print('- ' * 30)\n",
    "    print(\"Bounding Boxes' Statistics:\")\n",
    "\n",
    "    print(\n",
    "        \"\\t- total number of bounding boxes:\",\n",
    "        total_n_bounding_boxes\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- total number of images:\",\n",
    "        total_n_images\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average number of bounding boxes per image:\",\n",
    "        round(number=total_n_bounding_boxes/total_n_images, ndigits=2)\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum number of bounding boxes per image:\",\n",
    "        minimum_n_bounding_boxes_per_image\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum number of bounding boxes per image:\",\n",
    "        maximum_n_bounding_boxes_per_image\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- total number of empty images:\",\n",
    "        total_n_empty_images\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding box height [pixels]:\",\n",
    "        round(\n",
    "            number=cumulative_bounding_box_height/total_n_bounding_boxes,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding box width [pixels]:\",\n",
    "        round(\n",
    "            number=cumulative_bounding_box_width/total_n_bounding_boxes,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding boxes' centers distance [pixels]:\",\n",
    "        round(\n",
    "            number=(\n",
    "                cumulative_bounding_boxes_centers_distance /\n",
    "                total_n_bounding_boxes_center_distances_cumulated\n",
    "            ),\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding boxes' centers x-coord distance [pixels]:\",\n",
    "        round(\n",
    "            number=(\n",
    "                cumulative_bounding_boxes_centers_x_coord_distance /\n",
    "                total_n_bounding_boxes_center_distances_cumulated\n",
    "            ),\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding boxes' centers y-coord distance [pixels]:\",\n",
    "        round(\n",
    "            number=(\n",
    "                cumulative_bounding_boxes_centers_y_coord_distance /\n",
    "                total_n_bounding_boxes_center_distances_cumulated\n",
    "            ),\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding box height [pixels]:\",\n",
    "        minimum_bounding_box_height\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding box width [pixels]:\",\n",
    "        minimum_bounding_box_width\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding boxes' centers distance [pixels]:\",\n",
    "        round(\n",
    "            number=minimum_bounding_boxes_centers_distance,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding boxes' centers x-coord distance [pixels]:\",\n",
    "        minimum_bounding_boxes_centers_x_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding boxes' centers y-coord distance [pixels]:\",\n",
    "        minimum_bounding_boxes_centers_y_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding box height [pixels]:\",\n",
    "        maximum_bounding_box_height\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding box width [pixels]:\",\n",
    "        maximum_bounding_box_width\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding boxes' centers distance [pixels]:\",\n",
    "        round(\n",
    "            number=maximum_bounding_boxes_centers_distance,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding boxes' centers x-coord distance [pixels]:\",\n",
    "        maximum_bounding_boxes_centers_x_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding boxes' centers y-coord distance [pixels]:\",\n",
    "        maximum_bounding_boxes_centers_y_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of number of bounding boxes per image: see plot\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of bounding boxes' centers distance [pixels]: \" +\n",
    "        \"see plot\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of bounding boxes' centers x-coord distance \" +\n",
    "        \"[pixels]: see plot\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of bounding boxes' centers y-coord distance \" +\n",
    "        \"[pixels]: see plot\"\n",
    "    )\n",
    "\n",
    "    plt_figure()\n",
    "\n",
    "    what_it_represent = \"Histogram of Number of Bounding Boxes per Image\"\n",
    "    plt_hist(\n",
    "        x=n_bounding_boxes_per_image_for_histogram,\n",
    "        bins=maximum_n_bounding_boxes_per_image,\n",
    "        align='left',\n",
    "        color='skyblue',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(range(maximum_n_bounding_boxes_per_image))\n",
    "    )\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    what_it_represent = (\n",
    "        \"Histogram of Bounding Boxes' Centers Distance [pixels]\"\n",
    "    )\n",
    "    plt_hist(\n",
    "        x=bounding_boxes_centers_distances_for_histogram,\n",
    "        bins=list(range(int(sqrt(IMAGE_N_COLUMNS**2 + IMAGE_N_ROWS**2)))),\n",
    "        align='left',\n",
    "        color='chartreuse',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(\n",
    "            range(0, int(sqrt(IMAGE_N_COLUMNS**2 + IMAGE_N_ROWS**2)), 20)\n",
    "        ),\n",
    "        fontsize=6,\n",
    "        rotation=90\n",
    "    )\n",
    "    figure_manager = get_current_fig_manager()\n",
    "    figure_manager.resize(*figure_manager.window.maxsize())\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    what_it_represent = (\n",
    "        \"Histogram of Bounding Boxes' Centers X-Coordinate Distance [pixels]\"\n",
    "    )\n",
    "    plt_hist(\n",
    "        x=bounding_boxes_centers_x_coord_distances_for_histogram,\n",
    "        bins=list(range(IMAGE_N_COLUMNS)),\n",
    "        align='left',\n",
    "        color='mediumslateblue',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(range(0, IMAGE_N_COLUMNS, 20)),\n",
    "        fontsize=6,\n",
    "        rotation=90\n",
    "    )\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    what_it_represent = (\n",
    "        \"Histogram of Bounding Boxes' Centers Y-Coordinate Distance [pixels]\"\n",
    "    )\n",
    "    plt_hist(\n",
    "        x=bounding_boxes_centers_y_coord_distances_for_histogram,\n",
    "        bins=list(range(IMAGE_N_ROWS)),\n",
    "        align='left',\n",
    "        color='violet',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(range(0, IMAGE_N_ROWS, 20)),\n",
    "        fontsize=6,\n",
    "        rotation=90\n",
    "    )\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    plt_close()\n",
    "\n",
    "    print('- ' * 30)\n",
    "\n",
    "\n",
    "def label_line_to_image_path_2_bounding_boxes_and_2_model_output(\n",
    "        csv_label_line_segments: List[str]\n",
    ") -> Tuple[\n",
    "        Dict[str, List[Dict[str, int]]],\n",
    "        Dict[str, List[List[Tuple[int, int, int, int]]]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Turn any line of the CSV labels file from the original format\n",
    "    'video_id,sequence,video_frame,sequence_frame,image_id,annotations' into\n",
    "    two dictionariies: the former with the respective image file path as key\n",
    "    and the respective bounding boxes as value, the latter with the respective\n",
    "    image file path as key and the respective model outputs as value.\n",
    "    \"\"\"\n",
    "    image_path = path_join(\n",
    "        DATASET_DIR,\n",
    "        'train_images',\n",
    "        'video_' + csv_label_line_segments[0],\n",
    "        csv_label_line_segments[2] + '.jpg'\n",
    "    )\n",
    "    bounding_boxes = json_loads(\n",
    "        csv_label_line_segments[5]\n",
    "        .replace('\"', '\"\"\"')\n",
    "        .replace(\"'\", '\"')\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            bytes(image_path, 'utf-8'): bounding_boxes\n",
    "        },\n",
    "        {\n",
    "            bytes(image_path, 'utf-8'): turn_bounding_boxes_to_model_outputs(\n",
    "                raw_bounding_boxes=bounding_boxes\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def load_labels_as_paths_to_bounding_boxes_and_model_outputs_dicts() -> Tuple[\n",
    "        Dict[str, List[Dict[str, int]]],\n",
    "        Dict[str, List[List[Tuple[int, int, int, int]]]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the labels' information from the CSV file and return them as a two\n",
    "    dictionaries, the former associating image file paths to respective\n",
    "    bounding boxes and the latter associating image file paths to respective\n",
    "    model outputs.\n",
    "    \"\"\"\n",
    "    image_paths_to_bounding_boxes = {}\n",
    "    image_paths_to_model_outputs = {}\n",
    "\n",
    "    with open(LABELS_FILE_PATH, 'r') as labels_file:\n",
    "        labels_reader = csv_reader(\n",
    "            labels_file,\n",
    "            delimiter=',',\n",
    "            quotechar='\"'\n",
    "        )\n",
    "\n",
    "        for line_index, line_segments in enumerate(labels_reader):\n",
    "            if line_index == 0:\n",
    "                continue\n",
    "\n",
    "            # turning the label from the raw format into processed\n",
    "            # dictionaries to retrieve bounding boxes and model outputs of\n",
    "            # images easily from respective image file paths:\n",
    "            (\n",
    "                image_path_to_bounding_boxes,\n",
    "                image_path_to_model_outputs\n",
    "            ) = label_line_to_image_path_2_bounding_boxes_and_2_model_output(\n",
    "                csv_label_line_segments=line_segments\n",
    "            )\n",
    "            image_paths_to_bounding_boxes.update(image_path_to_bounding_boxes)\n",
    "            image_paths_to_model_outputs.update(image_path_to_model_outputs)\n",
    "\n",
    "    return (image_paths_to_bounding_boxes, image_paths_to_model_outputs)\n",
    "\n",
    "\n",
    "def load_sample_and_get_bounding_boxes(image_path: Tensor) -> Tuple[\n",
    "        Tensor, Tensor\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the sample and get the label - representing bounding boxes - of the\n",
    "    image represented by the input path.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        decode_jpeg(\n",
    "            contents=read_file(\n",
    "                filename=image_path\n",
    "            )\n",
    "        ),\n",
    "        convert_to_tensor(\n",
    "            # bounding boxes as network output values:\n",
    "            value=[\n",
    "                [\n",
    "                    bounding_box_dict['x'],\n",
    "                    bounding_box_dict['y'],\n",
    "                    bounding_box_dict['width'],\n",
    "                    bounding_box_dict['height']\n",
    "                ] for bounding_box_dict in\n",
    "                IMAGE_PATHS_TO_BOUNDING_BOXES[image_path.numpy()]\n",
    "            ],\n",
    "            dtype=DATA_TYPE_FOR_OUTPUTS\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def load_sample_and_get_model_outputs(image_path: Tensor) -> Tuple[\n",
    "        Tensor, Tensor\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the sample and get the label - representing model outputs - of the\n",
    "    image represented by the input path.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        decode_jpeg(\n",
    "            contents=read_file(\n",
    "                filename=image_path\n",
    "            )\n",
    "        ),\n",
    "        convert_to_tensor(\n",
    "            # bounding boxes as network output values:\n",
    "            value=IMAGE_PATHS_TO_MODEL_OUTPUTS[image_path.numpy()],\n",
    "            dtype=DATA_TYPE_FOR_OUTPUTS\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set: Dataset\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Split the input dataset into a training set and a validation set, both\n",
    "    already divided into mini-batches.\n",
    "    \"\"\"\n",
    "    n_samples_in_validation_set = int(\n",
    "        VALIDATION_SET_PORTION_OF_DATA * N_TRAINING_PLUS_VALIDATION_SAMPLES\n",
    "    )\n",
    "    n_samples_in_training_set = (\n",
    "        N_TRAINING_PLUS_VALIDATION_SAMPLES - n_samples_in_validation_set\n",
    "    )\n",
    "\n",
    "    training_set = (\n",
    "        training_plus_validation_set\n",
    "        # selecting only the training samples and labels:\n",
    "        .take(count=n_samples_in_training_set)\n",
    "        # creating mini-batches:\n",
    "        .batch(\n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            drop_remainder=False,\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "            deterministic=True\n",
    "        )\n",
    "    )\n",
    "    # only when running everything in a unified notebook on Kaggle's servers:\n",
    "    if __name__ == 'main_by_mattia':\n",
    "        # optimizing performances by caching end-results:\n",
    "        training_set = training_set.cache(\n",
    "            filename=CACHE_FILE_PATH_FOR_TRAINING_SET\n",
    "        )\n",
    "    # optimizing performances by pre-fetching final elements:\n",
    "    training_set = training_set.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    validation_set = (\n",
    "        training_plus_validation_set\n",
    "        # selecting only the validation samples and labels:\n",
    "        .skip(count=n_samples_in_training_set)\n",
    "        .take(count=n_samples_in_validation_set)\n",
    "        # creating mini-batches:\n",
    "        .batch(\n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            drop_remainder=False,\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "            deterministic=True\n",
    "        )\n",
    "    )\n",
    "    # only when running everything in a unified notebook on Kaggle's servers:\n",
    "    if __name__ == 'main_by_mattia':\n",
    "        # optimizing performances by caching end-results:\n",
    "        validation_set = validation_set.cache(\n",
    "            filename=CACHE_FILE_PATH_FOR_TRAINING_SET\n",
    "        )\n",
    "    # optimizing performances by pre-fetching final elements:\n",
    "    validation_set = validation_set.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return (training_set, validation_set)\n",
    "\n",
    "\n",
    "def show_dataset_as_movie(\n",
    "        ordered_samples_and_labels: Dataset,\n",
    "        bounding_boxes_or_model_outputs: str = 'bounding_boxes'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Show the dataset images frame by frame, reconstructing the video\n",
    "    sequences, with boundinx boxes contained displayed over the respective\n",
    "    sample/frame.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        bounding_boxes_or_model_outputs in ('bounding_boxes', 'model_outputs')\n",
    "    ), \"Invalid 'bounding_boxes_or_model_outputs' input.\"\n",
    "\n",
    "    _, axes = subplots(1, 1)\n",
    "\n",
    "    # for each sample-label pair, a frame fusing them together is shown:\n",
    "    for index, sample_and_label in enumerate(ordered_samples_and_labels):\n",
    "        if index % 1000 == 0:\n",
    "            print(f\"{index} frames shown\")\n",
    "\n",
    "        # clearing axes from the previous frame information:\n",
    "        axes.clear()\n",
    "\n",
    "        # showing the image:\n",
    "        axes.imshow(sample_and_label[0].numpy())\n",
    "\n",
    "        # showing labels...\n",
    "\n",
    "        # ... either as bounding boxes:\n",
    "        if bounding_boxes_or_model_outputs == 'bounding_boxes':\n",
    "            # for each bounding box:\n",
    "            for bounding_box in sample_and_label[1].numpy().tolist():\n",
    "                # drawing the bounding box over the frame image:\n",
    "                axes.add_patch(\n",
    "                    p=Rectangle(\n",
    "                        xy=(bounding_box[0], bounding_box[1]),\n",
    "                        width=bounding_box[2],\n",
    "                        height=bounding_box[3],\n",
    "                        linewidth=2,\n",
    "                        edgecolor='#00ff00',\n",
    "                        facecolor='none'\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # ... or as model output grid cells:\n",
    "        elif bounding_boxes_or_model_outputs == 'model_outputs':\n",
    "            # for each model output grid cell whose label contains anchors:\n",
    "            for cell_row_index in range(OUTPUT_GRID_N_ROWS):\n",
    "                for cell_column_index in range(OUTPUT_GRID_N_COLUMNS):\n",
    "                    # filtering out grid cells not containing any anchor:\n",
    "                    if (\n",
    "                            sample_and_label[1][\n",
    "                                cell_row_index,\n",
    "                                cell_column_index,\n",
    "                                :,\n",
    "                                :\n",
    "                            ].numpy() == zeros(\n",
    "                                shape=(\n",
    "                                    N_ANCHORS_PER_CELL,\n",
    "                                    N_OUTPUTS_PER_ANCHOR\n",
    "                                )\n",
    "                            )\n",
    "                    ).all():\n",
    "                        continue\n",
    "\n",
    "                    # highlighting the full cell over the frame image:\n",
    "                    axes.add_patch(\n",
    "                        p=Rectangle(\n",
    "                            xy=(\n",
    "                                OUTPUT_GRID_CELL_CORNERS_XY_COORDS[\n",
    "                                    cell_row_index,\n",
    "                                    cell_column_index\n",
    "                                ]\n",
    "                            ),\n",
    "                            width=OUTPUT_GRID_CELL_N_COLUMNS,\n",
    "                            height=OUTPUT_GRID_CELL_N_ROWS,\n",
    "                            linewidth=2,\n",
    "                            edgecolor='#00ff00',\n",
    "                            facecolor='none'\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Ill-conceived code.\")\n",
    "\n",
    "        # making the plot go adeah with the next frame after a small pause for\n",
    "        # better observation:\n",
    "        plt_show(block=False)\n",
    "        plt_pause(interval=0.000001)\n",
    "\n",
    "\n",
    "def turn_bounding_boxes_to_model_outputs(\n",
    "        raw_bounding_boxes: List[Dict[str, int]]\n",
    ") -> Dict[str, List[List[Tuple[int, int, int, int]]]]:\n",
    "    \"\"\"\n",
    "    Turn the input, raw list of bounding boxes' position information into the\n",
    "    equivalent information from the model outputs' perspective, as direct\n",
    "    supervision labels - for a single image.\n",
    "    \"\"\"\n",
    "    labels = zeros(\n",
    "        shape=(\n",
    "            OUTPUT_GRID_N_ROWS,\n",
    "            OUTPUT_GRID_N_COLUMNS,\n",
    "            N_ANCHORS_PER_CELL,\n",
    "            N_OUTPUTS_PER_ANCHOR\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # for each bounding box in the image:\n",
    "    for bounding_box in raw_bounding_boxes:\n",
    "        # computing the absolute x and y coordinates of the bounding box\n",
    "        # center:\n",
    "        bounging_box_center_absolute_x_coord = (\n",
    "            bounding_box['x'] + (bounding_box['width'] / 2)\n",
    "        )\n",
    "        bounging_box_center_absolute_y_coord = (\n",
    "            bounding_box['y'] + (bounding_box['height'] / 2)\n",
    "        )\n",
    "\n",
    "        # getting the required information about the grid cell that contains\n",
    "        # its center:\n",
    "        (\n",
    "            cell_row_index,\n",
    "            cell_column_index,\n",
    "            cell_corner_x_coord,\n",
    "            cell_corner_y_coord\n",
    "        ) = get_cell_containing_bounding_box_center(\n",
    "            center_absolute_x_and_y_coords=(\n",
    "                bounging_box_center_absolute_x_coord,\n",
    "                bounging_box_center_absolute_y_coord\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # transforming and normalizing the bounding box coordinates, now\n",
    "        # meaning respectively for x and y the relative center offset from the\n",
    "        # encoling cell top-left corner normalized to the grid cell size and\n",
    "        # for width and height the relative heights and heights with respect\n",
    "        # to the image sides:\n",
    "        relative_x_coord = (\n",
    "            (bounging_box_center_absolute_x_coord - cell_corner_x_coord) /\n",
    "            OUTPUT_GRID_CELL_N_COLUMNS\n",
    "        )\n",
    "        relative_y_coord = (\n",
    "            (bounging_box_center_absolute_y_coord - cell_corner_y_coord) /\n",
    "            OUTPUT_GRID_CELL_N_ROWS\n",
    "        )\n",
    "        relative_width = bounding_box['width'] / IMAGE_N_COLUMNS\n",
    "        relative_height = bounding_box['height'] / IMAGE_N_ROWS\n",
    "\n",
    "        # getting the index of the anchor with closest aspect ratio to the\n",
    "        # considered bounding box:\n",
    "        label_anchor_index = get_index_of_anchor_with_closest_aspect_ratio(\n",
    "            absolute_width=bounding_box['width'],\n",
    "            absolute_height=bounding_box['height']\n",
    "        )\n",
    "\n",
    "        # associating the bounding box attributes to the respective anchor\n",
    "        # labels - after checking there are no intrinsic limitations of the\n",
    "        # employed design choices:\n",
    "        label_cannot_be_associated_to_respective_anchor = (\n",
    "            labels[cell_row_index, cell_column_index, label_anchor_index, :] !=\n",
    "            [.0] * N_OUTPUTS_PER_ANCHOR\n",
    "        ).any()\n",
    "        if label_cannot_be_associated_to_respective_anchor:\n",
    "            raise Exception(\n",
    "                f\"Either more than {N_ANCHORS_PER_CELL} anchors or a \" +\n",
    "                \"better output resolution are required, as more bounding \" +\n",
    "                \"boxes than the set number of anchors are falling within \" +\n",
    "                \"the same output cell in this sample.\"\n",
    "            )\n",
    "        labels[cell_row_index, cell_column_index, label_anchor_index, :] = [\n",
    "            1.0,  # FIXME: supposed to be an objectiveness score or an IoU?\n",
    "            relative_x_coord,\n",
    "            relative_y_coord,\n",
    "            relative_width,\n",
    "            relative_height\n",
    "        ]\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "(\n",
    "    IMAGE_PATHS_TO_BOUNDING_BOXES,\n",
    "    IMAGE_PATHS_TO_MODEL_OUTPUTS\n",
    ") = load_labels_as_paths_to_bounding_boxes_and_model_outputs_dicts()\n",
    "\n",
    "N_TRAINING_PLUS_VALIDATION_SAMPLES = len(IMAGE_PATHS_TO_BOUNDING_BOXES)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if SHOW_BOUNDING_BOXES_STATISTICS:\n",
    "        inspect_bounding_boxes_statistics_on_training_n_validation_set()\n",
    "\n",
    "    samples_n_bounding_boxes_dataset = dataset_of_samples_and_bounding_boxes()\n",
    "\n",
    "    if SHOW_DATASET_MOVIES:\n",
    "        show_dataset_as_movie(\n",
    "            ordered_samples_and_labels=samples_n_bounding_boxes_dataset,\n",
    "            bounding_boxes_or_model_outputs='bounding_boxes'\n",
    "        )\n",
    "\n",
    "    samples_n_model_outputs_dataset = dataset_of_samples_and_model_outputs(\n",
    "        # not shuffling when needing adjacent frames for showing the movie:\n",
    "        shuffle=(not SHOW_DATASET_MOVIES)\n",
    "    )\n",
    "\n",
    "    if SHOW_DATASET_MOVIES:\n",
    "        show_dataset_as_movie(\n",
    "            ordered_samples_and_labels=samples_n_model_outputs_dataset,\n",
    "            bounding_boxes_or_model_outputs='model_outputs'\n",
    "        )\n",
    "\n",
    "    (\n",
    "        training_dataset, validation_dataset\n",
    "    ) = split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set=samples_n_model_outputs_dataset\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed58d7",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities for inference time, for converting model outputs to bounding boxes'\n",
    "predictions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# pylint: disable=import-error,no-name-in-module\n",
    "from tensorflow import (\n",
    "    cast,\n",
    "    clip_by_value,\n",
    "    concat,\n",
    "    convert_to_tensor,\n",
    "    expand_dims,\n",
    "    reshape,\n",
    "    squeeze,\n",
    "    stack,\n",
    "    Tensor,\n",
    "    tile\n",
    ")\n",
    "from tensorflow.image import combined_non_max_suppression\n",
    "from tensorflow.math import (\n",
    "    add,\n",
    "    divide,\n",
    "    subtract,\n",
    "    multiply\n",
    ")\n",
    "# pylint: enable=import-error,no-name-in-module\n",
    "\n",
    "# only when running everything in a unified notebook on Kaggle's servers:\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import (\n",
    "        DATA_TYPE_FOR_OUTPUTS,\n",
    "        IMAGE_N_COLUMNS,\n",
    "        IMAGE_N_ROWS,\n",
    "        N_ANCHORS_PER_CELL,\n",
    "        N_OUTPUTS_PER_ANCHOR,\n",
    "        OUTPUT_GRID_CELL_CORNERS_XY_COORDS,\n",
    "        OUTPUT_GRID_CELL_N_COLUMNS,\n",
    "        OUTPUT_GRID_CELL_N_ROWS\n",
    "    )\n",
    "    from model_architecture import YOLOv3Variant\n",
    "    from samples_and_labels import (\n",
    "        dataset_of_samples_and_model_outputs,\n",
    "        split_dataset_into_batched_training_and_validation_sets\n",
    "    )\n",
    "\n",
    "\n",
    "IOU_THRESHOLD_FOR_NON_MAXIMUM_SUPPRESSION = 0.5\n",
    "MINIMUM_BOUNDING_BOX_SIDE_DIMENSION_TOLERANCE = 0.1\n",
    "MAXIMUM_N_BOUNDING_BOXES_AFTER_NMS = 100\n",
    "SCORE_THRESHOLD_FOR_NON_MAXIMUM_SUPPRESSION = 0.5\n",
    "\n",
    "\n",
    "def batched_anchors_rel_to_real_abs_x_y_w_h(\n",
    "        batched_anchors_relative_x_y_w_h: Tensor,\n",
    "        batched_anchors_corners_absolute_x_y: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Turn batches of arrays of anchors where every anchor is represented by\n",
    "    relative (x center, y center, w, h) values into batches of the same\n",
    "    anchors where each anchor is represented by absolute (x top-left corner,\n",
    "    y top-left corner, w, h) values - x and y represent respectively the x and\n",
    "    y coordinates of the center in the inputs and of the top-left corner in\n",
    "    the output, w and y represent respectively the width and height of sides.\n",
    "    NOTE: this function changes not only the scale but also the meaning of x\n",
    "    and y\n",
    "    ---\n",
    "        Input Shapes:\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                N_ANCHORS_PER_IMAGE,\n",
    "                1,\n",
    "                4\n",
    "            )\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                N_ANCHORS_PER_IMAGE,\n",
    "                2\n",
    "            )\n",
    "    ---\n",
    "        Output Shape:\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                N_ANCHORS_PER_IMAGE,\n",
    "                1,\n",
    "                4\n",
    "            )\n",
    "    \"\"\"\n",
    "    expanded_batched_anchors_corners_absolute_x_y = cast(  # noqa: E501 pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "        # NOTE: they are already discretized, so any truncation due to casting\n",
    "        # is not relevant\n",
    "        x=expand_dims(\n",
    "            input=batched_anchors_corners_absolute_x_y,\n",
    "            axis=2\n",
    "        ),\n",
    "        dtype=DATA_TYPE_FOR_OUTPUTS\n",
    "    )  # shape  (samples, anchors_per_image, 1, 2)\n",
    "\n",
    "    batched_anchors_absolute_w = multiply(\n",
    "        x=batched_anchors_relative_x_y_w_h[..., 2],\n",
    "        y=IMAGE_N_COLUMNS\n",
    "    )  # shape  (samples, anchors_per_image, 1)\n",
    "\n",
    "    batched_anchors_absolute_h = multiply(\n",
    "        x=batched_anchors_relative_x_y_w_h[..., 3],\n",
    "        y=IMAGE_N_ROWS\n",
    "    )  # shape  (samples, anchors_per_image, 1)\n",
    "\n",
    "    batched_anchors_absolute_x = subtract(\n",
    "        x=add(\n",
    "            x=multiply(\n",
    "                x=batched_anchors_relative_x_y_w_h[..., 0],\n",
    "                y=float(OUTPUT_GRID_CELL_N_COLUMNS)\n",
    "            ),  # shape  (samples, anchors_per_image, 1)\n",
    "            y=expanded_batched_anchors_corners_absolute_x_y[..., 0]\n",
    "        ),  # shape  (samples, anchors_per_image, 1)\n",
    "        y=divide(\n",
    "            x=batched_anchors_absolute_w,\n",
    "            y=float(2)\n",
    "        ),  # shape  (samples, anchors_per_image, 1)\n",
    "    )  # shape  (samples, anchors_per_image, 1)\n",
    "\n",
    "    batched_anchors_absolute_y = subtract(\n",
    "        x=add(\n",
    "            x=multiply(\n",
    "                x=batched_anchors_relative_x_y_w_h[..., 1],\n",
    "                y=float(OUTPUT_GRID_CELL_N_ROWS)\n",
    "            ),  # shape  (samples, anchors_per_image, 1)\n",
    "            y=expanded_batched_anchors_corners_absolute_x_y[..., 1]\n",
    "        ),  # shape  (samples, anchors_per_image, 1)\n",
    "        y=divide(\n",
    "            x=batched_anchors_absolute_h,\n",
    "            y=float(2)\n",
    "        ),  # shape  (samples, anchors_per_image, 1)\n",
    "    )  # shape  (samples, anchors_per_image, 1)\n",
    "\n",
    "    return expand_dims(\n",
    "        input=concat(  # noqa: E501 pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "            values=(\n",
    "                batched_anchors_absolute_x,\n",
    "                batched_anchors_absolute_y,\n",
    "                batched_anchors_absolute_w,\n",
    "                batched_anchors_absolute_h\n",
    "            ),\n",
    "            axis=-1\n",
    "        ),  # shape  (samples, anchors_per_image, 4)\n",
    "        axis=2\n",
    "    )  # shape  (samples, anchors_per_image, 1, 4)\n",
    "\n",
    "\n",
    "def batched_anchors_x_y_w_h_to_x1_y1_x2_y2(\n",
    "        batched_anchors_absolute_x_y_w_h: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Turn batches of several anchors each where every anchor is represented by\n",
    "    absolute (x, y, w, h) values, into batches of the same anchors where each\n",
    "    anchor is represented by absolute (x1, y1, x2, y2) values - x and y\n",
    "    represent respectively the x and y coordinates of the top-left corner, w\n",
    "    and y represent respectively the width and height of sides, x1 and y1\n",
    "    represent respectively the x and y coordinates of the top-left corner, x2\n",
    "    and y2 represent respectively the x and y coordinates of the bottom-right\n",
    "    corner - eventually clipping all output coordinates' values to fall inside\n",
    "    the image.\n",
    "    ---\n",
    "        Input Shape:\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                N_ANCHORS_PER_IMAGE,\n",
    "                1,\n",
    "                4\n",
    "            )\n",
    "    ---\n",
    "        Output Shape:\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                N_ANCHORS_PER_IMAGE,\n",
    "                1,\n",
    "                4\n",
    "            )\n",
    "    \"\"\"\n",
    "    batched_anchors_absolute_x1 = clip_by_value(\n",
    "        t=batched_anchors_absolute_x_y_w_h[..., 0],\n",
    "        # shape  (samples, anchors_per_image, 1)\n",
    "        clip_value_min=0,\n",
    "        clip_value_max=(IMAGE_N_COLUMNS - 1)\n",
    "    )  # shape  (samples, anchors_per_image, 1)\n",
    "\n",
    "    batched_anchors_absolute_y1 = clip_by_value(\n",
    "        t=batched_anchors_absolute_x_y_w_h[..., 1],\n",
    "        # shape  (samples, anchors_per_image, 1)\n",
    "        clip_value_min=0,\n",
    "        clip_value_max=(IMAGE_N_ROWS - 1)\n",
    "    )  # shape  (samples, anchors_per_image, 1)\n",
    "\n",
    "    batched_anchors_absolute_x2 = clip_by_value(\n",
    "        t=add(\n",
    "            x=batched_anchors_absolute_x_y_w_h[..., 0],\n",
    "            y=batched_anchors_absolute_x_y_w_h[..., 2]\n",
    "        ),  # shape  (samples, anchors_per_image, 1)\n",
    "        clip_value_min=0,\n",
    "        clip_value_max=(IMAGE_N_COLUMNS - 1)\n",
    "    )  # shape  (samples, anchors_per_image, 1)\n",
    "\n",
    "    batched_anchors_absolute_y2 = clip_by_value(\n",
    "        t=add(\n",
    "            x=batched_anchors_absolute_x_y_w_h[..., 1],\n",
    "            y=batched_anchors_absolute_x_y_w_h[..., 3]\n",
    "        ),  # shape  (samples, anchors_per_image, 1)\n",
    "        clip_value_min=0,\n",
    "        clip_value_max=(IMAGE_N_ROWS - 1)\n",
    "    )  # shape  (samples, anchors_per_image, 1)\n",
    "\n",
    "    return expand_dims(\n",
    "        input=concat(  # noqa: E501 pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "            values=(\n",
    "                batched_anchors_absolute_x1,\n",
    "                batched_anchors_absolute_y1,\n",
    "                batched_anchors_absolute_x2,\n",
    "                batched_anchors_absolute_y2\n",
    "            ),\n",
    "            axis=-1\n",
    "        ),  # shape  (samples, anchors_per_image, 4)\n",
    "        axis=2\n",
    "    )  # shape  (samples, anchors_per_image, 1, 4)\n",
    "\n",
    "\n",
    "def batched_anchors_x1_y1_x2_y2_to_x_y_w_h(\n",
    "        batched_anchors_absolute_x1_y1_x2_y2: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Turn batches of several anchors each where every anchor is represented by\n",
    "    absolute (x1, y1, x2, y2) values, into batches of the same anchors where\n",
    "     eachanchor is represented by absolute (x, y, w, h) values - x and y\n",
    "    represent respectively the x and y coordinates of the top-left corner, w\n",
    "    and y represent respectively the width and height of sides, x1 and y1\n",
    "    represent respectively the x and y coordinates of the top-left corner, x2\n",
    "    and y2 represent respectively the x and y coordinates of the bottom-right\n",
    "    corner.\n",
    "    ---\n",
    "        Input Shape:\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                VARIABLE_N_BOUNDING_BOXES,\n",
    "                4\n",
    "            )\n",
    "    ---\n",
    "        Output Shape:\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                VARIABLE_N_BOUNDING_BOXES,\n",
    "                4\n",
    "            )\n",
    "    \"\"\"\n",
    "    batched_anchors_absolute_x = batched_anchors_absolute_x1_y1_x2_y2[..., 0]\n",
    "    # shape  (samples, boxes)\n",
    "\n",
    "    batched_anchors_absolute_y = batched_anchors_absolute_x1_y1_x2_y2[..., 1]\n",
    "    # shape  (samples, boxes)\n",
    "\n",
    "    batched_anchors_absolute_w = subtract(\n",
    "        x=batched_anchors_absolute_x1_y1_x2_y2[..., 2],\n",
    "        y=batched_anchors_absolute_x1_y1_x2_y2[..., 0]\n",
    "    )  # shape  (samples, boxes)\n",
    "\n",
    "    batched_anchors_absolute_h = subtract(\n",
    "        x=batched_anchors_absolute_x1_y1_x2_y2[..., 3],\n",
    "        y=batched_anchors_absolute_x1_y1_x2_y2[..., 1]\n",
    "    )  # shape  (samples, boxes)\n",
    "\n",
    "    return stack(\n",
    "        values=(\n",
    "            batched_anchors_absolute_x,\n",
    "            batched_anchors_absolute_y,\n",
    "            batched_anchors_absolute_w,\n",
    "            batched_anchors_absolute_h\n",
    "        ),\n",
    "        axis=-1\n",
    "    )  # shape  (samples, boxes, 4)\n",
    "\n",
    "\n",
    "def convert_batched_bounding_boxes_to_final_format(\n",
    "        batched_bounding_boxes: Tensor,\n",
    "        batched_n_valid_bounding_boxes: Tensor,\n",
    "        predicting_online: bool = True,\n",
    "        as_strings: bool = True\n",
    ") -> Union[\n",
    "        Union[str, Tuple[float, int, int, int, int]],\n",
    "        List[Union[str, Tuple[float, int, int, int, int]]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "     - eventually discretizing all absolute coordinates' values to\n",
    "    respect the physical constrant of representing image pixels\n",
    "    ---\n",
    "        Input Shapes:\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                VARIABLE_N_BOUNDING_BOXES,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "            )\n",
    "    \"\"\"\n",
    "    # if the batched inputs represent a single sample:\n",
    "    if predicting_online:\n",
    "        # NOTE: this also automatically asserts that the mini-batch contains\n",
    "        # only a single sample:\n",
    "        n_valid_image_bounding_boxes = int(batched_n_valid_bounding_boxes)\n",
    "\n",
    "        return convert_bounding_boxes_to_final_format(\n",
    "            image_bounding_boxes=squeeze(\n",
    "                input=batched_bounding_boxes,\n",
    "                axis=0\n",
    "            ),\n",
    "            n_valid_bounding_boxes=n_valid_image_bounding_boxes,\n",
    "            as_string=as_strings\n",
    "        )\n",
    "\n",
    "    # if the batched inputs contain more than a single sample:\n",
    "\n",
    "    batch_of_converted_bounding_boxes = []\n",
    "    for current_image_bounding_boxes, n_valid_image_bounding_boxes in zip(\n",
    "            batched_bounding_boxes, batched_n_valid_bounding_boxes\n",
    "    ):\n",
    "        batch_of_converted_bounding_boxes.append(\n",
    "            convert_bounding_boxes_to_final_format(\n",
    "                image_bounding_boxes=current_image_bounding_boxes,\n",
    "                n_valid_bounding_boxes=n_valid_image_bounding_boxes,\n",
    "                as_string=as_strings\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return batch_of_converted_bounding_boxes\n",
    "\n",
    "\n",
    "def convert_bounding_boxes_to_final_format(\n",
    "        image_bounding_boxes: Tensor,\n",
    "        n_valid_bounding_boxes: int,\n",
    "        as_string: bool = True\n",
    ") -> Union[str, Tuple[float, int, int, int, int]]:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    if n_valid_bounding_boxes == 0:\n",
    "        return '' if as_string else []\n",
    "\n",
    "    image_bounding_boxes = (\n",
    "        image_bounding_boxes.numpy().tolist()[:n_valid_bounding_boxes]\n",
    "    )\n",
    "\n",
    "    converted_bounding_boxes = '' if as_string else []\n",
    "    for index, bounding_box_attributes in enumerate(\n",
    "            image_bounding_boxes\n",
    "    ):\n",
    "        if as_string:\n",
    "            if index != 0:\n",
    "                converted_bounding_boxes += ' '\n",
    "            converted_bounding_boxes += (\n",
    "                '{confidence} {x} {y} {width} {height}'.format(\n",
    "                    confidence=bounding_box_attributes[0],\n",
    "                    x=round(bounding_box_attributes[1]),\n",
    "                    y=round(bounding_box_attributes[2]),\n",
    "                    width=round(bounding_box_attributes[3]),\n",
    "                    height=round(bounding_box_attributes[4])\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            converted_bounding_boxes.append(\n",
    "                [\n",
    "                    bounding_box_attributes[0],\n",
    "                    round(bounding_box_attributes[1]),\n",
    "                    round(bounding_box_attributes[2]),\n",
    "                    round(bounding_box_attributes[3]),\n",
    "                    round(bounding_box_attributes[4]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return converted_bounding_boxes\n",
    "\n",
    "\n",
    "def get_bounding_boxes_from_model_outputs(\n",
    "        model_outputs: Tensor,\n",
    "        from_labels: bool = False\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Post-process model outputs by applying format conversion, un-normalization\n",
    "    and reconstruction, and also non-maximum suppression in case the inputs do\n",
    "    not intended as labels but as predictions, to turn batched model outputs\n",
    "    into batches of bounding boxes expressed as (score, x, y, w, h), where x,\n",
    "    y, w, and h respectively represent the top-lect corner absolute x and y\n",
    "    coordinates and the absolute width and height, all in pixels - thus as\n",
    "    (positive or null) integers.\n",
    "\n",
    "    NOTE: in my approach, anchors are just used to create labels as relative\n",
    "    aspect ratios, neither to recreate predictions nor as absolute sizes -\n",
    "    that's why anchors are not used here\n",
    "    ---\n",
    "        Input Shape:\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                N_ANCHORS_PER_CELL,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "    ---\n",
    "        Output Shapes:\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "                VARIABLE_N_BOUNDING_BOXES,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "            - (\n",
    "                VARIABLE_N_SAMPLES,\n",
    "            )\n",
    "    \"\"\"\n",
    "    n_mini_batch_samples = model_outputs.shape[0]\n",
    "\n",
    "    # turning the model outputs into flattened (except along the batch\n",
    "    # dimension) anchor predictions:\n",
    "    anchors_outputs = reshape(\n",
    "        tensor=model_outputs,\n",
    "        shape=(n_mini_batch_samples, -1, N_OUTPUTS_PER_ANCHOR)\n",
    "    )  # shape  (samples, anchors_per_image, attributes)\n",
    "\n",
    "    # crating a corresponding tensor of flattened (except along the batch\n",
    "    # dimension) anchor corners' absolute (x, y) coordinates - NOTE:\n",
    "    # TensorFlow uses a row-major ordering for reshaping, but the following\n",
    "    # procedure ensures that the same ordering as for flattened anchor outputs\n",
    "    # is followed:\n",
    "    anchors_corners_absolute_x_y = reshape(\n",
    "        tensor=tile(\n",
    "            input=expand_dims(\n",
    "                input=tile(\n",
    "                    input=expand_dims(\n",
    "                        input=convert_to_tensor(\n",
    "                            value=OUTPUT_GRID_CELL_CORNERS_XY_COORDS\n",
    "                        ),\n",
    "                        # shape  (rows, columns, 2)\n",
    "                        axis=0\n",
    "                    ),  # shape  (1, rows, columns, 2)\n",
    "                    multiples=(n_mini_batch_samples, 1, 1, 1)\n",
    "                ),  # shape  (samples, rows, columns, 2)\n",
    "                axis=3\n",
    "            ),  # shape  (samples, rows, columns, 1, 2)\n",
    "            multiples=(1, 1, 1, N_ANCHORS_PER_CELL, 1)\n",
    "        ),  # shape  (samples, rows, columns, anchors_per_cell, 2)\n",
    "        shape=(n_mini_batch_samples, -1, 2)\n",
    "    )  # shape  (samples, anchors_per_image, 2)\n",
    "\n",
    "    # applying non-maximum suppression to generate robust bounding box\n",
    "    # candidates with respective reliability scores when the model outputs\n",
    "    # are intended as predictions - non-maximum suppression is not relevant\n",
    "    # when the model outputs are intended as labels as they are already\n",
    "    # discretized:\n",
    "\n",
    "    # adding a dummy class dimension for the later Tensorflow's function\n",
    "    # application - NOTE: a single class in considered in the task of\n",
    "    # interest:\n",
    "    anchors_outputs = expand_dims(input=anchors_outputs, axis=2)\n",
    "    # shape  (samples, anchors_per_image, 1, attributes)\n",
    "\n",
    "    anchors_scores = anchors_outputs[..., 0]\n",
    "    # shape  (samples, anchors_per_image, 1)\n",
    "\n",
    "    anchors_relative_x_y_w_h = anchors_outputs[..., 1:]\n",
    "    # shape  (samples, anchors_per_image, 1, 4)\n",
    "\n",
    "    anchors_absolute_x_y_w_h = batched_anchors_rel_to_real_abs_x_y_w_h(\n",
    "        batched_anchors_relative_x_y_w_h=anchors_relative_x_y_w_h,\n",
    "        batched_anchors_corners_absolute_x_y=anchors_corners_absolute_x_y\n",
    "    )  # shape  (samples, anchors_per_image, 1, 4)\n",
    "\n",
    "    anchors_absolute_x1_y1_x2_y2 = batched_anchors_x_y_w_h_to_x1_y1_x2_y2(\n",
    "        batched_anchors_absolute_x_y_w_h=anchors_absolute_x_y_w_h\n",
    "    )  # shape  (samples, anchors_per_image, 1, 4)\n",
    "\n",
    "    (\n",
    "        boxes_absolute_x1_y1_x2_y2,  # shape  (samples, boxes, 4)\n",
    "        boxes_scores,  # shape  (samples, boxes)\n",
    "        _,  # classes of boxes for each sample, not relevant here\n",
    "        n_valid_bounding_boxes  # shape  (samples,)\n",
    "    ) = combined_non_max_suppression(\n",
    "        boxes=anchors_absolute_x1_y1_x2_y2,\n",
    "        scores=anchors_scores,\n",
    "        # NOTE: a single class in considered in the task of interest:\n",
    "        max_output_size_per_class=MAXIMUM_N_BOUNDING_BOXES_AFTER_NMS,\n",
    "        max_total_size=MAXIMUM_N_BOUNDING_BOXES_AFTER_NMS,\n",
    "        iou_threshold=(\n",
    "            IOU_THRESHOLD_FOR_NON_MAXIMUM_SUPPRESSION if not from_labels\n",
    "            else 0\n",
    "        ),\n",
    "        score_threshold=(\n",
    "            SCORE_THRESHOLD_FOR_NON_MAXIMUM_SUPPRESSION if not from_labels\n",
    "            else (1 - 1e-6)\n",
    "        ),\n",
    "        pad_per_class=False,\n",
    "        clip_boxes=False\n",
    "    )\n",
    "\n",
    "    boxes_absolute_x_y_w_h = batched_anchors_x1_y1_x2_y2_to_x_y_w_h(\n",
    "        batched_anchors_absolute_x1_y1_x2_y2=boxes_absolute_x1_y1_x2_y2\n",
    "    )  # shape  (samples, boxes, 4)\n",
    "\n",
    "    bounding_boxes_scores_plus_absolute_x_y_w_h = concat(  # noqa: E501 pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "        values=(\n",
    "            expand_dims(input=boxes_scores, axis=-1),\n",
    "            # shape  (samples, boxes, 1)\n",
    "            boxes_absolute_x_y_w_h\n",
    "            # shape  (samples, boxes, 4)\n",
    "        ),\n",
    "        axis=-1\n",
    "    )  # shape  (samples, boxes, attributes)\n",
    "\n",
    "    return (\n",
    "        bounding_boxes_scores_plus_absolute_x_y_w_h,\n",
    "        # shape  (samples, boxes, attributes)\n",
    "        n_valid_bounding_boxes\n",
    "        # shape  (samples,)\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (\n",
    "        training_samples_and_labels, validation_samples_and_labels\n",
    "    ) = split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set=dataset_of_samples_and_model_outputs(\n",
    "            shuffle=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = YOLOv3Variant()\n",
    "\n",
    "    for samples_and_labels in training_samples_and_labels:\n",
    "        print('\\n' + '-'*90)\n",
    "\n",
    "        (\n",
    "            expected_bounding_boxes,\n",
    "            n_valid_expected_bounding_boxes\n",
    "        ) = get_bounding_boxes_from_model_outputs(\n",
    "            model_outputs=samples_and_labels[1],\n",
    "            from_labels=True\n",
    "        )\n",
    "        print(\n",
    "            expected_bounding_boxes.shape,\n",
    "            '-',\n",
    "            n_valid_expected_bounding_boxes.shape\n",
    "        )\n",
    "\n",
    "        predictions = model(samples_and_labels[0])\n",
    "\n",
    "        (\n",
    "            inferred_bounding_boxes,\n",
    "            n_valid_inferred_bounding_boxes\n",
    "        ) = get_bounding_boxes_from_model_outputs(\n",
    "            model_outputs=predictions,\n",
    "            from_labels=False\n",
    "        )\n",
    "        print(\n",
    "            inferred_bounding_boxes.shape,\n",
    "            '-',\n",
    "            n_valid_inferred_bounding_boxes.shape\n",
    "        )\n",
    "\n",
    "        break\n",
    "\n",
    "    print('\\n' + '_'*120)\n",
    "\n",
    "    training_samples_and_labels = (\n",
    "        training_samples_and_labels.unbatch().batch(1)\n",
    "    )\n",
    "    for samples_and_labels in training_samples_and_labels:\n",
    "        print('\\n' + '-'*90)\n",
    "\n",
    "        (\n",
    "            expected_bounding_boxes,\n",
    "            n_valid_expected_bounding_boxes\n",
    "        ) = get_bounding_boxes_from_model_outputs(\n",
    "            model_outputs=samples_and_labels[1],\n",
    "            from_labels=True\n",
    "        )\n",
    "        print(\n",
    "            expected_bounding_boxes.shape,\n",
    "            '-',\n",
    "            n_valid_expected_bounding_boxes.shape\n",
    "        )\n",
    "\n",
    "        submissions = convert_batched_bounding_boxes_to_final_format(\n",
    "            batched_bounding_boxes=expected_bounding_boxes,\n",
    "            batched_n_valid_bounding_boxes=n_valid_expected_bounding_boxes,\n",
    "            predicting_online=True,\n",
    "            as_strings=True\n",
    "        )\n",
    "        print(submissions)\n",
    "\n",
    "        predictions = model(samples_and_labels[0])\n",
    "\n",
    "        (\n",
    "            inferred_bounding_boxes,\n",
    "            n_valid_inferred_bounding_boxes\n",
    "        ) = get_bounding_boxes_from_model_outputs(\n",
    "            model_outputs=predictions,\n",
    "            from_labels=False\n",
    "        )\n",
    "        print(\n",
    "            inferred_bounding_boxes.shape,\n",
    "            '-',\n",
    "            n_valid_inferred_bounding_boxes.shape\n",
    "        )\n",
    "\n",
    "        submissions = convert_batched_bounding_boxes_to_final_format(\n",
    "            batched_bounding_boxes=inferred_bounding_boxes,\n",
    "            batched_n_valid_bounding_boxes=n_valid_inferred_bounding_boxes,\n",
    "            predicting_online=True,\n",
    "            as_strings=True\n",
    "        )\n",
    "        print(submissions)\n",
    "\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93aff2b",
   "metadata": {},
   "source": [
    "#### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model architecture definition.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# pylint: disable=import-error,no-name-in-module\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Convolution2D,\n",
    "    LeakyReLU,\n",
    "    MaxPooling2D,\n",
    "    Reshape\n",
    ")\n",
    "from tensorflow.keras.layers.experimental.preprocessing import (\n",
    "    RandomFlip,\n",
    "    Rescaling\n",
    ")\n",
    "# pylint: enable=import-error,no-name-in-module\n",
    "\n",
    "# only when running everything in a unified notebook on Kaggle's servers:\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import (\n",
    "        DOWNSAMPLING_STEPS,\n",
    "        IMAGE_N_CHANNELS,\n",
    "        IMAGE_N_COLUMNS,\n",
    "        IMAGE_N_ROWS,\n",
    "        N_ANCHORS_PER_CELL,\n",
    "        N_OUTPUTS_PER_ANCHOR,\n",
    "        OUTPUT_GRID_N_COLUMNS,\n",
    "        OUTPUT_GRID_N_ROWS\n",
    "    )\n",
    "\n",
    "\n",
    "CONVOLUTIONAL_LAYERS_COMMON_KWARGS = {\n",
    "    'kernel_size': (3, 3),\n",
    "    'strides': (1, 1),\n",
    "    'padding': 'same',\n",
    "    'data_format': 'channels_last',\n",
    "    'dilation_rate': (1, 1),\n",
    "    'groups': 1,\n",
    "    'activation': None,\n",
    "    'use_bias': True\n",
    "}\n",
    "FIRST_LAYER_N_CONVOLUTIONAL_FILTERS = 16  # TODO\n",
    "INPUT_NORMALIZATION_OFFSET = 0.0\n",
    "INPUT_NORMALIZATION_RESCALING_FACTOR = (1. / 255)\n",
    "LEAKY_RELU_NEGATIVE_SLOPE = 0.1\n",
    "N_CONVOLUTIONS_AT_SAME_RESOLUTION = 3\n",
    "POOLING_LAYERS_COMMON_KWARGS = {\n",
    "    'pool_size': (2, 2),\n",
    "    'strides': (2, 2),\n",
    "    'padding': 'valid',\n",
    "    'data_format': 'channels_last',\n",
    "}\n",
    "\n",
    "\n",
    "class YOLOv3Variant(Model):  # noqa: E501 pylint: disable=abstract-method, too-many-ancestors\n",
    "    \"\"\"\n",
    "    Customized architecture variant of YOLOv3.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_plus_norm_plus_activation(\n",
    "            n_of_filters: int\n",
    "    ) -> Sequential:\n",
    "        \"\"\"\n",
    "        Return an instance of an enriched convolutional layer block composed,\n",
    "        going from inputs to outputs, of:\n",
    "        - a 2D convolutional layer without any non-linearity;\n",
    "        - a batch-normalization layer;\n",
    "        - a leaky rectified linear unit activation function.\n",
    "        \"\"\"\n",
    "        return Sequential(\n",
    "            [\n",
    "                Convolution2D(\n",
    "                    filters=n_of_filters,\n",
    "                    **CONVOLUTIONAL_LAYERS_COMMON_KWARGS\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                LeakyReLU(\n",
    "                    alpha=LEAKY_RELU_NEGATIVE_SLOPE\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_fully_convolutional_yolov3_architecture() -> Model:\n",
    "        \"\"\"\n",
    "        Return an instance of the herein defined YOLOv3 model architecture\n",
    "        that represents its fully-convolutional part, that is excluding\n",
    "        bounding boxes' postprocessing (filtering & aggregation).\n",
    "        \"\"\"\n",
    "        inputs = Input(\n",
    "            shape=(IMAGE_N_ROWS, IMAGE_N_COLUMNS, IMAGE_N_CHANNELS)\n",
    "        )\n",
    "\n",
    "        # rescaling the input image to normalize its pixels' intensities:\n",
    "        outputs = Rescaling(\n",
    "            scale=INPUT_NORMALIZATION_RESCALING_FACTOR,\n",
    "            offset=INPUT_NORMALIZATION_OFFSET\n",
    "        )(inputs)\n",
    "\n",
    "        # randomly flipping input images horizontally as a form of data\n",
    "        # augmentation during training:\n",
    "        outputs = RandomFlip(mode='horizontal', seed=0,)(outputs)\n",
    "        # NOTE: step carried out here to take advantage of GPU acceleration,\n",
    "        # unlike as if it were in the training dataset\n",
    "\n",
    "        current_n_of_filters = FIRST_LAYER_N_CONVOLUTIONAL_FILTERS\n",
    "        # for each iso-resolution block of convolutional processing ended by a\n",
    "        # downsampling:\n",
    "        for _ in range(DOWNSAMPLING_STEPS):\n",
    "            # for each enriched convolutional layer in the current\n",
    "            # iso-resolution block:\n",
    "            for _ in range(N_CONVOLUTIONS_AT_SAME_RESOLUTION):\n",
    "                outputs = YOLOv3Variant.conv_plus_norm_plus_activation(\n",
    "                    n_of_filters=current_n_of_filters\n",
    "                )(outputs)\n",
    "\n",
    "            # downsampling, ending the iso-resolution block:\n",
    "            outputs = MaxPooling2D(**POOLING_LAYERS_COMMON_KWARGS)(outputs)\n",
    "\n",
    "            # updating the number of filters for the next iso-resolution\n",
    "            # convolutional layers (by doubling them):\n",
    "            current_n_of_filters *= 2\n",
    "\n",
    "        # final 1x1 convolutions to predict bounding boxes' attributes from\n",
    "        # grid anchors' feature maps:\n",
    "        outputs = Convolution2D(  # pylint: disable=repeated-keyword\n",
    "            filters=(N_ANCHORS_PER_CELL * N_OUTPUTS_PER_ANCHOR),\n",
    "            **(\n",
    "                dict(CONVOLUTIONAL_LAYERS_COMMON_KWARGS, kernel_size=(1, 1))\n",
    "            )\n",
    "        )(outputs)\n",
    "        # NOTE: now bounding boxes' attributes respect the order of meaning\n",
    "        # (object centered probability, x, y, width, height)\n",
    "\n",
    "        # asserting the correctness of the current outputs' shape:\n",
    "        assert (\n",
    "            outputs.shape[1:] == (\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                N_ANCHORS_PER_CELL * N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "        ), \"Unmatched expectations between outputs and labels shape.\"\n",
    "\n",
    "        # reshaping the last output dimension to split anchors and their\n",
    "        # features along two separate dimensions:\n",
    "        outputs = Reshape(\n",
    "            target_shape=(\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                N_ANCHORS_PER_CELL,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "        )(outputs)\n",
    "\n",
    "        # applying an element-wise sigmoidal activation function as all 5\n",
    "        # bounding boxes' output attributes must belong to [0;1] range,\n",
    "        # since they are either probabilities of a single class (the first\n",
    "        # attribute) or relative coordinates (the second and third one) or\n",
    "        # relative sizes (the fourth and fifth one):\n",
    "        outputs = sigmoid(outputs)\n",
    "        # NOTE: these sigmoidal computations are carried out here instead of\n",
    "        # with the loss computation (and during inference) since computing\n",
    "        # them together with the loss functions's operations would not allow\n",
    "        # to achieve better gradients during training, since the objectness\n",
    "        # score needs to undergo the sigmoidal transformation beforehand and\n",
    "        # the other attributes of the anchors do not udnergo transformations\n",
    "        # as BCE, that can be fused together with softmax improving gradients'\n",
    "        # flow, but they all undergo MSE instead, since they represent\n",
    "        # coordinates and not likelihoods/probabilities\n",
    "\n",
    "        return Model(\n",
    "            inputs=inputs,\n",
    "            outputs=outputs\n",
    "        )\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(YOLOv3Variant, self).__init__()\n",
    "        self.yolov3_fcn = self.build_fully_convolutional_yolov3_architecture()\n",
    "\n",
    "    def call(self, inputs: Tensor, training: bool = False) -> Tensor:  # noqa: E501 pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Forward propagation definition.\n",
    "        \"\"\"\n",
    "        # passing the inputs through the fully-convolutional network:\n",
    "        fcn_outputs = self.yolov3_fcn(\n",
    "            inputs=inputs,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # NOTE: this step is not carried out here because the validation loss\n",
    "        # would not otherwise receive the same inputs as during training when\n",
    "        # computed at inference time, with training = False:\n",
    "\n",
    "        # # at inference time:\n",
    "        # if not training:\n",
    "        #     # post-processing the bounding boxes outputs to return only the\n",
    "        #     # final, filtered and aggregated ones:\n",
    "        #     get_bounding_boxes_from_model_outputs(\n",
    "        #         model_outputs=fcn_outputs,\n",
    "        #         from_labels=False\n",
    "        #     )\n",
    "        # # at training time:\n",
    "        # else:\n",
    "        #     # no post-processing:\n",
    "        #     outputs = fcn_outputs\n",
    "\n",
    "        # return outputs\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        return fcn_outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = YOLOv3Variant()\n",
    "\n",
    "    model.yolov3_fcn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4a8ff",
   "metadata": {},
   "source": [
    "#### Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d313cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definitions of the employed loss function and metrics.\n",
    "\n",
    "NOTE on the employed metric, citing the competition explanation:\n",
    "------------------------------------------------------------------------------\n",
    "\"This competition is evaluated on the F2 Score at different intersection over\n",
    "union (IoU) thresholds. The F2 metric weights recall more heavily than\n",
    "precision, as in this case it makes sense to tolerate some false positives\n",
    "in order to ensure very few starfish are missed.\n",
    "\n",
    "The metric sweeps over IoU thresholds in the range of 0.3 to 0.8 with a step\n",
    "size of 0.05, calculating an F2 score at each threshold. For example, at a\n",
    "threshold of 0.5, a predicted object is considered a \"hit\" if its IoU with a\n",
    "ground truth object is at least 0.5.\n",
    "\n",
    "A true positive is the first (in confidence order, see details below)\n",
    "submission box in a sample with an IoU greater than the threshold against an\n",
    "unmatched solution box.\n",
    "\n",
    "Once all submission boxes have been evaluated, any unmatched submission boxes\n",
    "are false positives; any unmatched solution boxes are false negatives.\n",
    "\n",
    "The final F2 Score is calculated as the mean of the F2 scores at each IoU\n",
    "threshold. Within each IoU threshold the competition metric uses micro\n",
    "averaging; every true positive, false positive, and false negative has equal\n",
    "weight compared to each other true positive, false positive, and false\n",
    "negative.\n",
    "\n",
    "In your submission, you are also asked to provide a confidence level for each\n",
    "bounding box. Bounding boxes are evaluated in order of their confidence\n",
    "levels. This means that bounding boxes with higher confidence will be checked\n",
    "first for matches against solutions, which determines what boxes are\n",
    "considered true and false positives.\"\n",
    "------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "# pylint: disable=import-error,no-name-in-module\n",
    "from tensorflow import (\n",
    "    expand_dims,\n",
    "    stack,\n",
    "    Tensor,\n",
    "    tile,\n",
    "    where,\n",
    "    zeros\n",
    ")\n",
    "from tensorflow.keras.losses import binary_crossentropy, mean_absolute_error\n",
    "from tensorflow.math import (\n",
    "    add,\n",
    "    greater_equal,\n",
    "    logical_not,\n",
    "    multiply,\n",
    "    reduce_mean,\n",
    "    reduce_sum\n",
    ")\n",
    "# pylint: enable=import-error,no-name-in-module\n",
    "\n",
    "# only when running everything in a unified notebook on Kaggle's servers:\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import (\n",
    "        LOSS_CONTRIBUTE_IMPORTANCE_OF_EMPTY_ANCHORS,\n",
    "        LOSS_CONTRIBUTE_IMPORTANCE_OF_FULL_ANCHORS,\n",
    "        OUTPUT_GRID_N_ROWS,\n",
    "        OUTPUT_GRID_N_COLUMNS,\n",
    "        N_ANCHORS_PER_CELL,\n",
    "        N_OUTPUTS_PER_ANCHOR\n",
    "    )\n",
    "    from inference import (\n",
    "        get_bounding_boxes_from_model_outputs,\n",
    "        convert_batched_bounding_boxes_to_final_format\n",
    "    )\n",
    "    from samples_and_labels import (\n",
    "        MINI_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "\n",
    "EPSILON = 1e-7\n",
    "IOU_THRESHOLDS = [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]\n",
    "LABELS_FULL_SHAPE = (\n",
    "    MINI_BATCH_SIZE,\n",
    "    OUTPUT_GRID_N_ROWS,\n",
    "    OUTPUT_GRID_N_COLUMNS,\n",
    "    N_ANCHORS_PER_CELL,\n",
    "    N_OUTPUTS_PER_ANCHOR\n",
    ")\n",
    "\n",
    "\n",
    "def compute_intersection_over_union(\n",
    "        x_y_w_h_first_box: Tuple[int, int, int, int],\n",
    "        x_y_w_h_second_box: Tuple[int, int, int, int]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the intersection over union (IoU) between two boxes represented\n",
    "    by the two integer sets of {top-left corner x coordinate, top-left corner\n",
    "    y coordinate, box width, box height} given as inputs.\n",
    "    \"\"\"\n",
    "    # boxes intersection area:\n",
    "    boxes_intersection_area = (\n",
    "        (  # x-side intersection length:\n",
    "            max(\n",
    "                (\n",
    "                    min(\n",
    "                        x_y_w_h_first_box[0] + x_y_w_h_first_box[2],\n",
    "                        x_y_w_h_second_box[0] + x_y_w_h_second_box[2]\n",
    "                    ) - max(\n",
    "                        x_y_w_h_first_box[0],\n",
    "                        x_y_w_h_second_box[0]\n",
    "                    )\n",
    "                ),\n",
    "                0\n",
    "            )\n",
    "        ) * (  # y-side intersection length:\n",
    "            max(\n",
    "                (\n",
    "                    min(\n",
    "                        x_y_w_h_first_box[1] + x_y_w_h_first_box[3],\n",
    "                        x_y_w_h_second_box[1] + x_y_w_h_second_box[3]\n",
    "                    ) - max(\n",
    "                        x_y_w_h_first_box[1],\n",
    "                        x_y_w_h_second_box[1]\n",
    "                    )\n",
    "                ),\n",
    "                0\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        boxes_intersection_area / (  # boxes union area:\n",
    "            (x_y_w_h_first_box[2] * x_y_w_h_first_box[3])  # 1st box area:\n",
    "            + (x_y_w_h_second_box[2] * x_y_w_h_second_box[3])  # 2nd box area\n",
    "            - boxes_intersection_area\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_mean_f2_scores(\n",
    "        images_matches: List[Tuple[float, float, float]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Return the F2-scores of each mini-batch sample, given their numbers of\n",
    "    false positives, false negatives and true positives as inputs.\n",
    "    \"\"\"\n",
    "    cumulative_f2_score = 0\n",
    "    number_of_f2_scores_summed = 0\n",
    "    for true_positives, false_positives, false_negatives in images_matches:\n",
    "        number_of_f2_scores_summed += 1\n",
    "        cumulative_f2_score += (\n",
    "            true_positives / (\n",
    "                true_positives + 0.8*false_negatives + 0.2*false_positives\n",
    "                + EPSILON\n",
    "            )\n",
    "        )\n",
    "    return cumulative_f2_score / number_of_f2_scores_summed\n",
    "\n",
    "\n",
    "def evaluate_batched_bounding_boxes_matching(\n",
    "        expected_bounding_boxes: List[Tuple[float, int, int, int, int]],\n",
    "        predicted_bounding_boxes: List[Tuple[float, int, int, int, int]],\n",
    "        iou_threshold: float\n",
    ") -> List[Tuple[int, int, int]]:\n",
    "    \"\"\"\n",
    "    Retun the true positives, false positives, false negatives - according to\n",
    "    the competition metric definition - for each pair of arrays of predicted\n",
    "    vs expected bounding boxes in the batched inputs.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for image_expected_bounding_boxes, image_predicted_bounding_boxes in zip(\n",
    "            expected_bounding_boxes, predicted_bounding_boxes\n",
    "    ):\n",
    "        # sorting the predicted bounding boxes of the considered image by\n",
    "        # relevance according to the predicted confidence score:\n",
    "        best_to_worst_predicted_bounding_boxes = sorted(\n",
    "            image_predicted_bounding_boxes,\n",
    "            key=lambda bounding_box_attributes: bounding_box_attributes[0],\n",
    "            reverse=True\n",
    "        )\n",
    "        # NOTE: the expected bounding boxes are equally important, no sorting\n",
    "        # is required\n",
    "\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "\n",
    "        for predicted_bounding_box in best_to_worst_predicted_bounding_boxes:\n",
    "            current_bounding_box_matched = True\n",
    "\n",
    "            for index, expected_bounding_box in enumerate(\n",
    "                    image_expected_bounding_boxes\n",
    "            ):\n",
    "                if (\n",
    "                        compute_intersection_over_union(\n",
    "                            x_y_w_h_first_box=predicted_bounding_box[1:],\n",
    "                            x_y_w_h_second_box=expected_bounding_box[1:]\n",
    "                        ) >= iou_threshold\n",
    "                ):\n",
    "                    current_bounding_box_matched = True\n",
    "                    image_expected_bounding_boxes.remove(index)\n",
    "                    true_positives += 1\n",
    "                    break\n",
    "\n",
    "            if not current_bounding_box_matched:\n",
    "                false_positives += 1\n",
    "\n",
    "        false_negatives = len(image_expected_bounding_boxes)\n",
    "\n",
    "        matches.append([true_positives, false_positives, false_negatives])\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "def iou_threshold_averaged_f2_score(y_true: Tensor, y_pred: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Metric used to validate the model goodness - according to the competition\n",
    "    aim - that represents the F2 score, as they decided to favor recall twice\n",
    "    as much as precision, avereaged over different IoU thresholds for\n",
    "    considering bounding boxes as detected or not, with these thresholds\n",
    "    being: {0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8}.\n",
    "    \"\"\"\n",
    "    # turning the labels representing model outputs into bounding boxes,\n",
    "    # following the same format that the predictions assume at inference time,\n",
    "    # when they undergo an additional post-processing, unlike during training:\n",
    "    (\n",
    "        labels_bounding_boxes, labels_n_valid_bounding_boxes\n",
    "    ) = get_bounding_boxes_from_model_outputs(\n",
    "        model_outputs=y_true,\n",
    "        from_labels=True\n",
    "    )\n",
    "    (\n",
    "        predictions_bounding_boxes, predictions_n_valid_bounding_boxes\n",
    "    ) = get_bounding_boxes_from_model_outputs(\n",
    "        model_outputs=y_pred,\n",
    "        from_labels=False\n",
    "    )\n",
    "\n",
    "    labels_as_lists_of_bounding_boxes = (\n",
    "        convert_batched_bounding_boxes_to_final_format(\n",
    "            batched_bounding_boxes=labels_bounding_boxes,\n",
    "            batched_n_valid_bounding_boxes=labels_n_valid_bounding_boxes,\n",
    "            predicting_online=False,\n",
    "            as_strings=False\n",
    "        )\n",
    "    )\n",
    "    predictions_as_lists_of_bounding_boxes = (\n",
    "        convert_batched_bounding_boxes_to_final_format(\n",
    "            batched_bounding_boxes=predictions_bounding_boxes,\n",
    "            batched_n_valid_bounding_boxes=predictions_n_valid_bounding_boxes,\n",
    "            predicting_online=False,\n",
    "            as_strings=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mean_f2_scores_for_different_iou_thresholds = []\n",
    "\n",
    "    for threshold in IOU_THRESHOLDS:\n",
    "        mean_f2_scores_for_different_iou_thresholds.append(\n",
    "            compute_mean_f2_scores(\n",
    "                images_matches=evaluate_batched_bounding_boxes_matching(\n",
    "                    expected_bounding_boxes=labels_as_lists_of_bounding_boxes,\n",
    "                    predicted_bounding_boxes=(\n",
    "                        predictions_as_lists_of_bounding_boxes\n",
    "                    ),\n",
    "                    iou_threshold=threshold\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return reduce_mean(\n",
    "        input_tensor=stack(\n",
    "            values=mean_f2_scores_for_different_iou_thresholds,\n",
    "            axis=-1\n",
    "        ),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def yolov3_variant_loss(y_true: Tensor, y_pred: Tensor) -> Tensor:  # noqa: E501 pylint: disable=too-many-locals\n",
    "    \"\"\"\n",
    "    Loss function minimized to train the defined YOLOv3 variant.\n",
    "    ---\n",
    "        Input Shapes:\n",
    "            - (\n",
    "                MINI_BATCH_SIZE,\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                N_ANCHORS_PER_CELL,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "            - (\n",
    "                MINI_BATCH_SIZE,\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                N_ANCHORS_PER_CELL,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "    ---\n",
    "        Output Shape:\n",
    "            - (MINI_BATCH_SIZE,)\n",
    "    \"\"\"\n",
    "    dummy_zeros_to_get_no_loss = zeros(shape=LABELS_FULL_SHAPE)\n",
    "\n",
    "    true_anchors_with_objects_flags = tile(\n",
    "        input=expand_dims(\n",
    "            input=greater_equal(\n",
    "                x=y_true[..., 0],\n",
    "                y=0.5\n",
    "            ),\n",
    "            axis=-1\n",
    "        ),\n",
    "        multiples=(1, 1, 1, 1, N_OUTPUTS_PER_ANCHOR)\n",
    "    )  # shape  (samples, rows, columns, anchors, attributes)\n",
    "    true_anchors_without_objects_flags = logical_not(\n",
    "        x=true_anchors_with_objects_flags\n",
    "    )  # shape  (samples, rows, columns, anchors, attributes)\n",
    "\n",
    "    y_true_full_anchors = expand_dims(\n",
    "        input=where(\n",
    "            condition=true_anchors_with_objects_flags,\n",
    "            x=y_true,\n",
    "            y=dummy_zeros_to_get_no_loss\n",
    "        ),\n",
    "        axis=-1\n",
    "    )  # shape  (samples, rows, columns, anchors, attributes, 1)\n",
    "    y_true_empty_anchors = expand_dims(\n",
    "        input=where(\n",
    "            condition=true_anchors_without_objects_flags,\n",
    "            x=y_true,\n",
    "            y=dummy_zeros_to_get_no_loss\n",
    "        ),\n",
    "        axis=-1\n",
    "    )  # shape  (samples, rows, columns, anchors, attributes, 1)\n",
    "    y_pred_full_anchors = expand_dims(\n",
    "        input=where(\n",
    "            condition=true_anchors_with_objects_flags,\n",
    "            x=y_pred,\n",
    "            y=dummy_zeros_to_get_no_loss\n",
    "        ),\n",
    "        axis=-1\n",
    "    )  # shape  (samples, rows, columns, anchors, attributes, 1)\n",
    "    y_pred_empty_anchors = expand_dims(\n",
    "        input=where(\n",
    "            condition=true_anchors_without_objects_flags,\n",
    "            x=y_pred,\n",
    "            y=dummy_zeros_to_get_no_loss\n",
    "        ),\n",
    "        axis=-1\n",
    "    )  # shape  (samples, rows, columns, anchors, attributes, 1)\n",
    "\n",
    "    full_anchors_objectness_loss_per_anchor = binary_crossentropy(\n",
    "        y_true=y_true_full_anchors[..., 0, :],\n",
    "        y_pred=y_pred_full_anchors[..., 0, :],\n",
    "        from_logits=False,\n",
    "        axis=-1,\n",
    "    )  # shape  (samples, rows, columns, anchors)\n",
    "\n",
    "    empty_anchors_objectness_loss_per_anchor = binary_crossentropy(\n",
    "        y_true=y_true_empty_anchors[..., 0, :],\n",
    "        y_pred=y_pred_empty_anchors[..., 0, :],\n",
    "        from_logits=False,\n",
    "        axis=-1,\n",
    "    )  # shape  (samples, rows, columns, anchors)\n",
    "\n",
    "    full_anchors_coordinates_offsets_loss_per_anchor = reduce_sum(\n",
    "        input_tensor=mean_absolute_error(\n",
    "            y_true=y_true_full_anchors[..., 1:3, :],\n",
    "            y_pred=y_pred_full_anchors[..., 1:3, :],\n",
    "        ),\n",
    "        axis=-1\n",
    "    )  # shape  (samples, rows, columns, anchors)\n",
    "    full_anchors_coordinates_scales_loss_per_anchor = reduce_sum(\n",
    "        input_tensor=mean_absolute_error(\n",
    "            y_true=y_true_full_anchors[..., 3:, :],\n",
    "            y_pred=y_pred_full_anchors[..., 3:, :],\n",
    "        ),\n",
    "        axis=-1\n",
    "    )  # shape  (samples, rows, columns, anchors)\n",
    "\n",
    "    full_anchors_coordinates_loss_per_anchor = add(\n",
    "        x=full_anchors_coordinates_offsets_loss_per_anchor,\n",
    "        y=full_anchors_coordinates_scales_loss_per_anchor\n",
    "    )  # shape  (samples, rows, columns, anchors)\n",
    "\n",
    "    full_anchors_mean_loss = reduce_mean(\n",
    "        input_tensor=add(\n",
    "            x=full_anchors_objectness_loss_per_anchor,\n",
    "            y=full_anchors_coordinates_loss_per_anchor\n",
    "        ),\n",
    "        axis=[1, 2, 3]\n",
    "    )  # shape  (samples,)\n",
    "\n",
    "    empty_anchors_mean_loss = reduce_mean(\n",
    "        input_tensor=empty_anchors_objectness_loss_per_anchor,\n",
    "        axis=[1, 2, 3]\n",
    "    )  # shape  (samples,)\n",
    "\n",
    "    # NOTE: without weighting, here, after mean reduction, it means that both\n",
    "    # terms will have the same weight, irrespectively of their imbalance\n",
    "    return add(\n",
    "        x=multiply(\n",
    "            x=full_anchors_mean_loss,\n",
    "            y=LOSS_CONTRIBUTE_IMPORTANCE_OF_FULL_ANCHORS\n",
    "        ),\n",
    "        y=multiply(\n",
    "            x=empty_anchors_mean_loss,\n",
    "            y=LOSS_CONTRIBUTE_IMPORTANCE_OF_EMPTY_ANCHORS\n",
    "        )\n",
    "    )  # shape  (samples,)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f1188",
   "metadata": {},
   "source": [
    "#### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b502bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execution of the defined model training and validation on the respective\n",
    "preprocessed dataset splits, optimizing the defined loss and monitoring the\n",
    "metrics of interest.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from os import getcwd, pardir\n",
    "from os.path import join as path_join\n",
    "from typing import List\n",
    "\n",
    "from matplotlib.pyplot import (\n",
    "    close,\n",
    "    figure,\n",
    "    pause,\n",
    "    plot,\n",
    "    savefig,\n",
    "    show,\n",
    "    xlabel,\n",
    "    ylabel\n",
    ")\n",
    "# pylint: disable=import-error,no-name-in-module\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# pylint: enable=import-error,no-name-in-module\n",
    "\n",
    "# only when running everything in a unified notebook on Kaggle's servers:\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from loss_and_metrics import (\n",
    "        iou_threshold_averaged_f2_score,\n",
    "        yolov3_variant_loss\n",
    "    )\n",
    "    from model_architecture import YOLOv3Variant\n",
    "    from samples_and_labels import (\n",
    "        dataset_of_samples_and_model_outputs,\n",
    "        split_dataset_into_batched_training_and_validation_sets\n",
    "    )\n",
    "\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# NOTE: these are 1-based indexes:\n",
    "EPOCHS_WHEN_VALIDATION_CARRIED_OUT = [\n",
    "    # 1,\n",
    "    int(N_EPOCHS / 2),\n",
    "    # (N_EPOCHS - 1),\n",
    "    N_EPOCHS\n",
    "]\n",
    "\n",
    "# only when running everything in a unified notebook on Kaggle's servers:\n",
    "if __name__ != 'main_by_mattia':\n",
    "    TRAINING_AND_VALIDATION_STATISTICS_DIR = path_join(\n",
    "        getcwd(),\n",
    "        pardir,\n",
    "        'docs',\n",
    "        'pictures'\n",
    "    )\n",
    "else:\n",
    "    TRAINING_AND_VALIDATION_STATISTICS_DIR = getcwd()\n",
    "\n",
    "\n",
    "def plot_and_save_training_and_validation_statistics(\n",
    "        training_epoch_numbers: List[int],\n",
    "        training_loss_values: List[float],\n",
    "        validation_epoch_numbers: List[int],\n",
    "        validation_loss_values: List[float],\n",
    "        validation_metric_values: List[float],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot and save the training and validation loss and metric trends with\n",
    "    epochs.\n",
    "    \"\"\"\n",
    "    figure()\n",
    "\n",
    "    plot(training_epoch_numbers, training_loss_values)\n",
    "    plot(validation_epoch_numbers, validation_loss_values, 'ro')\n",
    "\n",
    "    xlabel(xlabel=\"Epoch Number\")\n",
    "    ylabel(ylabel=\"Loss\")\n",
    "\n",
    "    savefig(\n",
    "        fname=path_join(\n",
    "            TRAINING_AND_VALIDATION_STATISTICS_DIR,\n",
    "            'Training and Validation Loss Trends.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    show(block=False)\n",
    "    pause(interval=5)\n",
    "\n",
    "    close()\n",
    "\n",
    "    figure()\n",
    "\n",
    "    plot(validation_epoch_numbers, validation_metric_values, 'ro')\n",
    "\n",
    "    xlabel(xlabel=\"Epoch Number\")\n",
    "    ylabel(ylabel=\"Metric\")\n",
    "\n",
    "    savefig(\n",
    "        fname=path_join(\n",
    "            TRAINING_AND_VALIDATION_STATISTICS_DIR,\n",
    "            'Training and Validation Metric Trends.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    show(block=False)\n",
    "    pause(interval=5)\n",
    "\n",
    "    close()\n",
    "\n",
    "\n",
    "def train_and_validate_model(\n",
    "        model_instance: Model,\n",
    "        training_set: Dataset,\n",
    "        validation_set: Dataset\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compile (in TensorFlow's language acception, i.e. associate optimizer,\n",
    "    loss function and metrics to) the input model instance and alternatively\n",
    "    training and validating it on the respective input datasets, eventually\n",
    "    plotting and saving training and validation statistics.\n",
    "    \"\"\"\n",
    "    # the same optimizer is references throughout all the training procedure\n",
    "    # so as not to lose its internal states/weights, since it's a stateful\n",
    "    # optimizer whose parameters are updated during training - as well as the\n",
    "    # model ones:\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # initializing the training and validation statistics:\n",
    "    epoch_numbers = []\n",
    "    training_loss_trend = []\n",
    "    validation_loss_trend = []\n",
    "    validation_metric_trend = []\n",
    "\n",
    "    # for each epoch:\n",
    "    for epoch_number in range(1, (N_EPOCHS + 1)):\n",
    "        epoch_numbers.append(epoch_number)\n",
    "\n",
    "        # training:\n",
    "\n",
    "        # re-compiling the model to avoid the eager metric computation:\n",
    "        model_instance.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=yolov3_variant_loss,\n",
    "            # NOTE: the defined metric cannot be run when not in eager mode,\n",
    "            # so it is not evaluated while training:\n",
    "            metrics=[]\n",
    "        )\n",
    "        # training the model (on the training set):\n",
    "        trainin_history = model_instance.fit(\n",
    "            x=training_set,\n",
    "            epochs=1,\n",
    "        )\n",
    "        training_loss_trend.append(trainin_history.history['loss'][0])\n",
    "\n",
    "        if epoch_number in EPOCHS_WHEN_VALIDATION_CARRIED_OUT:\n",
    "            # validation:\n",
    "\n",
    "            # re-compiling the model to allow for the eager metric\n",
    "            # computation:\n",
    "            model_instance.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=yolov3_variant_loss,\n",
    "                metrics=[iou_threshold_averaged_f2_score],\n",
    "                # NOTE: the defined metric can only be run in eager mode:\n",
    "                run_eagerly=True\n",
    "            )\n",
    "            # validating the model (on the validation set):\n",
    "            loss_and_metric = model_instance.evaluate(\n",
    "                x=validation_set\n",
    "            )\n",
    "            validation_loss_trend.append(loss_and_metric[0])\n",
    "            validation_metric_trend.append(loss_and_metric[1])\n",
    "\n",
    "    plot_and_save_training_and_validation_statistics(\n",
    "        training_epoch_numbers=epoch_numbers,\n",
    "        training_loss_values=training_loss_trend,\n",
    "        validation_epoch_numbers=EPOCHS_WHEN_VALIDATION_CARRIED_OUT,\n",
    "        validation_loss_values=validation_loss_trend,\n",
    "        validation_metric_values=validation_metric_trend,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (\n",
    "        training_samples_and_labels, validation_samples_and_labels\n",
    "    ) = split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set=dataset_of_samples_and_model_outputs()\n",
    "    )\n",
    "\n",
    "    model = YOLOv3Variant()\n",
    "\n",
    "    train_and_validate_model(\n",
    "        model_instance=model,\n",
    "        training_set=training_samples_and_labels.take(4),\n",
    "        validation_set=validation_samples_and_labels.take(3)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98d691",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execution of the proposed competition solution.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from random import seed as random_seed\n",
    "\n",
    "from numpy.random import seed as numpy_seed\n",
    "# pylint: disable=import-error,no-name-in-module\n",
    "from tensorflow import convert_to_tensor, expand_dims\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.random import set_seed\n",
    "# pylint: enable=import-error,no-name-in-module\n",
    "\n",
    "# only when running everything in a unified notebook on Kaggle's servers:\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import DATA_TYPE_FOR_INPUTS\n",
    "    from inference import (\n",
    "        convert_batched_bounding_boxes_to_final_format,\n",
    "        get_bounding_boxes_from_model_outputs\n",
    "    )\n",
    "    from model_architecture import YOLOv3Variant\n",
    "    from samples_and_labels import (\n",
    "        dataset_of_samples_and_model_outputs,\n",
    "        split_dataset_into_batched_training_and_validation_sets\n",
    "    )\n",
    "    from training_and_validation import train_and_validate_model\n",
    "\n",
    "\n",
    "def fix_seeds_for_reproducible_results() -> None:\n",
    "    \"\"\"\n",
    "    Make the subsequent instructions produce purely deterministic outputs by\n",
    "    fixing all the relevant seeds.\n",
    "    \"\"\"\n",
    "    random_seed(a=0)\n",
    "    _ = numpy_seed(seed=0)\n",
    "    set_seed(seed=0)\n",
    "\n",
    "\n",
    "def infer_on_test_set_and_submit(trained_model_instance: Model) -> None:\n",
    "    \"\"\"\n",
    "    Predict bounding boxes on all test set images, while submitting\n",
    "    predictions, in an online fashione: one sample at a time.\n",
    "    NOTE: the logic is the same as specified in the Kaggle competition's\n",
    "    rules.\n",
    "    NOTE: the 'pixel_array's served by the competition API iterator are Numpy\n",
    "    arrays with shape (720, 1280, 3), thus a single sample at a time is\n",
    "    served, actually having to predict online.\n",
    "    \"\"\"\n",
    "    import greatbarrierreef  # noqa: E501 pylint: disable=import-outside-toplevel,import-error\n",
    "\n",
    "    # initialize the environment:\n",
    "    env = greatbarrierreef.make_env()\n",
    "\n",
    "    # an iterator which loops over the test set and sample submission:\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "    for (pixel_array, sample_prediction_df) in iter_test:\n",
    "        sample_prediction_df['annotations'] = (  # make your predictions here\n",
    "            convert_batched_bounding_boxes_to_final_format(\n",
    "                *(\n",
    "                    get_bounding_boxes_from_model_outputs(\n",
    "                        model_outputs=trained_model_instance(\n",
    "                            expand_dims(\n",
    "                                input=convert_to_tensor(\n",
    "                                    value=pixel_array,\n",
    "                                    dtype=DATA_TYPE_FOR_INPUTS\n",
    "                                ),\n",
    "                                axis=0\n",
    "                            )\n",
    "                        ),\n",
    "                        from_labels=False\n",
    "                    )\n",
    "                ),\n",
    "                predicting_online=True,\n",
    "                as_strings=True\n",
    "            )\n",
    "        )\n",
    "        env.predict(sample_prediction_df)   # register your predictions\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Execute the proposed competition solution.\n",
    "    \"\"\"\n",
    "    fix_seeds_for_reproducible_results()\n",
    "\n",
    "    (\n",
    "        training_samples_and_labels, validation_samples_and_labels\n",
    "    ) = split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set=dataset_of_samples_and_model_outputs()\n",
    "    )\n",
    "\n",
    "    model = YOLOv3Variant()\n",
    "\n",
    "    train_and_validate_model(\n",
    "        model_instance=model,\n",
    "        training_set=training_samples_and_labels,\n",
    "        validation_set=validation_samples_and_labels\n",
    "    )\n",
    "\n",
    "    infer_on_test_set_and_submit(trained_model_instance=model)\n",
    "\n",
    "\n",
    "# only when running everything in a unified notebook on Kaggle's servers:\n",
    "if __name__ == 'main_by_mattia':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
