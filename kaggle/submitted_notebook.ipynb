{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f762c1",
   "metadata": {},
   "source": [
    "# Mattia Sarti's Notebook\n",
    "### The following source code is illustrated [here](https://github.com/MattiaSarti/yolo-to-help-protect-the-great-barrier-reef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76929f8d",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__ = 'main_by_mattia'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fec222",
   "metadata": {},
   "source": [
    "#### Common constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convenient definitions of common constants.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from numpy import arange, meshgrid, ndarray, stack\n",
    "# pylint: disable=import-error\n",
    "from tensorflow import float32 as tf_float32, uint8 as tf_uint8\n",
    "# pylint: enable=import-error\n",
    "\n",
    "\n",
    "def compute_grid_cell_centers_xy_coords() -> Tuple[ndarray, ndarray]:\n",
    "    \"\"\"\n",
    "    Return two 3D arrays respectively representing the output grid cell\n",
    "    centers' (x, y) coordinates and top-left corners' (x, y) coordinates,\n",
    "    indexed along the first two dimensions as rows and columns of cells in the\n",
    "    output grid.\n",
    "    ---\n",
    "        Outputs' Shapes:\n",
    "            - (OUTPUT_GRID_N_ROWS, OUTPUT_GRID_N_COLUMNS, 2)\n",
    "            - (OUTPUT_GRID_N_ROWS, OUTPUT_GRID_N_COLUMNS, 2)\n",
    "    ---\n",
    "        Outputs' Meanings:\n",
    "            - the first dimension is the row index of the grid cell and the\n",
    "            second dimension is the column index of the grid cell, while the\n",
    "            third dimension represents the tuple of center (x, y) coordinates\n",
    "            of the considered grid cell\n",
    "            - the first dimension is the row index of the grid cell and the\n",
    "            second dimension is the column index of the grid cell, while the\n",
    "            third dimension represents the tuple of top-left corner (x, y)\n",
    "            coordinates of the considered grid cell\n",
    "    \"\"\"\n",
    "    # x and y possible values spanned by grid cell centers:\n",
    "    centers_x_coords_values = arange(\n",
    "        start=int(OUTPUT_GRID_CELL_N_COLUMNS / 2),\n",
    "        stop=IMAGE_N_COLUMNS,\n",
    "        step=OUTPUT_GRID_CELL_N_COLUMNS\n",
    "    )\n",
    "    assert centers_x_coords_values.shape == (OUTPUT_GRID_N_COLUMNS,)\n",
    "    centers_y_coords_values = arange(\n",
    "        start=int(OUTPUT_GRID_CELL_N_ROWS / 2),\n",
    "        stop=IMAGE_N_ROWS,\n",
    "        step=OUTPUT_GRID_CELL_N_ROWS\n",
    "    )\n",
    "    assert centers_y_coords_values.shape == (OUTPUT_GRID_N_ROWS,)\n",
    "\n",
    "    # x and y possible values spanned by grid cell top-left corners:\n",
    "    corners_x_coords_values = arange(\n",
    "        start=0,\n",
    "        stop=IMAGE_N_COLUMNS,\n",
    "        step=OUTPUT_GRID_CELL_N_COLUMNS\n",
    "    )\n",
    "    assert corners_x_coords_values.shape == (OUTPUT_GRID_N_COLUMNS,)\n",
    "    corners_y_coords_values = arange(\n",
    "        start=0,\n",
    "        stop=IMAGE_N_ROWS,\n",
    "        step=OUTPUT_GRID_CELL_N_ROWS\n",
    "    )\n",
    "    assert corners_y_coords_values.shape == (OUTPUT_GRID_N_ROWS,)\n",
    "\n",
    "    # grid of cells containing the respective center x and y coordinates each:\n",
    "    centers_xy_coords = stack(\n",
    "        arrays=meshgrid(centers_x_coords_values, centers_y_coords_values),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    # grid of cells containing the respective top-left corner x and y\n",
    "    # coordinates each:\n",
    "    corners_xy_coords = stack(\n",
    "        arrays=meshgrid(corners_x_coords_values, corners_y_coords_values),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        centers_xy_coords,\n",
    "        corners_xy_coords\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_weights_to_balance_anchors_emptiness() -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Return the weights, for the loss function terms, that balance full vs\n",
    "    empty anchors.\n",
    "    \"\"\"\n",
    "    average_n_full_anchors_per_image = (\n",
    "        AVERAGE_N_BOUNDING_BOXES_PER_IMAGE / N_ANCHORS_PER_IMAGE\n",
    "    )\n",
    "    average_n_empty_anchors_per_image = (\n",
    "        N_ANCHORS_PER_IMAGE - average_n_full_anchors_per_image\n",
    "    )\n",
    "\n",
    "    full_anchors_weight = 1 / average_n_full_anchors_per_image\n",
    "    empty_anchors_weight = 1  / average_n_empty_anchors_per_image\n",
    "\n",
    "    weights_sum = full_anchors_weight + empty_anchors_weight\n",
    "    \n",
    "    normalized_full_anchors_weight = full_anchors_weight / weights_sum\n",
    "    normalized_empty_anchors_weight = empty_anchors_weight / weights_sum\n",
    "\n",
    "    return normalized_full_anchors_weight, normalized_empty_anchors_weight\n",
    "\n",
    "\n",
    "AVERAGE_N_BOUNDING_BOXES_PER_IMAGE = 0.51\n",
    "\n",
    "DATA_TYPE_FOR_INPUTS = tf_uint8\n",
    "DATA_TYPE_FOR_OUTPUTS = tf_float32\n",
    "\n",
    "DOWNSAMPLING_STEPS = 4\n",
    "\n",
    "IMAGE_N_CHANNELS = 3\n",
    "IMAGE_N_COLUMNS = 1280\n",
    "IMAGE_N_ROWS = 720\n",
    "\n",
    "N_OUTPUTS_PER_ANCHOR = 5\n",
    "\n",
    "ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS = (\n",
    "    (0.6, 0.4),\n",
    "    (0.5, 0.5),\n",
    "    # (0.4, 0.6)  # NOTE: empirically observed: this anchor is less relevant\n",
    ")  # TODO: choose these based on the dataset\n",
    "assert all(\n",
    "    [\n",
    "        (weight[0] + weight[1] == 1) for weight in\n",
    "        ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS\n",
    "    ]\n",
    ")\n",
    "N_ANCHORS_PER_CELL = len(\n",
    "    ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS\n",
    ")\n",
    "\n",
    "OUTPUT_GRID_CELL_N_COLUMNS = 16  # NOTE: this may vary with the architecture\n",
    "OUTPUT_GRID_CELL_N_ROWS = 16  # NOTE: this may vary with the architecture\n",
    "# NOTE: common divisors of 1280 and 720: {1, 2, 4, 5, 8, 10, 16, 20, 40, 80},\n",
    "# and the ones that respect the training-plus-validation set bounding boxes'\n",
    "# distinction when using a single anchor are: {1, 2, 4, 5, 8, 10, 16}\n",
    "\n",
    "OUTPUT_GRID_N_COLUMNS = int(IMAGE_N_COLUMNS / OUTPUT_GRID_CELL_N_COLUMNS)\n",
    "OUTPUT_GRID_N_ROWS = int(IMAGE_N_ROWS / OUTPUT_GRID_CELL_N_ROWS)\n",
    "\n",
    "N_ANCHORS_PER_IMAGE = (\n",
    "    OUTPUT_GRID_N_COLUMNS * OUTPUT_GRID_N_ROWS * N_ANCHORS_PER_CELL\n",
    ")\n",
    "\n",
    "(\n",
    "    OUTPUT_GRID_CELL_CENTERS_XY_COORDS,\n",
    "    OUTPUT_GRID_CELL_CORNERS_XY_COORDS\n",
    ") = compute_grid_cell_centers_xy_coords()\n",
    "\n",
    "(\n",
    "    LOSS_CONTRIBUTE_IMPORTANCE_OF_FULL_ANCHORS,\n",
    "    LOSS_CONTRIBUTE_IMPORTANCE_OF_EMPTY_ANCHORS\n",
    ") = compute_weights_to_balance_anchors_emptiness()\n",
    "# FIXME: is this balancing reasonable?  with 0.999999990162037 vs 9.837962962962963e-09,\n",
    "# using float32 will truncate the second term to 0!!\n",
    "(\n",
    "    LOSS_CONTRIBUTE_IMPORTANCE_OF_FULL_ANCHORS,\n",
    "    LOSS_CONTRIBUTE_IMPORTANCE_OF_EMPTY_ANCHORS\n",
    ") = (0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae3744",
   "metadata": {},
   "source": [
    "#### Samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769430fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample and label extraction from the raw dataset files, inspection and\n",
    "preprocessing for feeding the model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from csv import reader as csv_reader\n",
    "from itertools import combinations\n",
    "from json import loads as json_loads\n",
    "from math import sqrt\n",
    "from os import getcwd, pardir\n",
    "from os.path import join as path_join\n",
    "from typing import Dict, List, Tuple\n",
    "from cv2 import determinant\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.pyplot import (\n",
    "    clf as plt_clf,\n",
    "    close as plt_close,\n",
    "    figure as plt_figure,\n",
    "    hist as plt_hist,\n",
    "    get_current_fig_manager,\n",
    "    pause as plt_pause,\n",
    "    savefig as plt_savefig,\n",
    "    show as plt_show,\n",
    "    subplots,\n",
    "    title as plt_title,\n",
    "    xticks as plt_xticks\n",
    ")\n",
    "from numpy import argmin, sum as np_sum, unravel_index, zeros\n",
    "# pylint: disable=import-error\n",
    "from tensorflow import convert_to_tensor, py_function, Tensor\n",
    "from tensorflow.data import AUTOTUNE, Dataset\n",
    "from tensorflow.io import decode_jpeg, read_file\n",
    "# pylint: enable=import-error\n",
    "\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import (\n",
    "        ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS,\n",
    "        DATA_TYPE_FOR_INPUTS,\n",
    "        DATA_TYPE_FOR_OUTPUTS,\n",
    "        IMAGE_N_COLUMNS,\n",
    "        IMAGE_N_ROWS,\n",
    "        N_ANCHORS_PER_CELL,\n",
    "        N_OUTPUTS_PER_ANCHOR,\n",
    "        OUTPUT_GRID_CELL_CENTERS_XY_COORDS,\n",
    "        OUTPUT_GRID_CELL_CORNERS_XY_COORDS,\n",
    "        OUTPUT_GRID_CELL_N_COLUMNS,\n",
    "        OUTPUT_GRID_CELL_N_ROWS,\n",
    "        OUTPUT_GRID_N_COLUMNS,\n",
    "        OUTPUT_GRID_N_ROWS\n",
    "    )\n",
    "\n",
    "\n",
    "MINI_BATCH_SIZE = 4  # TODO\n",
    "VALIDATION_SET_PORTION_OF_DATA = 0.3\n",
    "\n",
    "if __name__ != 'main_by_mattia':\n",
    "    DATASET_DIR = path_join(\n",
    "        getcwd(),\n",
    "        pardir,\n",
    "        'tensorflow-great-barrier-reef'\n",
    "    )\n",
    "else:\n",
    "    DATASET_DIR = path_join(\n",
    "        getcwd(),\n",
    "        pardir,\n",
    "        'input',\n",
    "        'tensorflow-great-barrier-reef'\n",
    "    )\n",
    "CACHE_DIR = path_join(\n",
    "    getcwd(),\n",
    "    'cache'\n",
    ")\n",
    "CACHE_FILE_PATH_FOR_STATISTICS_SET = path_join(\n",
    "    CACHE_DIR,\n",
    "    'statistics.tmp'\n",
    ")\n",
    "CACHE_FILE_PATH_FOR_TRAINING_SET = path_join(\n",
    "    CACHE_DIR,\n",
    "    'training.tmp'\n",
    ")\n",
    "CACHE_FILE_PATH_FOR_VALIDATION_SET = path_join(\n",
    "    CACHE_DIR,\n",
    "    'validation.tmp'\n",
    ")\n",
    "LABELS_FILE_PATH = path_join(\n",
    "    DATASET_DIR,\n",
    "    'train.csv'\n",
    ")\n",
    "PICTURES_DIR = path_join(\n",
    "    getcwd(),\n",
    "    pardir,\n",
    "    'docs',\n",
    "    'pictures'\n",
    ")\n",
    "\n",
    "SHOW_BOUNDING_BOXES_STATISTICS = False\n",
    "SHOW_DATASET_MOVIES = False\n",
    "\n",
    "\n",
    "def get_cell_containing_bounding_box_center(\n",
    "        center_absolute_x_and_y_coords: Tuple[float, float]\n",
    ") -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Find the output grid cell whose center is closest to the bounding box one\n",
    "    (the input one), returning the grid cell's row and column indexes and its\n",
    "    x and y coordinates.\n",
    "    ---\n",
    "        Output Shape:\n",
    "            - (4,)\n",
    "    ---\n",
    "        Output Meaning:\n",
    "            - [\n",
    "                grid cell row index,\n",
    "                grid cell column index,\n",
    "                x coordindate of cell center,\n",
    "                y coordindate of cell center\n",
    "            ]\n",
    "    \"\"\"\n",
    "    (\n",
    "        grid_cell_enclosing_bounding_box_center_row_index,\n",
    "        grid_cell_enclosing_bounding_box_center_column_index\n",
    "    ) = unravel_index(\n",
    "        indices=argmin(  # NOTE: in case of equivalent minima, the first one is picked\n",
    "            # grid of squared element-wise center pairs' distances representing\n",
    "            # the minimized objective to find the closest grid cell center:\n",
    "            a=np_sum(\n",
    "                a=(\n",
    "                    (OUTPUT_GRID_CELL_CENTERS_XY_COORDS -\n",
    "                    center_absolute_x_and_y_coords) ** 2\n",
    "                ),\n",
    "                axis=-1\n",
    "            )\n",
    "        ),\n",
    "        shape=(OUTPUT_GRID_N_ROWS, OUTPUT_GRID_N_COLUMNS),\n",
    "        order='C'\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        # [grid cell row index, grid cell column index]:\n",
    "        [\n",
    "            grid_cell_enclosing_bounding_box_center_row_index,\n",
    "            grid_cell_enclosing_bounding_box_center_column_index\n",
    "        ] +\n",
    "        # [x coordindate of cell center, y coordindate of cell center]:\n",
    "        OUTPUT_GRID_CELL_CENTERS_XY_COORDS[\n",
    "            grid_cell_enclosing_bounding_box_center_row_index,\n",
    "            grid_cell_enclosing_bounding_box_center_column_index,\n",
    "            :\n",
    "        ].tolist()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_index_of_anchor_with_closest_aspect_ratio(\n",
    "        absolute_width: float,\n",
    "        absolute_height: float\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Return the index of the anchor whose aspect ratio is close to the\n",
    "    considered bounding box represented by the input relative width and\n",
    "    relative height.\n",
    "    \"\"\"\n",
    "    width_weight = absolute_width / (absolute_width + absolute_height)\n",
    "    height_weight = absolute_height / (absolute_width + absolute_height)\n",
    "\n",
    "    return (\n",
    "        ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS.index(\n",
    "            sorted(\n",
    "                ANCHORS_WIDTH_VS_HEIGHT_WEIGHTS,\n",
    "                key=lambda width_vs_height_weights: (\n",
    "                    abs(width_vs_height_weights[0] - width_weight) +\n",
    "                    abs(width_vs_height_weights[1] - height_weight)\n",
    "                ),\n",
    "                reverse=False\n",
    "            )[0]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def dataset_of_samples_and_bounding_boxes() -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a TensorFlow dataset that can iterate over all the dataset samples\n",
    "    and the respective labels containing bounding boxes.\n",
    "    \"\"\"\n",
    "    image_paths_dataset = Dataset.from_tensor_slices(\n",
    "        tensors=[*IMAGE_PATHS_TO_BOUNDING_BOXES]  # only keys included\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        image_paths_dataset\n",
    "        .map(\n",
    "            map_func=lambda image_path: py_function(\n",
    "                func=load_sample_and_get_bounding_boxes,\n",
    "                inp=[image_path],\n",
    "                Tout=(DATA_TYPE_FOR_INPUTS, DATA_TYPE_FOR_OUTPUTS)\n",
    "            ),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "            deterministic=True\n",
    "        )\n",
    "        # optimizing performances by caching end-results:\n",
    "        # .cache(filename=CACHE_FILE_PATH_FOR_STATISTICS_SET)  # FIXME\n",
    "        # optimizing performances by pre-fetching final elements:\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "def dataset_of_samples_and_model_outputs(shuffle: bool = True) -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a TensorFlow dataset that can iterate over all the dataset samples\n",
    "    and the respective labels containing model outputs, in a shuffled order.\n",
    "    \"\"\"\n",
    "    image_paths_dataset = Dataset.from_tensor_slices(\n",
    "        tensors=[*IMAGE_PATHS_TO_MODEL_OUTPUTS]  # only keys included\n",
    "    )\n",
    "\n",
    "    # NOTE: shuffling is carried out here to have acceptable performance with\n",
    "    # a shuffling buffer size that allows to take the whole set into memory\n",
    "    # in case shuffling is desired:\n",
    "    if shuffle:\n",
    "        image_paths_dataset.shuffle(\n",
    "            buffer_size=N_TRAINING_PLUS_VALIDATION_SAMPLES,\n",
    "            seed=0,\n",
    "            reshuffle_each_iteration=False  # NOTE: relevant when splitting\n",
    "        )\n",
    "\n",
    "    # NOTE: further optimizations on this dataset - that is the one employed\n",
    "    # for training/validation - are carried out later, after\n",
    "    # training/validation splitting and batching, to optimize performances\n",
    "\n",
    "    return image_paths_dataset.map(\n",
    "        map_func=lambda image_path: py_function(\n",
    "            func=load_sample_and_get_model_outputs,\n",
    "            inp=[image_path],\n",
    "            Tout=(DATA_TYPE_FOR_INPUTS, DATA_TYPE_FOR_OUTPUTS)\n",
    "        ),\n",
    "        num_parallel_calls=AUTOTUNE,\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "\n",
    "def inspect_bounding_boxes_statistics_on_training_n_validation_set() -> None:\n",
    "    \"\"\"\n",
    "    Inspect and print the following statistics of bounding boxes in the\n",
    "    training-plus-validation set:\n",
    "        - total number of bounding boxes\n",
    "        - total number of images\n",
    "        - average number of bounding boxes per image\n",
    "        - minimum number of bounding boxes per image\n",
    "        - maximum number of bounding boxes per image\n",
    "        - total number of empty images\n",
    "        - average bounding box height [pixels]\n",
    "        - average bounding box width [pixels]\n",
    "        - average bounding boxes' centers distance [pixels]\n",
    "        - average bounding boxes' centers x-coord distance [pixels]\n",
    "        - average bounding boxes' centers y-coord distance [pixels]\n",
    "        - minimum bounding box height [pixels]\n",
    "        - minimum bounding box width [pixels]\n",
    "        - minimum bounding boxes' centers distance [pixels]\n",
    "        - minimum bounding boxes' centers x-coord distance [pixels]\n",
    "        - minimum bounding boxes' centers y-coord distance [pixels]\n",
    "        - maximum bounding box height [pixels]\n",
    "        - maximum bounding box width [pixels]\n",
    "        - maximum bounding boxes' centers distance [pixels]\n",
    "        - maximum bounding boxes' centers x-coord distance [pixels]\n",
    "        - maximum bounding boxes' centers y-coord distance [pixels]\n",
    "        - histogram of number of bounding boxes per image\n",
    "        - histogram of bounding boxes' centers distance [pixels]\n",
    "        - histogram of bounding boxes' centers x-coord distance [pixels]\n",
    "        - histogram of bounding boxes' centers y-coord distance [pixels]\n",
    "    \"\"\"\n",
    "    total_n_images = len(IMAGE_PATHS_TO_BOUNDING_BOXES)\n",
    "\n",
    "    bounding_boxes_centers_distances_for_histogram = []\n",
    "    bounding_boxes_centers_x_coord_distances_for_histogram = []\n",
    "    bounding_boxes_centers_y_coord_distances_for_histogram = []\n",
    "    cumulative_bounding_box_height = 0\n",
    "    cumulative_bounding_box_width = 0\n",
    "    cumulative_bounding_boxes_centers_distance = 0\n",
    "    cumulative_bounding_boxes_centers_x_coord_distance = 0\n",
    "    cumulative_bounding_boxes_centers_y_coord_distance = 0\n",
    "    minimum_bounding_box_height = 99999\n",
    "    minimum_bounding_box_width = 99999\n",
    "    minimum_bounding_boxes_centers_distance = 99999\n",
    "    minimum_bounding_boxes_centers_x_coord_distance = 99999\n",
    "    minimum_bounding_boxes_centers_y_coord_distance = 99999\n",
    "    minimum_n_bounding_boxes_per_image = 99999\n",
    "    maximum_bounding_box_height = 0\n",
    "    maximum_bounding_box_width = 0\n",
    "    maximum_bounding_boxes_centers_distance = 0\n",
    "    maximum_bounding_boxes_centers_x_coord_distance = 0\n",
    "    maximum_bounding_boxes_centers_y_coord_distance = 0\n",
    "    maximum_n_bounding_boxes_per_image = 0\n",
    "    n_bounding_boxes_per_image_for_histogram = []\n",
    "    total_n_bounding_boxes = 0\n",
    "    total_n_bounding_boxes_center_distances_cumulated = 0\n",
    "    total_n_empty_images = 0\n",
    "\n",
    "    for image_bounding_boxes in IMAGE_PATHS_TO_BOUNDING_BOXES.values():\n",
    "        n_bounding_boxes = len(image_bounding_boxes)\n",
    "        n_bounding_boxes_per_image_for_histogram.append(\n",
    "            n_bounding_boxes\n",
    "        )\n",
    "\n",
    "        total_n_bounding_boxes += n_bounding_boxes\n",
    "        if n_bounding_boxes < minimum_n_bounding_boxes_per_image:\n",
    "            minimum_n_bounding_boxes_per_image = n_bounding_boxes\n",
    "        if n_bounding_boxes > maximum_n_bounding_boxes_per_image:\n",
    "            maximum_n_bounding_boxes_per_image = n_bounding_boxes\n",
    "        if n_bounding_boxes == 0:\n",
    "            total_n_empty_images += 1\n",
    "\n",
    "        bounding_boxes_centers_x_and_y_coords = []\n",
    "        for bounding_box in image_bounding_boxes:\n",
    "            cumulative_bounding_box_height += bounding_box['height']\n",
    "            cumulative_bounding_box_width += bounding_box['width']\n",
    "\n",
    "            bounding_boxes_centers_x_and_y_coords.append(\n",
    "                {\n",
    "                    'x': (bounding_box['x'] + bounding_box['width']) / 2,\n",
    "                    'y': (bounding_box['y'] + bounding_box['height']) / 2\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if bounding_box['height'] < minimum_bounding_box_height:\n",
    "                minimum_bounding_box_height = bounding_box['height']\n",
    "            if bounding_box['width'] < minimum_bounding_box_width:\n",
    "                minimum_bounding_box_width = bounding_box['width']\n",
    "\n",
    "            if bounding_box['height'] > maximum_bounding_box_height:\n",
    "                maximum_bounding_box_height = bounding_box['height']\n",
    "            if bounding_box['width'] > maximum_bounding_box_width:\n",
    "                maximum_bounding_box_width = bounding_box['width']\n",
    "        \n",
    "        if n_bounding_boxes > 1:\n",
    "            for centers_coords_pair in combinations(\n",
    "                    iterable=bounding_boxes_centers_x_and_y_coords,\n",
    "                    r=2\n",
    "            ):\n",
    "                total_n_bounding_boxes_center_distances_cumulated += 1\n",
    "\n",
    "                x_coord_difference = abs(\n",
    "                    centers_coords_pair[0]['x'] - centers_coords_pair[1]['x']\n",
    "                )\n",
    "                y_coord_difference = abs(\n",
    "                    centers_coords_pair[0]['y'] - centers_coords_pair[1]['y']\n",
    "                )\n",
    "                distance = sqrt(\n",
    "                    x_coord_difference**2 + y_coord_difference**2\n",
    "                )\n",
    "\n",
    "                bounding_boxes_centers_distances_for_histogram.append(\n",
    "                    distance\n",
    "                )\n",
    "                bounding_boxes_centers_x_coord_distances_for_histogram.append(\n",
    "                    x_coord_difference\n",
    "                )\n",
    "                bounding_boxes_centers_y_coord_distances_for_histogram.append(\n",
    "                    y_coord_difference\n",
    "                )\n",
    "\n",
    "                cumulative_bounding_boxes_centers_distance += (\n",
    "                    distance\n",
    "                )\n",
    "                cumulative_bounding_boxes_centers_x_coord_distance += (\n",
    "                    x_coord_difference\n",
    "                )\n",
    "                cumulative_bounding_boxes_centers_y_coord_distance += (\n",
    "                    y_coord_difference\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                        distance <\n",
    "                        minimum_bounding_boxes_centers_distance\n",
    "                ):\n",
    "                    minimum_bounding_boxes_centers_distance = (\n",
    "                        distance\n",
    "                    )\n",
    "                if (\n",
    "                        x_coord_difference <\n",
    "                        minimum_bounding_boxes_centers_x_coord_distance\n",
    "                ):\n",
    "                    minimum_bounding_boxes_centers_x_coord_distance = (\n",
    "                        x_coord_difference\n",
    "                    )\n",
    "                if (\n",
    "                        y_coord_difference <\n",
    "                        minimum_bounding_boxes_centers_y_coord_distance\n",
    "                ):\n",
    "                    minimum_bounding_boxes_centers_y_coord_distance = (\n",
    "                        y_coord_difference\n",
    "                    )\n",
    "\n",
    "                if (\n",
    "                        distance >\n",
    "                        maximum_bounding_boxes_centers_distance\n",
    "                ):\n",
    "                    maximum_bounding_boxes_centers_distance = (\n",
    "                        distance\n",
    "                    )\n",
    "                if (\n",
    "                        x_coord_difference >\n",
    "                        maximum_bounding_boxes_centers_x_coord_distance\n",
    "                ):\n",
    "                    maximum_bounding_boxes_centers_x_coord_distance = (\n",
    "                        x_coord_difference\n",
    "                    )\n",
    "                if (\n",
    "                    y_coord_difference > maximum_bounding_boxes_centers_y_coord_distance\n",
    "                ):\n",
    "                    maximum_bounding_boxes_centers_y_coord_distance = (\n",
    "                        y_coord_difference\n",
    "                    )\n",
    "\n",
    "    print('- ' * 30)\n",
    "    print(\"Bounding Boxes' Statistics:\")\n",
    "\n",
    "    print(\n",
    "        \"\\t- total number of bounding boxes:\",\n",
    "        total_n_bounding_boxes\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- total number of images:\",\n",
    "        total_n_images\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average number of bounding boxes per image:\",\n",
    "        round(number=total_n_bounding_boxes/total_n_images, ndigits=2)\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum number of bounding boxes per image:\",\n",
    "        minimum_n_bounding_boxes_per_image\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum number of bounding boxes per image:\",\n",
    "        maximum_n_bounding_boxes_per_image\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- total number of empty images:\",\n",
    "        total_n_empty_images\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding box height [pixels]:\",\n",
    "        round(\n",
    "            number=cumulative_bounding_box_height/total_n_bounding_boxes,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding box width [pixels]:\",\n",
    "        round(\n",
    "            number=cumulative_bounding_box_width/total_n_bounding_boxes,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding boxes' centers distance [pixels]:\",\n",
    "        round(\n",
    "            number=(\n",
    "                cumulative_bounding_boxes_centers_distance /\n",
    "                total_n_bounding_boxes_center_distances_cumulated\n",
    "            ),\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding boxes' centers x-coord distance [pixels]:\",\n",
    "        round(\n",
    "            number=(\n",
    "                cumulative_bounding_boxes_centers_x_coord_distance /\n",
    "                total_n_bounding_boxes_center_distances_cumulated\n",
    "            ),\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding boxes' centers y-coord distance [pixels]:\",\n",
    "        round(\n",
    "            number=(\n",
    "                cumulative_bounding_boxes_centers_y_coord_distance /\n",
    "                total_n_bounding_boxes_center_distances_cumulated\n",
    "            ),\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding box height [pixels]:\",\n",
    "        minimum_bounding_box_height\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding box width [pixels]:\",\n",
    "        minimum_bounding_box_width\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding boxes' centers distance [pixels]:\",\n",
    "        round(\n",
    "            number=minimum_bounding_boxes_centers_distance,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding boxes' centers x-coord distance [pixels]:\",\n",
    "        minimum_bounding_boxes_centers_x_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding boxes' centers y-coord distance [pixels]:\",\n",
    "        minimum_bounding_boxes_centers_y_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding box height [pixels]:\",\n",
    "        maximum_bounding_box_height\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding box width [pixels]:\",\n",
    "        maximum_bounding_box_width\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding boxes' centers distance [pixels]:\",\n",
    "        round(\n",
    "            number=maximum_bounding_boxes_centers_distance,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding boxes' centers x-coord distance [pixels]:\",\n",
    "        maximum_bounding_boxes_centers_x_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding boxes' centers y-coord distance [pixels]:\",\n",
    "        maximum_bounding_boxes_centers_y_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of number of bounding boxes per image: see plot\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of bounding boxes' centers distance [pixels]: \" +\n",
    "        \"see plot\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of bounding boxes' centers x-coord distance [pixels]: \" +\n",
    "        \"see plot\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of bounding boxes' centers y-coord distance [pixels]: \" +\n",
    "        \"see plot\"\n",
    "    )\n",
    "\n",
    "    plt_figure()\n",
    "\n",
    "    what_it_represent = \"Histogram of Number of Bounding Boxes per Image\"\n",
    "    plt_hist(\n",
    "        x=n_bounding_boxes_per_image_for_histogram,\n",
    "        bins=maximum_n_bounding_boxes_per_image,\n",
    "        align='left',\n",
    "        color='skyblue',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(range(maximum_n_bounding_boxes_per_image))\n",
    "    )\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    what_it_represent = (\n",
    "        \"Histogram of Bounding Boxes' Centers Distance [pixels]\"\n",
    "    )\n",
    "    plt_hist(\n",
    "        x=bounding_boxes_centers_distances_for_histogram,\n",
    "        bins=list(range(int(sqrt(IMAGE_N_COLUMNS**2 + IMAGE_N_ROWS**2)))),\n",
    "        align='left',\n",
    "        color='chartreuse',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(\n",
    "            range(0, int(sqrt(IMAGE_N_COLUMNS**2 + IMAGE_N_ROWS**2)), 20)\n",
    "        ),\n",
    "        fontsize=6,\n",
    "        rotation=90\n",
    "    )\n",
    "    figure_manager = get_current_fig_manager()\n",
    "    figure_manager.resize(*figure_manager.window.maxsize())\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    what_it_represent = (\n",
    "        \"Histogram of Bounding Boxes' Centers X-Coordinate Distance [pixels]\"\n",
    "    )\n",
    "    plt_hist(\n",
    "        x=bounding_boxes_centers_x_coord_distances_for_histogram,\n",
    "        bins=list(range(IMAGE_N_COLUMNS)),\n",
    "        align='left',\n",
    "        color='mediumslateblue',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(range(0, IMAGE_N_COLUMNS, 20)),\n",
    "        fontsize=6,\n",
    "        rotation=90\n",
    "    )\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    what_it_represent = (\n",
    "        \"Histogram of Bounding Boxes' Centers Y-Coordinate Distance [pixels]\"\n",
    "    )\n",
    "    plt_hist(\n",
    "        x=bounding_boxes_centers_y_coord_distances_for_histogram,\n",
    "        bins=list(range(IMAGE_N_ROWS)),\n",
    "        align='left',\n",
    "        color='violet',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(range(0, IMAGE_N_ROWS, 20)),\n",
    "        fontsize=6,\n",
    "        rotation=90\n",
    "    )\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    plt_close()\n",
    "\n",
    "    print('- ' * 30)\n",
    "\n",
    "\n",
    "def label_line_to_image_path_2_bounding_boxes_and_2_model_output(\n",
    "        csv_label_line_segments: List[str]\n",
    ") -> Tuple[\n",
    "        Dict[str, List[Dict[str, int]]],\n",
    "        Dict[str, List[List[Tuple[int, int, int, int]]]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Turn any line of the CSV labels file from the original format\n",
    "    'video_id,sequence,video_frame,sequence_frame,image_id,annotations' into\n",
    "    two dictionariies: the former with the respective image file path as key\n",
    "    and the respective bounding boxes as value, the latter with the respective\n",
    "    image file path as key and the respective model outputs as value.\n",
    "    \"\"\"\n",
    "    image_path = path_join(\n",
    "        DATASET_DIR,\n",
    "        'train_images',\n",
    "        'video_' + csv_label_line_segments[0],\n",
    "        csv_label_line_segments[2] + '.jpg'\n",
    "    )\n",
    "    bounding_boxes = json_loads(\n",
    "        csv_label_line_segments[5]\n",
    "        .replace('\"', '\"\"\"')\n",
    "        .replace(\"'\", '\"')\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            bytes(image_path, 'utf-8'): bounding_boxes\n",
    "        },\n",
    "        {\n",
    "            bytes(image_path, 'utf-8'): turn_bounding_boxes_to_model_outputs(\n",
    "                raw_bounding_boxes=bounding_boxes\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def load_labels_as_paths_to_bounding_boxes_and_model_outputs_dicts() -> Tuple[\n",
    "        Dict[str, List[Dict[str, int]]],\n",
    "        Dict[str, List[List[Tuple[int, int, int, int]]]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the labels' information from the CSV file and return them as a two\n",
    "    dictionaries, the former associating image file paths to respective\n",
    "    bounding boxes and the latter associating image file paths to respective\n",
    "    model outputs.\n",
    "    \"\"\"\n",
    "    image_paths_to_bounding_boxes = {}\n",
    "    image_paths_to_model_outputs = {}\n",
    "\n",
    "    with open(LABELS_FILE_PATH, 'r') as labels_file:\n",
    "        labels_reader = csv_reader(\n",
    "            labels_file,\n",
    "            delimiter=',',\n",
    "            quotechar='\"'\n",
    "        )\n",
    "\n",
    "        for line_index, line_segments in enumerate(labels_reader):\n",
    "            if line_index == 0:\n",
    "                continue\n",
    "\n",
    "            # turning the label from the raw format into processed\n",
    "            # dictionaries to retrieve bounding boxes and model outputs of\n",
    "            # images easily from respective image file paths:\n",
    "            (\n",
    "                image_path_to_bounding_boxes,\n",
    "                image_path_to_model_outputs\n",
    "            ) = label_line_to_image_path_2_bounding_boxes_and_2_model_output(\n",
    "                csv_label_line_segments=line_segments\n",
    "            )\n",
    "            image_paths_to_bounding_boxes.update(image_path_to_bounding_boxes)\n",
    "            image_paths_to_model_outputs.update(image_path_to_model_outputs)\n",
    "\n",
    "    return (image_paths_to_bounding_boxes, image_paths_to_model_outputs)\n",
    "\n",
    "\n",
    "def load_sample_and_get_bounding_boxes(image_path: Tensor) -> Tuple[\n",
    "        Tensor, Tensor\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the sample and get the label - representing bounding boxes - of the\n",
    "    image represented by the input path.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        decode_jpeg(\n",
    "            contents=read_file(\n",
    "                filename=image_path\n",
    "            )\n",
    "        ),\n",
    "        convert_to_tensor(\n",
    "            # bounding boxes as network output values:\n",
    "            value=[\n",
    "                [\n",
    "                    bounding_box_dict['x'],\n",
    "                    bounding_box_dict['y'],\n",
    "                    bounding_box_dict['width'],\n",
    "                    bounding_box_dict['height']\n",
    "                ] for bounding_box_dict in\n",
    "                IMAGE_PATHS_TO_BOUNDING_BOXES[image_path.numpy()]\n",
    "            ],\n",
    "            dtype=DATA_TYPE_FOR_OUTPUTS\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def load_sample_and_get_model_outputs(image_path: Tensor) -> Tuple[\n",
    "        Tensor, Tensor\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the sample and get the label - representing model outputs - of the\n",
    "    image represented by the input path.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        decode_jpeg(\n",
    "            contents=read_file(\n",
    "                filename=image_path\n",
    "            )\n",
    "        ),\n",
    "        convert_to_tensor(\n",
    "            # bounding boxes as network output values:\n",
    "            value=IMAGE_PATHS_TO_MODEL_OUTPUTS[image_path.numpy()],\n",
    "            dtype=DATA_TYPE_FOR_OUTPUTS\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set: Dataset\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Split the input dataset into a training set and a validation set, both\n",
    "    already divided into mini-batches.\n",
    "    \"\"\"\n",
    "    n_samples_in_validation_set = int(\n",
    "        VALIDATION_SET_PORTION_OF_DATA * N_TRAINING_PLUS_VALIDATION_SAMPLES\n",
    "    )\n",
    "    n_samples_in_training_set = (\n",
    "        N_TRAINING_PLUS_VALIDATION_SAMPLES - n_samples_in_validation_set\n",
    "    )\n",
    "\n",
    "    training_set = (\n",
    "        training_plus_validation_set\n",
    "        # selecting only the training samples and labels:\n",
    "        .take(count=n_samples_in_training_set)\n",
    "        # creating mini-batches:\n",
    "        .batch(\n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            drop_remainder=False,\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "            deterministic=True\n",
    "        )\n",
    "        # optimizing performances by caching end-results:\n",
    "        # .cache(filename=CACHE_FILE_PATH_FOR_TRAINING_SET)  # FIXME\n",
    "        # optimizing performances by pre-fetching final elements:\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )\n",
    "    validation_set = (\n",
    "        training_plus_validation_set\n",
    "        # selecting only the validation samples and labels:\n",
    "        .skip(count=n_samples_in_training_set)\n",
    "        .take(count=n_samples_in_validation_set)\n",
    "        # creating mini-batches:\n",
    "        .batch(\n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            drop_remainder=False,\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "            deterministic=True\n",
    "        )\n",
    "        # optimizing performances by caching end-results:\n",
    "        # .cache(filename=CACHE_FILE_PATH_FOR_TRAINING_SET)  # FIXME\n",
    "        # optimizing performances by pre-fetching final elements:\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return (training_set, validation_set)\n",
    "\n",
    "\n",
    "def show_dataset_as_movie(\n",
    "        ordered_samples_and_labels: Dataset,\n",
    "        bounding_boxes_or_model_outputs: str = 'bounding_boxes'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Show the dataset images frame by frame, reconstructing the video\n",
    "    sequences, with boundinx boxes contained displayed over the respective\n",
    "    sample/frame.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        bounding_boxes_or_model_outputs in ('bounding_boxes', 'model_outputs')\n",
    "    ), \"Invalid 'bounding_boxes_or_model_outputs' input.\"\n",
    "\n",
    "    _, axes = subplots(1, 1)\n",
    "\n",
    "    # for each sample-label pair, a frame fusing them together is shown:\n",
    "    for index, sample_and_label in enumerate(ordered_samples_and_labels):\n",
    "        if index % 1000 == 0:\n",
    "            print(f\"{index} frames shown\")\n",
    "\n",
    "        # clearing axes from the previous frame information:\n",
    "        axes.clear()\n",
    "\n",
    "        # showing the image:\n",
    "        axes.imshow(sample_and_label[0].numpy())\n",
    "\n",
    "        # showing labels...\n",
    "\n",
    "        # ... either as bounding boxes:\n",
    "        if bounding_boxes_or_model_outputs == 'bounding_boxes':\n",
    "            # for each bounding box:\n",
    "            for bounding_box in sample_and_label[1].numpy().tolist():\n",
    "                # drawing the bounding box over the frame image:\n",
    "                axes.add_patch(\n",
    "                    p=Rectangle(\n",
    "                        xy=(bounding_box[0], bounding_box[1]),\n",
    "                        width=bounding_box[2],\n",
    "                        height=bounding_box[3],\n",
    "                        linewidth=2,\n",
    "                        edgecolor='#00ff00',\n",
    "                        facecolor='none'\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # ... or as model output grid cells:\n",
    "        elif bounding_boxes_or_model_outputs == 'model_outputs':\n",
    "            # for each model output grid cell whose label contains anchors:\n",
    "            for cell_row_index in range(OUTPUT_GRID_N_ROWS):\n",
    "                for cell_column_index in range(OUTPUT_GRID_N_COLUMNS):\n",
    "                    # filtering out grid cells not containing any anchor:\n",
    "                    if (\n",
    "                        sample_and_label[1][\n",
    "                            cell_row_index,\n",
    "                            cell_column_index,\n",
    "                            :,\n",
    "                            :\n",
    "                        ].numpy() == zeros(\n",
    "                            shape=(N_ANCHORS_PER_CELL, N_OUTPUTS_PER_ANCHOR)\n",
    "                        )\n",
    "                    ).all():\n",
    "                        continue\n",
    "\n",
    "                    # highlighting the full cell over the frame image:\n",
    "                    axes.add_patch(\n",
    "                        p=Rectangle(\n",
    "                            xy=(\n",
    "                                OUTPUT_GRID_CELL_CORNERS_XY_COORDS[\n",
    "                                    cell_row_index,\n",
    "                                    cell_column_index\n",
    "                                ]\n",
    "                            ),\n",
    "                            width=OUTPUT_GRID_CELL_N_COLUMNS,\n",
    "                            height=OUTPUT_GRID_CELL_N_ROWS,\n",
    "                            linewidth=2,\n",
    "                            edgecolor='#00ff00',\n",
    "                            facecolor='none'\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Ill-conceived code.\")\n",
    "\n",
    "        # making the plot go adeah with the next frame after a small pause for\n",
    "        # better observation:\n",
    "        plt_show(block=False)\n",
    "        plt_pause(interval=0.000001)\n",
    "\n",
    "\n",
    "def turn_bounding_boxes_to_model_outputs(\n",
    "        raw_bounding_boxes: List[Dict[str, int]]\n",
    ") -> Dict[str, List[List[Tuple[int, int, int, int]]]]:\n",
    "    \"\"\"\n",
    "    Turn the input, raw list of bounding boxes' position information into the\n",
    "    equivalent information from the model outputs' perspective, as direct\n",
    "    supervision labels - for a single image.\n",
    "    \"\"\"\n",
    "    labels = zeros(\n",
    "        shape=(\n",
    "            OUTPUT_GRID_N_ROWS,\n",
    "            OUTPUT_GRID_N_COLUMNS,\n",
    "            N_ANCHORS_PER_CELL,\n",
    "            N_OUTPUTS_PER_ANCHOR\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # for each bounding box in the image:\n",
    "    for bounding_box in raw_bounding_boxes:\n",
    "        # getting information about the grid cell that contains its center:\n",
    "        (\n",
    "            cell_row_index,\n",
    "            cell_column_index,\n",
    "            cell_x_coord,\n",
    "            cell_y_coord\n",
    "        ) = get_cell_containing_bounding_box_center(\n",
    "            center_absolute_x_and_y_coords=(\n",
    "                bounding_box['x'] + (bounding_box['width'] / 2),\n",
    "                bounding_box['y'] + (bounding_box['height'] / 2)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # normalizing the boundinx box coordinates:\n",
    "        relative_x_coord = (\n",
    "            (bounding_box['x'] - cell_x_coord) / OUTPUT_GRID_CELL_N_COLUMNS\n",
    "        )\n",
    "        relative_y_coord = (\n",
    "            bounding_box['y'] - cell_y_coord / OUTPUT_GRID_CELL_N_ROWS\n",
    "        )\n",
    "        relative_width = bounding_box['width'] / IMAGE_N_COLUMNS\n",
    "        relative_height = bounding_box['height'] / IMAGE_N_ROWS\n",
    "\n",
    "        # getting the index of the anchor with closest aspect ratio to the\n",
    "        # considered bounding box:\n",
    "        label_anchor_index = get_index_of_anchor_with_closest_aspect_ratio(\n",
    "            absolute_width=bounding_box['width'],\n",
    "            absolute_height=bounding_box['height']\n",
    "        )\n",
    "\n",
    "        # associating the bounding box attributes to the respective anchor\n",
    "        # labels - after checking there are no intrinsic limitations of the\n",
    "        # employed design choices:\n",
    "        label_cannot_be_associated_to_respective_anchor = (\n",
    "            labels[cell_row_index, cell_column_index, label_anchor_index, :] !=\n",
    "            [.0] * N_OUTPUTS_PER_ANCHOR\n",
    "        ).any()\n",
    "        if label_cannot_be_associated_to_respective_anchor:\n",
    "            raise Exception(\n",
    "                f\"Either more than {N_ANCHORS_PER_CELL} anchors or a \" +\n",
    "                \"better output resolution are required, as more bounding \" +\n",
    "                \"boxes than the set number of anchors are falling within \" +\n",
    "                \"the same output cell in this sample.\"\n",
    "            )\n",
    "        labels[cell_row_index, cell_column_index, label_anchor_index, :] = [\n",
    "            1.0,  # FIXME: is this supposed to be just an objectiveness score or an IoU?\n",
    "            relative_x_coord,\n",
    "            relative_y_coord,\n",
    "            relative_width,\n",
    "            relative_height\n",
    "        ]\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "(\n",
    "    IMAGE_PATHS_TO_BOUNDING_BOXES,\n",
    "    IMAGE_PATHS_TO_MODEL_OUTPUTS\n",
    ") = load_labels_as_paths_to_bounding_boxes_and_model_outputs_dicts()\n",
    "\n",
    "N_TRAINING_PLUS_VALIDATION_SAMPLES = len(IMAGE_PATHS_TO_BOUNDING_BOXES)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if SHOW_BOUNDING_BOXES_STATISTICS:\n",
    "        inspect_bounding_boxes_statistics_on_training_n_validation_set()\n",
    "\n",
    "    samples_n_bounding_boxes_dataset = dataset_of_samples_and_bounding_boxes()\n",
    "\n",
    "    if SHOW_DATASET_MOVIES:\n",
    "        show_dataset_as_movie(\n",
    "            ordered_samples_and_labels=samples_n_bounding_boxes_dataset,\n",
    "            bounding_boxes_or_model_outputs='bounding_boxes'\n",
    "        )\n",
    "\n",
    "    samples_n_model_outputs_dataset = dataset_of_samples_and_model_outputs(\n",
    "        # not shuffling when needing adjacent frames for showing the movie:\n",
    "        shuffle=(not SHOW_DATASET_MOVIES)\n",
    "    )\n",
    "\n",
    "    if SHOW_DATASET_MOVIES:\n",
    "        show_dataset_as_movie(\n",
    "            ordered_samples_and_labels=samples_n_model_outputs_dataset,\n",
    "            bounding_boxes_or_model_outputs='model_outputs'\n",
    "        )\n",
    "\n",
    "    (\n",
    "        training_set, validation_set\n",
    "    ) = split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set=samples_n_model_outputs_dataset\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3155ce",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc2b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities for inference time, for converting model outputs to bounding boxes' predictions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "# pylint: disable=import-error\n",
    "from tensorflow import Tensor\n",
    "# pylint: enable=import-error\n",
    "\n",
    "\n",
    "IOU_THRESHOLD_FOR_NON_MAXIMUM_SUPPRESSION = 0.5\n",
    "\n",
    "\n",
    "def convert_bounding_boxes_to_submission_format(\n",
    "        bounding_boxes: Tensor\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_bounding_boxes_from_model_outputs(\n",
    "        model_outputs: Tensor,\n",
    "        from_labels: bool = False\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # when the model outputs are intended as labels:\n",
    "    if from_labels:\n",
    "        # non-maximum suppression and the IoU threshold are not relevant when\n",
    "        # the model outputs represent labels as they are already discretized:\n",
    "        pass\n",
    "\n",
    "    # when the model outputs are intended as predictions:\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # tf.image.generate_bounding_box_proposals\n",
    "    # tf.image.combined_non_max_suppression\n",
    "    # tf.image.non_max_suppression\n",
    "        # tf.image.non_max_suppression_overlaps\n",
    "        # tf.image.non_max_suppression_padded\n",
    "        # tf.image.non_max_suppression_with_scores\n",
    "    IOU_THRESHOLD_FOR_NON_MAXIMUM_SUPPRESSION\n",
    "\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f3759",
   "metadata": {},
   "source": [
    "#### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model architecture definition.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# pylint: disable=import-error\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Convolution2D,\n",
    "    LeakyReLU,\n",
    "    MaxPooling2D,\n",
    "    Reshape\n",
    ")\n",
    "from tensorflow.keras.layers.experimental.preprocessing import (\n",
    "    RandomFlip,\n",
    "    Rescaling\n",
    ")\n",
    "# pylint: enable=import-error\n",
    "\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import (\n",
    "        DOWNSAMPLING_STEPS,\n",
    "        IMAGE_N_CHANNELS,\n",
    "        IMAGE_N_COLUMNS,\n",
    "        IMAGE_N_ROWS,\n",
    "        N_ANCHORS_PER_CELL,\n",
    "        N_OUTPUTS_PER_ANCHOR,\n",
    "        OUTPUT_GRID_N_COLUMNS,\n",
    "        OUTPUT_GRID_N_ROWS\n",
    "    )\n",
    "    from inference import get_bounding_boxes_from_model_outputs\n",
    "\n",
    "\n",
    "CONVOLUTIONAL_LAYERS_COMMON_KWARGS = {\n",
    "    'kernel_size': (3, 3),\n",
    "    'strides': (1, 1),\n",
    "    'padding': 'same',\n",
    "    'data_format': 'channels_last',\n",
    "    'dilation_rate': (1, 1),\n",
    "    'groups': 1,\n",
    "    'activation': None,\n",
    "    'use_bias': True\n",
    "}\n",
    "FIRST_LAYER_N_CONVOLUTIONAL_FILTERS = 16  # TODO\n",
    "INPUT_NORMALIZATION_OFFSET = 0.0\n",
    "INPUT_NORMALIZATION_RESCALING_FACTOR = (1. / 255)\n",
    "LEAKY_RELU_NEGATIVE_SLOPE = 0.1\n",
    "N_CONVOLUTIONS_AT_SAME_RESOLUTION = 3\n",
    "POOLING_LAYERS_COMMON_KWARGS = {\n",
    "    'pool_size': (2, 2),\n",
    "    'strides': (2, 2),\n",
    "    'padding': 'valid',\n",
    "    'data_format': 'channels_last',\n",
    "}\n",
    "\n",
    "\n",
    "class YOLOv3Variant(Model):  # noqa: E501 pylint: disable=abstract-method, too-many-ancestors\n",
    "    \"\"\"\n",
    "    Customized architecture variant of YOLOv3.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_plus_norm_plus_activation(\n",
    "            n_of_filters: int\n",
    "    ) -> Sequential:\n",
    "        \"\"\"\n",
    "        Return an instance of an enriched convolutional layer block composed,\n",
    "        going from inputs to outputs, of:\n",
    "        - a 2D convolutional layer without any non-linearity;\n",
    "        - a batch-normalization layer;\n",
    "        - a leaky rectified linear unit activation function.\n",
    "        \"\"\"\n",
    "        return Sequential(\n",
    "            [\n",
    "                Convolution2D(\n",
    "                    filters=n_of_filters,\n",
    "                    **CONVOLUTIONAL_LAYERS_COMMON_KWARGS\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                LeakyReLU(\n",
    "                    alpha=LEAKY_RELU_NEGATIVE_SLOPE\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_fully_convolutional_yolov3_architecture() -> Model:\n",
    "        \"\"\"\n",
    "        Return an instance of the herein defined YOLOv3 model architecture\n",
    "        that represents its fully-convolutional part, that is excluding\n",
    "        bounding boxes' postprocessing (filtering & aggregation).\n",
    "        \"\"\"\n",
    "        inputs = Input(\n",
    "            shape=(IMAGE_N_ROWS, IMAGE_N_COLUMNS, IMAGE_N_CHANNELS)\n",
    "        )\n",
    "\n",
    "        # rescaling the input image to normalize its pixels' intensities:\n",
    "        outputs = Rescaling(\n",
    "            scale=INPUT_NORMALIZATION_RESCALING_FACTOR,\n",
    "            offset=INPUT_NORMALIZATION_OFFSET\n",
    "        )(inputs)\n",
    "\n",
    "        # randomly flipping input images horizontally as a form of data\n",
    "        # augmentation during training:\n",
    "        outputs = RandomFlip(mode='horizontal', seed=0,)(outputs)\n",
    "        # NOTE: step carried out here to take advantage of GPU acceleration,\n",
    "        # unlike as if it were in the training dataset\n",
    "\n",
    "        current_n_of_filters = FIRST_LAYER_N_CONVOLUTIONAL_FILTERS\n",
    "        # for each iso-resolution block of convolutional processing ended by a\n",
    "        # downsampling:\n",
    "        for _ in range(DOWNSAMPLING_STEPS):\n",
    "            # for each enriched convolutional layer in the current\n",
    "            # iso-resolution block:\n",
    "            for _ in range(N_CONVOLUTIONS_AT_SAME_RESOLUTION):\n",
    "                outputs = YOLOv3Variant.conv_plus_norm_plus_activation(\n",
    "                    n_of_filters=current_n_of_filters\n",
    "                )(outputs)\n",
    "\n",
    "            # downsampling, ending the iso-resolution block:\n",
    "            outputs = MaxPooling2D(**POOLING_LAYERS_COMMON_KWARGS)(outputs)\n",
    "\n",
    "            # updating the number of filters for the next iso-resolution\n",
    "            # convolutional layers (by doubling them):\n",
    "            current_n_of_filters *= 2\n",
    "\n",
    "        # final 1x1 convolutions to predict bounding boxes' attributes from\n",
    "        # grid anchors' feature maps:\n",
    "        outputs = Convolution2D(\n",
    "            filters=(N_ANCHORS_PER_CELL * N_OUTPUTS_PER_ANCHOR),\n",
    "            **(\n",
    "                dict(CONVOLUTIONAL_LAYERS_COMMON_KWARGS, kernel_size=(1, 1))\n",
    "            )\n",
    "        )(outputs)\n",
    "        # NOTE: now bounding boxes' attributes respect the order of meaning\n",
    "        # (object centered probability, x, y, width, height)\n",
    "\n",
    "        # asserting the correctness of the current outputs' shape:\n",
    "        assert (\n",
    "            outputs.shape[1:] == (\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                N_ANCHORS_PER_CELL * N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "        ), \"Unmatched expectations between outputs and labels shape.\"\n",
    "\n",
    "        # reshaping the last output dimension to split anchors and their\n",
    "        # features along two separate dimensions:\n",
    "        outputs = Reshape(\n",
    "            target_shape=(\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                N_ANCHORS_PER_CELL,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "        )(outputs)\n",
    "\n",
    "        # applying an element-wise sigmoidal activation function as all 5\n",
    "        # bounding boxes' output attributes must belong to [0;1] range,\n",
    "        # since they are either probabilities of a single class (the first\n",
    "        # attribute) or relative coordinates (the second and third one) or\n",
    "        # relative sizes (the fourth and fifth one):\n",
    "        outputs = sigmoid(outputs)\n",
    "        # NOTE: these sigmoidal computations are carried out here instead of\n",
    "        # with the loss computation (and during inference) since computing\n",
    "        # them together with the loss functions's operations would not allow\n",
    "        # to achieve better gradients during training, since the objectness\n",
    "        # score needs to undergo the sigmoidal transformation beforehand and\n",
    "        # the other attributes of the anchors do not udnergo transformations\n",
    "        # as BCE, that can be fused together with softmax improving gradients'\n",
    "        # flow, but they all undergo MSE instead, since they represent\n",
    "        # coordinates and not likelihoods/probabilities\n",
    "\n",
    "        return Model(\n",
    "            inputs=inputs,\n",
    "            outputs=outputs\n",
    "        )\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(YOLOv3Variant, self).__init__()\n",
    "        self.yolov3_fcn = self.build_fully_convolutional_yolov3_architecture()\n",
    "\n",
    "    def call(self, inputs: Tensor, training: bool = False) -> Tensor:  # noqa: E501 pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Forward propagation definition.\n",
    "        \"\"\"\n",
    "        # passing the inputs through the fully-convolutional network:\n",
    "        fcn_outputs = self.yolov3_fcn(\n",
    "            inputs=inputs,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        # at inference time:\n",
    "        if not training:\n",
    "            # post-processing the bounding boxes outputs to return only the\n",
    "            # final, filtered and aggregated ones:\n",
    "            get_bounding_boxes_from_model_outputs(\n",
    "                model_outputs=fcn_outputs,\n",
    "                from_labels=False\n",
    "            )\n",
    "\n",
    "        # at training time:\n",
    "        else:\n",
    "            # no post-processing:\n",
    "            outputs = fcn_outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = YOLOv3Variant()\n",
    "\n",
    "    model.yolov3_fcn.summary()\n",
    "    # TODO: model.plot_model(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e984a",
   "metadata": {},
   "source": [
    "#### Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definitions of the employed loss function and metrics.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from numpy import arange, ndarray\n",
    "# pylint: disable=import-error\n",
    "from tensorflow import (\n",
    "    broadcast_to,\n",
    "    convert_to_tensor,\n",
    "    stack,\n",
    "    Tensor,\n",
    "    where,\n",
    "    zeros\n",
    ")\n",
    "from tensorflow.keras.losses import binary_crossentropy, mean_absolute_error\n",
    "from tensorflow.math import (\n",
    "    add,\n",
    "    greater_equal,\n",
    "    logical_not,\n",
    "    multiply,\n",
    "    reduce_mean\n",
    ")\n",
    "# pylint: enable=import-error\n",
    "\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import (\n",
    "        DATA_TYPE_FOR_OUTPUTS,\n",
    "        LOSS_CONTRIBUTE_IMPORTANCE_OF_EMPTY_ANCHORS,\n",
    "        LOSS_CONTRIBUTE_IMPORTANCE_OF_FULL_ANCHORS,\n",
    "        OUTPUT_GRID_N_ROWS,\n",
    "        OUTPUT_GRID_N_COLUMNS,\n",
    "        N_ANCHORS_PER_CELL,\n",
    "        N_OUTPUTS_PER_ANCHOR\n",
    "    )\n",
    "    from inference import (\n",
    "        get_bounding_boxes_from_model_outputs\n",
    "    )\n",
    "    from samples_and_labels import (\n",
    "        MINI_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "\n",
    "EPSILON = 1e-7\n",
    "IOU_THRESHOLDS = [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]\n",
    "OBJECTNESS_PROBABILITY_THRESHOLD = 0.5  # FIXME: not required\n",
    "\n",
    "\n",
    "def evaluate_bounding_boxes_matching(\n",
    "        expected_bounding_boxes: Tensor,\n",
    "        predicted_bounding_boxes: Tensor,\n",
    "        iou_threshold: float\n",
    ") -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    return (\n",
    "        false_positives,\n",
    "        false_negatives,\n",
    "        true_positives\n",
    "    )\n",
    "\n",
    "\n",
    "def iou_threshold_averaged_f2_score(y_true: Tensor, y_pred: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Metric used to validate the model goodness - according to the competition\n",
    "    aim - that represents the F2 score, as they decided to favor recall twice\n",
    "    as much as precision, avereaged over different IoU thresholds for\n",
    "    considering bounding boxes as detected or not, with these thresholds\n",
    "    being: {0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8}.\n",
    "    \"\"\"\n",
    "    # turning the labels representing model outputs into bounding boxes,\n",
    "    # following the same format that the predictions assume at inference time,\n",
    "    # when they undergo an additional post-processing, unlike during training:\n",
    "    labels_as_bounding_boxes = get_bounding_boxes_from_model_outputs(\n",
    "        model_outputs=y_true,\n",
    "        from_labels=True\n",
    "    )\n",
    "\n",
    "    mean_f2_scores_for_different_iou_thresholds = []\n",
    "\n",
    "    for threshold in IOU_THRESHOLDS:\n",
    "        (\n",
    "            false_positives,\n",
    "            false_negatives,\n",
    "            true_positives\n",
    "        ) = evaluate_bounding_boxes_matching(\n",
    "            expected_bounding_boxes=labels_as_bounding_boxes,\n",
    "            predicted_bounding_boxes=y_pred,\n",
    "            iou_threshold=threshold\n",
    "        )\n",
    "\n",
    "        mean_f2_scores_for_different_iou_thresholds.append(\n",
    "            # ----------------------------------------------------------------\n",
    "            # convert_to_tensor(\n",
    "            #     value=[\n",
    "            #         mean_f2_scores(\n",
    "            #             false_positives=false_positives,\n",
    "            #             false_negatives=false_negatives,\n",
    "            #             true_positives=true_positives\n",
    "            #         )\n",
    "            #     ],\n",
    "            #     dtype=DATA_TYPE_FOR_OUTPUTS\n",
    "            # )\n",
    "            # ----------------------------------------------------------------\n",
    "            mean_f2_scores(\n",
    "                false_positives=false_positives,\n",
    "                false_negatives=false_negatives,\n",
    "                true_positives=true_positives\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return reduce_mean(\n",
    "        input_tensor=stack(\n",
    "            values=mean_f2_scores_for_different_iou_thresholds,\n",
    "            axis=-1\n",
    "        ),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def mean_f2_scores(\n",
    "        false_positives: Tensor,\n",
    "        false_negatives: Tensor,\n",
    "        true_positives: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Return the F2-scores of each mini-batch sample, given their numbers of\n",
    "    false positives, false negatives and true positives as inputs.\n",
    "    \"\"\"\n",
    "    # FIXME: vectorize considering batches and with TF\n",
    "    return (\n",
    "        true_positives /\n",
    "        (true_positives + 0.8*false_negatives + 0.2*false_positives + EPSILON)\n",
    "    )\n",
    "\n",
    "\n",
    "def yolov3_variant_loss(y_true: Tensor, y_pred: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Loss function minimized to train the defined YOLOv3 variant.\n",
    "    ---\n",
    "        Input Shapes:\n",
    "            - (\n",
    "                MINI_BATCH_SIZE,\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                N_ANCHORS_PER_CELL,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "            - (\n",
    "                MINI_BATCH_SIZE,\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                N_ANCHORS_PER_CELL,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "    ---\n",
    "        Output Shape:\n",
    "            - (MINI_BATCH_SIZE,)\n",
    "    \"\"\"\n",
    "    #  (samples, rows, columns, anchors, attributes)\n",
    "    full_shape = (\n",
    "        MINI_BATCH_SIZE,\n",
    "        OUTPUT_GRID_N_ROWS,\n",
    "        OUTPUT_GRID_N_COLUMNS,\n",
    "        N_ANCHORS_PER_CELL,\n",
    "        N_OUTPUTS_PER_ANCHOR\n",
    "    )\n",
    "\n",
    "    dummy_tensor = zeros(\n",
    "        shape=full_shape\n",
    "    )\n",
    "\n",
    "    true_anchors_with_objects_flags = broadcast_to(\n",
    "        input=greater_equal(\n",
    "            x=y_true[..., 0],\n",
    "            y=OBJECTNESS_PROBABILITY_THRESHOLD\n",
    "        ),\n",
    "        shape=full_shape\n",
    "    )\n",
    "    print('_'*90)\n",
    "    print(true_anchors_with_objects_flags.shape)\n",
    "    print('_'*90)\n",
    "    true_anchors_without_objects_flags = logical_not(\n",
    "        x=true_anchors_with_objects_flags\n",
    "    )\n",
    "\n",
    "    full_anchors_objectness_loss_per_anchor = binary_crossentropy(\n",
    "        y_true=where(\n",
    "            condition=true_anchors_with_objects_flags,\n",
    "            x=y_true,\n",
    "            y=dummy_tensor\n",
    "        )[..., 0],\n",
    "        y_pred=where(\n",
    "            condition=true_anchors_with_objects_flags,\n",
    "            x=y_pred,\n",
    "            y=dummy_tensor\n",
    "        )[..., 0],\n",
    "        from_logits=False,\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    empty_anchors_objectness_loss_per_anchor = binary_crossentropy(\n",
    "        y_true=where(\n",
    "            condition=true_anchors_without_objects_flags,\n",
    "            x=y_true,\n",
    "            y=dummy_tensor\n",
    "        )[..., 0],\n",
    "        y_pred=where(\n",
    "            condition=true_anchors_without_objects_flags,\n",
    "            x=y_pred,\n",
    "            y=dummy_tensor\n",
    "        )[..., 0],\n",
    "        from_logits=False,\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    full_anchors_coordinates_offsets_loss_per_anchor = mean_absolute_error(\n",
    "        y_true=where(\n",
    "            condition=true_anchors_with_objects_flags,\n",
    "            x=y_true,\n",
    "            y=dummy_tensor\n",
    "        )[..., 1:3],\n",
    "        y_pred=where(\n",
    "            condition=true_anchors_with_objects_flags,\n",
    "            x=y_pred,\n",
    "            y=dummy_tensor\n",
    "        )[..., 1:3],\n",
    "    )\n",
    "    full_anchors_coordinates_scales_loss_per_anchor = mean_absolute_error(\n",
    "        y_true=where(\n",
    "            condition=true_anchors_with_objects_flags,\n",
    "            x=y_true,\n",
    "            y=dummy_tensor\n",
    "        )[..., 3:],\n",
    "        y_pred=where(\n",
    "            condition=true_anchors_with_objects_flags,\n",
    "            x=y_pred,\n",
    "            y=dummy_tensor\n",
    "        )[..., 3:],\n",
    "    )\n",
    "\n",
    "    full_anchors_coordinates_loss_per_anchor = add(\n",
    "        x=full_anchors_coordinates_offsets_loss_per_anchor,\n",
    "        y=full_anchors_coordinates_scales_loss_per_anchor\n",
    "    )\n",
    "\n",
    "    full_anchors_mean_loss = reduce_mean(\n",
    "        input_tensor=add(\n",
    "            x=full_anchors_objectness_loss_per_anchor,\n",
    "            y=full_anchors_coordinates_loss_per_anchor\n",
    "        ),\n",
    "        axis=[1, 2, 3, 4]\n",
    "    )\n",
    "\n",
    "    empty_anchors_mean_loss = reduce_mean(\n",
    "        input_tensor=empty_anchors_objectness_loss_per_anchor,\n",
    "        axis=[1, 2, 3, 4]\n",
    "    )\n",
    "\n",
    "    # FIXME: without weighting, here, after mean reduction, it means that both\n",
    "    # terms will have the same weight, irrespectively of their imbalance\n",
    "    return add(\n",
    "        x=multiply(\n",
    "            x=full_anchors_mean_loss,\n",
    "            y=LOSS_CONTRIBUTE_IMPORTANCE_OF_FULL_ANCHORS\n",
    "        ),\n",
    "        y=multiply(\n",
    "            x=empty_anchors_mean_loss,\n",
    "            y=LOSS_CONTRIBUTE_IMPORTANCE_OF_EMPTY_ANCHORS\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66cf82",
   "metadata": {},
   "source": [
    "#### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execution of the defined model training and validation on the respective\n",
    "preprocessed dataset splits, optimizing the defined loss and monitoring the\n",
    "metrics of interest.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# pylint: disable=import-error\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# pylint: enable=import-error\n",
    "\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from loss_and_metrics import (\n",
    "        iou_threshold_averaged_f2_score, yolov3_variant_loss\n",
    "    )\n",
    "    from model_architecture import YOLOv3Variant\n",
    "    from samples_and_labels import (\n",
    "        dataset_of_samples_and_model_outputs,\n",
    "        split_dataset_into_batched_training_and_validation_sets\n",
    "    )\n",
    "\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# NOTE: these are 1-based indexes:\n",
    "EPOCHS_WHEN_VALIDATION_CARRIED_OUT = [1, 3, 5, 7, 9, N_EPOCHS]\n",
    "\n",
    "\n",
    "def train_and_validate_model(\n",
    "        model_instance: Model,\n",
    "        training_set: Dataset,\n",
    "        validation_set: Dataset\n",
    ") -> str:  # TODO: output dtype\n",
    "    \"\"\"\n",
    "    Compile (in TensorFlow's language acception, i.e. associate optimizer,\n",
    "    loss function and metrics to the model instance) the input model instance\n",
    "    first and then train and validate it on the respective input datasets.\n",
    "    \"\"\"\n",
    "    model_instance.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=yolov3_variant_loss,\n",
    "        metrics=[]  # TODO iou_threshold_averaged_f2_score\n",
    "    )\n",
    "\n",
    "    training_history = model_instance.fit(\n",
    "        x=training_set,\n",
    "        epochs=N_EPOCHS,\n",
    "        validation_data=validation_set,\n",
    "        validation_freq=EPOCHS_WHEN_VALIDATION_CARRIED_OUT\n",
    "    )\n",
    "\n",
    "    return training_history\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (\n",
    "        training_samples_and_labels, validation_samples_and_labels\n",
    "    ) = split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set=dataset_of_samples_and_model_outputs()\n",
    "    )\n",
    "    model = YOLOv3Variant()\n",
    "\n",
    "    training_history = train_and_validate_model(\n",
    "        model_instance=model,\n",
    "        training_set=training_samples_and_labels,\n",
    "        validation_set=validation_samples_and_labels,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e217e",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975adc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execution of the proposed competition solution.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from random import seed as random_seed\n",
    "\n",
    "from numpy.random import seed as numpy_seed\n",
    "# pylint: disable=import-error\n",
    "from tensorflow.random import set_seed\n",
    "# pylint: enable=import-error\n",
    "\n",
    "\n",
    "def fix_seeds_for_reproducible_results() -> None:\n",
    "    \"\"\"\n",
    "    Make the subsequent instructions produce purely deterministic outputs by\n",
    "    fixing all the relevant seeds.\n",
    "    \"\"\"\n",
    "    random_seed(a=0)\n",
    "    _ = numpy_seed(seed=0)\n",
    "    set_seed(seed=0)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Execute the proposed competition solution.\n",
    "    \"\"\"\n",
    "    fix_seeds_for_reproducible_results()\n",
    "\n",
    "    raise NotImplementedError\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
