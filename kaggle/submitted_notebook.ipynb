{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110d44d4",
   "metadata": {},
   "source": [
    "# Mattia Sarti's Notebook\n",
    "### The following source code is illustrated [here](https://github.com/MattiaSarti/yolo-to-help-protect-the-great-barrier-reef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7767c3",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__ == 'main_by_mattia'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8acd2",
   "metadata": {},
   "source": [
    "#### Common constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd3f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convenient definitions of common constants.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from numpy import arange, meshgrid, ndarray, stack\n",
    "# pylint: disable=import-error\n",
    "from tensorflow import float32 as tf_float32, uint8 as tf_uint8\n",
    "# pylint: enable=import-error\n",
    "\n",
    "\n",
    "def compute_grid_cell_centers_xy_coords() -> Tuple[ndarray, ndarray]:\n",
    "    \"\"\"\n",
    "    Return two 3D arrays respectively representing the output grid cell\n",
    "    centers' (x, y) coordinates and top-left corners' (x, y) coordinates,\n",
    "    indexed along the first two dimensions as rows and columns of cells in the\n",
    "    output grid.\n",
    "    ---\n",
    "        Outputs' Shapes:\n",
    "            - (OUTPUT_GRID_N_ROWS, OUTPUT_GRID_N_COLUMNS, 2)\n",
    "            - (OUTPUT_GRID_N_ROWS, OUTPUT_GRID_N_COLUMNS, 2)\n",
    "    ---\n",
    "        Outputs' Meanings:\n",
    "            - the first dimension is the row index of the grid cell and the\n",
    "            second dimension is the column index of the grid cell, while the\n",
    "            third dimension represents the tuple of center (x, y) coordinates\n",
    "            of the considered grid cell\n",
    "            - the first dimension is the row index of the grid cell and the\n",
    "            second dimension is the column index of the grid cell, while the\n",
    "            third dimension represents the tuple of top-left corner (x, y)\n",
    "            coordinates of the considered grid cell\n",
    "    \"\"\"\n",
    "    # x and y possible values spanned by grid cell centers:\n",
    "    centers_x_coords_values = arange(\n",
    "        start=int(OUTPUT_GRID_CELL_N_COLUMNS / 2),\n",
    "        stop=IMAGE_N_COLUMNS,\n",
    "        step=OUTPUT_GRID_CELL_N_COLUMNS\n",
    "    )\n",
    "    assert centers_x_coords_values.shape == (OUTPUT_GRID_N_COLUMNS,)\n",
    "    centers_y_coords_values = arange(\n",
    "        start=int(OUTPUT_GRID_CELL_N_ROWS / 2),\n",
    "        stop=IMAGE_N_ROWS,\n",
    "        step=OUTPUT_GRID_CELL_N_ROWS\n",
    "    )\n",
    "    assert centers_y_coords_values.shape == (OUTPUT_GRID_N_ROWS,)\n",
    "\n",
    "    # x and y possible values spanned by grid cell top-left corners:\n",
    "    corners_x_coords_values = arange(\n",
    "        start=0,\n",
    "        stop=IMAGE_N_COLUMNS,\n",
    "        step=OUTPUT_GRID_CELL_N_COLUMNS\n",
    "    )\n",
    "    assert corners_x_coords_values.shape == (OUTPUT_GRID_N_COLUMNS,)\n",
    "    corners_y_coords_values = arange(\n",
    "        start=0,\n",
    "        stop=IMAGE_N_ROWS,\n",
    "        step=OUTPUT_GRID_CELL_N_ROWS\n",
    "    )\n",
    "    assert corners_y_coords_values.shape == (OUTPUT_GRID_N_ROWS,)\n",
    "\n",
    "    # grid of cells containing the respective center x and y coordinates each:\n",
    "    centers_xy_coords = stack(\n",
    "        arrays=meshgrid(centers_x_coords_values, centers_y_coords_values),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    # grid of cells containing the respective top-left corner x and y\n",
    "    # coordinates each:\n",
    "    corners_xy_coords = stack(\n",
    "        arrays=meshgrid(corners_x_coords_values, corners_y_coords_values),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        centers_xy_coords,\n",
    "        corners_xy_coords\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_TYPE_FOR_INPUTS = tf_uint8\n",
    "DATA_TYPE_FOR_OUTPUTS = tf_float32\n",
    "\n",
    "DOWNSAMPLING_STEPS = 4\n",
    "\n",
    "IMAGE_N_CHANNELS = 3\n",
    "IMAGE_N_COLUMNS = 1280\n",
    "IMAGE_N_ROWS = 720\n",
    "\n",
    "N_OUTPUTS_PER_ANCHOR = 5\n",
    "\n",
    "OUTPUT_GRID_CELL_N_ANCHORS = 3\n",
    "OUTPUT_GRID_CELL_N_COLUMNS = 16  # NOTE: this may vary with the architecture\n",
    "OUTPUT_GRID_CELL_N_ROWS = 16  # NOTE: this may vary with the architecture\n",
    "# NOTE: common divisors of 1280 and 720: {1, 2, 4, 5, 8, 10, 16, 20, 40, 80},\n",
    "# and the ones that respect the training-plus-validation set vounding boxes'\n",
    "# distinction when using a single anchor are: {1, 2, 4, 5, 8, 10, 16}\n",
    "\n",
    "OUTPUT_GRID_N_COLUMNS = int(IMAGE_N_COLUMNS / OUTPUT_GRID_CELL_N_COLUMNS)\n",
    "OUTPUT_GRID_N_ROWS = int(IMAGE_N_ROWS / OUTPUT_GRID_CELL_N_ROWS)\n",
    "\n",
    "(\n",
    "    OUTPUT_GRID_CELL_CENTERS_XY_COORDS,\n",
    "    OUTPUT_GRID_CELL_CORNERS_XY_COORDS\n",
    ") = compute_grid_cell_centers_xy_coords()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a81cc",
   "metadata": {},
   "source": [
    "#### Samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21375f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample and label extraction from the raw dataset files, inspection and\n",
    "preprocessing for feeding the model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from csv import reader as csv_reader\n",
    "from itertools import combinations\n",
    "from json import loads as json_loads\n",
    "from math import sqrt\n",
    "from os import getcwd, pardir\n",
    "from os.path import join as path_join\n",
    "from typing import Dict, List, Tuple\n",
    "from cv2 import determinant\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.pyplot import (\n",
    "    clf as plt_clf,\n",
    "    close as plt_close,\n",
    "    figure as plt_figure,\n",
    "    hist as plt_hist,\n",
    "    get_current_fig_manager,\n",
    "    pause as plt_pause,\n",
    "    savefig as plt_savefig,\n",
    "    show as plt_show,\n",
    "    subplots,\n",
    "    title as plt_title,\n",
    "    xticks as plt_xticks\n",
    ")\n",
    "from numpy import argmin, sum as np_sum, unravel_index, zeros\n",
    "# pylint: disable=import-error\n",
    "from tensorflow import convert_to_tensor, py_function, Tensor\n",
    "from tensorflow.data import AUTOTUNE, Dataset\n",
    "from tensorflow.io import decode_jpeg, read_file\n",
    "# pylint: enable=import-error\n",
    "\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import (\n",
    "        DATA_TYPE_FOR_INPUTS,\n",
    "        DATA_TYPE_FOR_OUTPUTS,\n",
    "        IMAGE_N_COLUMNS,\n",
    "        IMAGE_N_ROWS,\n",
    "        N_OUTPUTS_PER_ANCHOR,\n",
    "        OUTPUT_GRID_CELL_CENTERS_XY_COORDS,\n",
    "        OUTPUT_GRID_CELL_CORNERS_XY_COORDS,\n",
    "        OUTPUT_GRID_CELL_N_ANCHORS,\n",
    "        OUTPUT_GRID_CELL_N_COLUMNS,\n",
    "        OUTPUT_GRID_CELL_N_ROWS,\n",
    "        OUTPUT_GRID_N_COLUMNS,\n",
    "        OUTPUT_GRID_N_ROWS\n",
    "    )\n",
    "\n",
    "\n",
    "MINI_BATCH_SIZE = 4  # TODO\n",
    "VALIDATION_SET_PORTION_OF_DATA = 0.3\n",
    "\n",
    "DATASET_DIR = path_join(\n",
    "    getcwd(),\n",
    "    pardir,\n",
    "    'tensorflow-great-barrier-reef'\n",
    ")\n",
    "LABELS_FILE_PATH = path_join(\n",
    "    DATASET_DIR,\n",
    "    'train.csv'\n",
    ")\n",
    "PICTURES_DIR = path_join(\n",
    "    getcwd(),\n",
    "    pardir,\n",
    "    'docs',\n",
    "    'pictures'\n",
    ")\n",
    "\n",
    "SHOW_BOUNDING_BOXES_STATISTICS = False\n",
    "SHOW_DATASET_MOVIES = False\n",
    "\n",
    "\n",
    "def dataset_of_samples_and_bounding_boxes() -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a TensorFlow dataset that can iterate over all the dataset samples\n",
    "    and the respective labels containing bounding boxes.\n",
    "    \"\"\"\n",
    "    image_paths_dataset = Dataset.from_tensor_slices(\n",
    "        tensors=[*IMAGE_PATHS_TO_BOUNDING_BOXES]  # only keys included\n",
    "    )\n",
    "\n",
    "    return image_paths_dataset.map(\n",
    "        map_func=lambda image_path: py_function(\n",
    "            func=load_sample_and_get_bounding_boxes,\n",
    "            inp=[image_path],\n",
    "            Tout=(DATA_TYPE_FOR_INPUTS, DATA_TYPE_FOR_OUTPUTS)\n",
    "        ),\n",
    "        num_parallel_calls=AUTOTUNE,  # TODO\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "\n",
    "def dataset_of_samples_and_model_outputs(shuffle: bool = True) -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a TensorFlow dataset that can iterate over all the dataset samples\n",
    "    and the respective labels containing model outputs, in a shuffled order.\n",
    "    \"\"\"\n",
    "    image_paths_dataset = Dataset.from_tensor_slices(\n",
    "        tensors=[*IMAGE_PATHS_TO_MODEL_OUTPUTS]  # only keys included\n",
    "    )\n",
    "\n",
    "    # NOTE: shuffling is carried out here to have acceptable performance with\n",
    "    # a shuffling buffer size that allows to take the whole set into memory\n",
    "    # in case shuffling is desired:\n",
    "    if shuffle:\n",
    "        image_paths_dataset.shuffle(\n",
    "            buffer_size=N_TRAINING_PLUS_VALIDATION_SAMPLES,\n",
    "            seed=0,\n",
    "            reshuffle_each_iteration=False  # NOTE: relevant when splitting\n",
    "        )\n",
    "\n",
    "    return image_paths_dataset.map(\n",
    "        map_func=lambda image_path: py_function(\n",
    "            func=load_sample_and_get_model_outputs,\n",
    "            inp=[image_path],\n",
    "            Tout=(DATA_TYPE_FOR_INPUTS, DATA_TYPE_FOR_OUTPUTS)\n",
    "        ),\n",
    "        num_parallel_calls=AUTOTUNE,  # TODO\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cell_containing_bounding_box_center(\n",
    "        center_absolute_x_and_y_coords: Tuple[float, float]\n",
    ") -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Find the output grid cell whose center is closest to the bounding box one\n",
    "    (the input one), returning the grid cell's row and column indexes and its\n",
    "    x and y coordinates.\n",
    "    ---\n",
    "        Output Shape:\n",
    "            - (4,)\n",
    "    ---\n",
    "        Output Meaning:\n",
    "            - [\n",
    "                grid cell row index,\n",
    "                grid cell column index,\n",
    "                x coordindate of cell center,\n",
    "                y coordindate of cell center\n",
    "            ]\n",
    "    \"\"\"\n",
    "    (\n",
    "        grid_cell_enclosing_bounding_box_center_row_index,\n",
    "        grid_cell_enclosing_bounding_box_center_column_index\n",
    "    ) = unravel_index(\n",
    "        indices=argmin(  # NOTE: in case of equivalent minima, the first one is picked\n",
    "            # grid of squared element-wise center pairs' distances representing\n",
    "            # the minimized objective to find the closest grid cell center:\n",
    "            a=np_sum(\n",
    "                a=(\n",
    "                    (OUTPUT_GRID_CELL_CENTERS_XY_COORDS -\n",
    "                    center_absolute_x_and_y_coords) ** 2\n",
    "                ),\n",
    "                axis=-1\n",
    "            )\n",
    "        ),\n",
    "        shape=(OUTPUT_GRID_N_ROWS, OUTPUT_GRID_N_COLUMNS),\n",
    "        order='C'\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        # [grid cell row index, grid cell column index]:\n",
    "        [\n",
    "            grid_cell_enclosing_bounding_box_center_row_index,\n",
    "            grid_cell_enclosing_bounding_box_center_column_index\n",
    "        ] +\n",
    "        # [x coordindate of cell center, y coordindate of cell center]:\n",
    "        OUTPUT_GRID_CELL_CENTERS_XY_COORDS[\n",
    "            grid_cell_enclosing_bounding_box_center_row_index,\n",
    "            grid_cell_enclosing_bounding_box_center_column_index,\n",
    "            :\n",
    "        ].tolist()\n",
    "    )\n",
    "\n",
    "\n",
    "def inspect_bounding_boxes_statistics_on_training_n_validation_set() -> None:\n",
    "    \"\"\"\n",
    "    Inspect and print the following statistics of bounding boxes in the\n",
    "    training-plus-validation set:\n",
    "        - total number of bounding boxes\n",
    "        - total number of images\n",
    "        - average number of bounding boxes per image\n",
    "        - minimum number of bounding boxes per image\n",
    "        - maximum number of bounding boxes per image\n",
    "        - total number of empty images\n",
    "        - average bounding box height [pixels]\n",
    "        - average bounding box width [pixels]\n",
    "        - average bounding boxes' centers distance [pixels]\n",
    "        - average bounding boxes' centers x-coord distance [pixels]\n",
    "        - average bounding boxes' centers y-coord distance [pixels]\n",
    "        - minimum bounding box height [pixels]\n",
    "        - minimum bounding box width [pixels]\n",
    "        - minimum bounding boxes' centers distance [pixels]\n",
    "        - minimum bounding boxes' centers x-coord distance [pixels]\n",
    "        - minimum bounding boxes' centers y-coord distance [pixels]\n",
    "        - maximum bounding box height [pixels]\n",
    "        - maximum bounding box width [pixels]\n",
    "        - maximum bounding boxes' centers distance [pixels]\n",
    "        - maximum bounding boxes' centers x-coord distance [pixels]\n",
    "        - maximum bounding boxes' centers y-coord distance [pixels]\n",
    "        - histogram of number of bounding boxes per image\n",
    "        - histogram of bounding boxes' centers distance [pixels]\n",
    "        - histogram of bounding boxes' centers x-coord distance [pixels]\n",
    "        - histogram of bounding boxes' centers y-coord distance [pixels]\n",
    "    \"\"\"\n",
    "    total_n_images = len(IMAGE_PATHS_TO_BOUNDING_BOXES)\n",
    "\n",
    "    bounding_boxes_centers_distances_for_histogram = []\n",
    "    bounding_boxes_centers_x_coord_distances_for_histogram = []\n",
    "    bounding_boxes_centers_y_coord_distances_for_histogram = []\n",
    "    cumulative_bounding_box_height = 0\n",
    "    cumulative_bounding_box_width = 0\n",
    "    cumulative_bounding_boxes_centers_distance = 0\n",
    "    cumulative_bounding_boxes_centers_x_coord_distance = 0\n",
    "    cumulative_bounding_boxes_centers_y_coord_distance = 0\n",
    "    minimum_bounding_box_height = 99999\n",
    "    minimum_bounding_box_width = 99999\n",
    "    minimum_bounding_boxes_centers_distance = 99999\n",
    "    minimum_bounding_boxes_centers_x_coord_distance = 99999\n",
    "    minimum_bounding_boxes_centers_y_coord_distance = 99999\n",
    "    minimum_n_bounding_boxes_per_image = 99999\n",
    "    maximum_bounding_box_height = 0\n",
    "    maximum_bounding_box_width = 0\n",
    "    maximum_bounding_boxes_centers_distance = 0\n",
    "    maximum_bounding_boxes_centers_x_coord_distance = 0\n",
    "    maximum_bounding_boxes_centers_y_coord_distance = 0\n",
    "    maximum_n_bounding_boxes_per_image = 0\n",
    "    n_bounding_boxes_per_image_for_histogram = []\n",
    "    total_n_bounding_boxes = 0\n",
    "    total_n_bounding_boxes_center_distances_cumulated = 0\n",
    "    total_n_empty_images = 0\n",
    "\n",
    "    for image_bounding_boxes in IMAGE_PATHS_TO_BOUNDING_BOXES.values():\n",
    "        n_bounding_boxes = len(image_bounding_boxes)\n",
    "        n_bounding_boxes_per_image_for_histogram.append(\n",
    "            n_bounding_boxes\n",
    "        )\n",
    "\n",
    "        total_n_bounding_boxes += n_bounding_boxes\n",
    "        if n_bounding_boxes < minimum_n_bounding_boxes_per_image:\n",
    "            minimum_n_bounding_boxes_per_image = n_bounding_boxes\n",
    "        if n_bounding_boxes > maximum_n_bounding_boxes_per_image:\n",
    "            maximum_n_bounding_boxes_per_image = n_bounding_boxes\n",
    "        if n_bounding_boxes == 0:\n",
    "            total_n_empty_images += 1\n",
    "\n",
    "        bounding_boxes_centers_x_and_y_coords = []\n",
    "        for bounding_box in image_bounding_boxes:\n",
    "            cumulative_bounding_box_height += bounding_box['height']\n",
    "            cumulative_bounding_box_width += bounding_box['width']\n",
    "\n",
    "            bounding_boxes_centers_x_and_y_coords.append(\n",
    "                {\n",
    "                    'x': (bounding_box['x'] + bounding_box['width']) / 2,\n",
    "                    'y': (bounding_box['y'] + bounding_box['height']) / 2\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if bounding_box['height'] < minimum_bounding_box_height:\n",
    "                minimum_bounding_box_height = bounding_box['height']\n",
    "            if bounding_box['width'] < minimum_bounding_box_width:\n",
    "                minimum_bounding_box_width = bounding_box['width']\n",
    "\n",
    "            if bounding_box['height'] > maximum_bounding_box_height:\n",
    "                maximum_bounding_box_height = bounding_box['height']\n",
    "            if bounding_box['width'] > maximum_bounding_box_width:\n",
    "                maximum_bounding_box_width = bounding_box['width']\n",
    "        \n",
    "        if n_bounding_boxes > 1:\n",
    "            for centers_coords_pair in combinations(\n",
    "                    iterable=bounding_boxes_centers_x_and_y_coords,\n",
    "                    r=2\n",
    "            ):\n",
    "                total_n_bounding_boxes_center_distances_cumulated += 1\n",
    "\n",
    "                x_coord_difference = abs(\n",
    "                    centers_coords_pair[0]['x'] - centers_coords_pair[1]['x']\n",
    "                )\n",
    "                y_coord_difference = abs(\n",
    "                    centers_coords_pair[0]['y'] - centers_coords_pair[1]['y']\n",
    "                )\n",
    "                distance = sqrt(\n",
    "                    x_coord_difference**2 + y_coord_difference**2\n",
    "                )\n",
    "\n",
    "                bounding_boxes_centers_distances_for_histogram.append(\n",
    "                    distance\n",
    "                )\n",
    "                bounding_boxes_centers_x_coord_distances_for_histogram.append(\n",
    "                    x_coord_difference\n",
    "                )\n",
    "                bounding_boxes_centers_y_coord_distances_for_histogram.append(\n",
    "                    y_coord_difference\n",
    "                )\n",
    "\n",
    "                cumulative_bounding_boxes_centers_distance += (\n",
    "                    distance\n",
    "                )\n",
    "                cumulative_bounding_boxes_centers_x_coord_distance += (\n",
    "                    x_coord_difference\n",
    "                )\n",
    "                cumulative_bounding_boxes_centers_y_coord_distance += (\n",
    "                    y_coord_difference\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                        distance <\n",
    "                        minimum_bounding_boxes_centers_distance\n",
    "                ):\n",
    "                    minimum_bounding_boxes_centers_distance = (\n",
    "                        distance\n",
    "                    )\n",
    "                if (\n",
    "                        x_coord_difference <\n",
    "                        minimum_bounding_boxes_centers_x_coord_distance\n",
    "                ):\n",
    "                    minimum_bounding_boxes_centers_x_coord_distance = (\n",
    "                        x_coord_difference\n",
    "                    )\n",
    "                if (\n",
    "                        y_coord_difference <\n",
    "                        minimum_bounding_boxes_centers_y_coord_distance\n",
    "                ):\n",
    "                    minimum_bounding_boxes_centers_y_coord_distance = (\n",
    "                        y_coord_difference\n",
    "                    )\n",
    "\n",
    "                if (\n",
    "                        distance >\n",
    "                        maximum_bounding_boxes_centers_distance\n",
    "                ):\n",
    "                    maximum_bounding_boxes_centers_distance = (\n",
    "                        distance\n",
    "                    )\n",
    "                if (\n",
    "                        x_coord_difference >\n",
    "                        maximum_bounding_boxes_centers_x_coord_distance\n",
    "                ):\n",
    "                    maximum_bounding_boxes_centers_x_coord_distance = (\n",
    "                        x_coord_difference\n",
    "                    )\n",
    "                if (\n",
    "                    y_coord_difference > maximum_bounding_boxes_centers_y_coord_distance\n",
    "                ):\n",
    "                    maximum_bounding_boxes_centers_y_coord_distance = (\n",
    "                        y_coord_difference\n",
    "                    )\n",
    "\n",
    "    print('- ' * 30)\n",
    "    print(\"Bounding Boxes' Statistics:\")\n",
    "\n",
    "    print(\n",
    "        \"\\t- total number of bounding boxes:\",\n",
    "        total_n_bounding_boxes\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- total number of images:\",\n",
    "        total_n_images\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average number of bounding boxes per image:\",\n",
    "        round(number=total_n_bounding_boxes/total_n_images, ndigits=2)\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum number of bounding boxes per image:\",\n",
    "        minimum_n_bounding_boxes_per_image\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum number of bounding boxes per image:\",\n",
    "        maximum_n_bounding_boxes_per_image\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- total number of empty images:\",\n",
    "        total_n_empty_images\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding box height [pixels]:\",\n",
    "        round(\n",
    "            number=cumulative_bounding_box_height/total_n_bounding_boxes,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding box width [pixels]:\",\n",
    "        round(\n",
    "            number=cumulative_bounding_box_width/total_n_bounding_boxes,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding boxes' centers distance [pixels]:\",\n",
    "        round(\n",
    "            number=(\n",
    "                cumulative_bounding_boxes_centers_distance /\n",
    "                total_n_bounding_boxes_center_distances_cumulated\n",
    "            ),\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding boxes' centers x-coord distance [pixels]:\",\n",
    "        round(\n",
    "            number=(\n",
    "                cumulative_bounding_boxes_centers_x_coord_distance /\n",
    "                total_n_bounding_boxes_center_distances_cumulated\n",
    "            ),\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- average bounding boxes' centers y-coord distance [pixels]:\",\n",
    "        round(\n",
    "            number=(\n",
    "                cumulative_bounding_boxes_centers_y_coord_distance /\n",
    "                total_n_bounding_boxes_center_distances_cumulated\n",
    "            ),\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding box height [pixels]:\",\n",
    "        minimum_bounding_box_height\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding box width [pixels]:\",\n",
    "        minimum_bounding_box_width\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding boxes' centers distance [pixels]:\",\n",
    "        round(\n",
    "            number=minimum_bounding_boxes_centers_distance,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding boxes' centers x-coord distance [pixels]:\",\n",
    "        minimum_bounding_boxes_centers_x_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- minimum bounding boxes' centers y-coord distance [pixels]:\",\n",
    "        minimum_bounding_boxes_centers_y_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding box height [pixels]:\",\n",
    "        maximum_bounding_box_height\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding box width [pixels]:\",\n",
    "        maximum_bounding_box_width\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding boxes' centers distance [pixels]:\",\n",
    "        round(\n",
    "            number=maximum_bounding_boxes_centers_distance,\n",
    "            ndigits=2\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding boxes' centers x-coord distance [pixels]:\",\n",
    "        maximum_bounding_boxes_centers_x_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- maximum bounding boxes' centers y-coord distance [pixels]:\",\n",
    "        maximum_bounding_boxes_centers_y_coord_distance\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of number of bounding boxes per image: see plot\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of bounding boxes' centers distance [pixels]: \" +\n",
    "        \"see plot\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of bounding boxes' centers x-coord distance [pixels]: \" +\n",
    "        \"see plot\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\t- histogram of bounding boxes' centers y-coord distance [pixels]: \" +\n",
    "        \"see plot\"\n",
    "    )\n",
    "\n",
    "    plt_figure()\n",
    "\n",
    "    what_it_represent = \"Histogram of Number of Bounding Boxes per Image\"\n",
    "    plt_hist(\n",
    "        x=n_bounding_boxes_per_image_for_histogram,\n",
    "        bins=maximum_n_bounding_boxes_per_image,\n",
    "        align='left',\n",
    "        color='skyblue',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(range(maximum_n_bounding_boxes_per_image))\n",
    "    )\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    what_it_represent = (\n",
    "        \"Histogram of Bounding Boxes' Centers Distance [pixels]\"\n",
    "    )\n",
    "    plt_hist(\n",
    "        x=bounding_boxes_centers_distances_for_histogram,\n",
    "        bins=list(range(int(sqrt(IMAGE_N_COLUMNS**2 + IMAGE_N_ROWS**2)))),\n",
    "        align='left',\n",
    "        color='chartreuse',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(\n",
    "            range(0, int(sqrt(IMAGE_N_COLUMNS**2 + IMAGE_N_ROWS**2)), 20)\n",
    "        ),\n",
    "        fontsize=6,\n",
    "        rotation=90\n",
    "    )\n",
    "    figure_manager = get_current_fig_manager()\n",
    "    figure_manager.resize(*figure_manager.window.maxsize())\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    what_it_represent = (\n",
    "        \"Histogram of Bounding Boxes' Centers X-Coordinate Distance [pixels]\"\n",
    "    )\n",
    "    plt_hist(\n",
    "        x=bounding_boxes_centers_x_coord_distances_for_histogram,\n",
    "        bins=list(range(IMAGE_N_COLUMNS)),\n",
    "        align='left',\n",
    "        color='mediumslateblue',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(range(0, IMAGE_N_COLUMNS, 20)),\n",
    "        fontsize=6,\n",
    "        rotation=90\n",
    "    )\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    what_it_represent = (\n",
    "        \"Histogram of Bounding Boxes' Centers Y-Coordinate Distance [pixels]\"\n",
    "    )\n",
    "    plt_hist(\n",
    "        x=bounding_boxes_centers_y_coord_distances_for_histogram,\n",
    "        bins=list(range(IMAGE_N_ROWS)),\n",
    "        align='left',\n",
    "        color='violet',\n",
    "        rwidth=0.8\n",
    "    )\n",
    "    plt_title(label=what_it_represent)\n",
    "    plt_xticks(\n",
    "        ticks=list(range(0, IMAGE_N_ROWS, 20)),\n",
    "        fontsize=6,\n",
    "        rotation=90\n",
    "    )\n",
    "    plt_savefig(\n",
    "        fname=path_join(\n",
    "            PICTURES_DIR,\n",
    "            what_it_represent + '.png'\n",
    "        ),\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt_show(block=False)\n",
    "    plt_pause(interval=1)\n",
    "    plt_clf()\n",
    "\n",
    "    plt_close()\n",
    "\n",
    "    print('- ' * 30)\n",
    "\n",
    "\n",
    "def label_line_to_image_path_2_bounding_boxes_and_2_model_output(\n",
    "        csv_label_line_segments: List[str]\n",
    ") -> Tuple[\n",
    "        Dict[str, List[Dict[str, int]]],\n",
    "        Dict[str, List[List[Tuple[int, int, int, int]]]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Turn any line of the CSV labels file from the original format\n",
    "    'video_id,sequence,video_frame,sequence_frame,image_id,annotations' into\n",
    "    two dictionariies: the former with the respective image file path as key\n",
    "    and the respective bounding boxes as value, the latter with the respective\n",
    "    image file path as key and the respective model outputs as value.\n",
    "    \"\"\"\n",
    "    image_path = path_join(\n",
    "        DATASET_DIR,\n",
    "        'train_images',\n",
    "        'video_' + csv_label_line_segments[0],\n",
    "        csv_label_line_segments[2] + '.jpg'\n",
    "    )\n",
    "    bounding_boxes = json_loads(\n",
    "        csv_label_line_segments[5]\n",
    "        .replace('\"', '\"\"\"')\n",
    "        .replace(\"'\", '\"')\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            bytes(image_path, 'utf-8'): bounding_boxes\n",
    "        },\n",
    "        {\n",
    "            bytes(image_path, 'utf-8'): turn_bounding_boxes_to_model_outputs(\n",
    "                raw_bounding_boxes=bounding_boxes\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def load_labels_as_paths_to_bounding_boxes_and_model_outputs_dicts() -> Tuple[\n",
    "        Dict[str, List[Dict[str, int]]],\n",
    "        Dict[str, List[List[Tuple[int, int, int, int]]]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the labels' information from the CSV file and return them as a two\n",
    "    dictionaries, the former associating image file paths to respective\n",
    "    bounding boxes and the latter associating image file paths to respective\n",
    "    model outputs.\n",
    "    \"\"\"\n",
    "    image_paths_to_bounding_boxes = {}\n",
    "    image_paths_to_model_outputs = {}\n",
    "\n",
    "    with open(LABELS_FILE_PATH, 'r') as labels_file:\n",
    "        labels_reader = csv_reader(\n",
    "            labels_file,\n",
    "            delimiter=',',\n",
    "            quotechar='\"'\n",
    "        )\n",
    "\n",
    "        for line_index, line_segments in enumerate(labels_reader):\n",
    "            if line_index == 0:\n",
    "                continue\n",
    "\n",
    "            # turning the label from the raw format into processed\n",
    "            # dictionaries to retrieve bounding boxes and model outputs of\n",
    "            # images easily from respective image file paths:\n",
    "            (\n",
    "                image_path_to_bounding_boxes,\n",
    "                image_path_to_model_outputs\n",
    "            ) = label_line_to_image_path_2_bounding_boxes_and_2_model_output(\n",
    "                csv_label_line_segments=line_segments\n",
    "            )\n",
    "            image_paths_to_bounding_boxes.update(image_path_to_bounding_boxes)\n",
    "            image_paths_to_model_outputs.update(image_path_to_model_outputs)\n",
    "\n",
    "    return (image_paths_to_bounding_boxes, image_paths_to_model_outputs)\n",
    "\n",
    "\n",
    "def load_sample_and_get_bounding_boxes(image_path: Tensor) -> Tuple[\n",
    "        Tensor, Tensor\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the sample and get the label - representing bounding boxes - of the\n",
    "    image represented by the input path.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        decode_jpeg(\n",
    "            contents=read_file(\n",
    "                filename=image_path\n",
    "            )\n",
    "        ),\n",
    "        convert_to_tensor(\n",
    "            # bounding boxes as network output values:\n",
    "            value=[\n",
    "                [\n",
    "                    bounding_box_dict['x'],\n",
    "                    bounding_box_dict['y'],\n",
    "                    bounding_box_dict['width'],\n",
    "                    bounding_box_dict['height']\n",
    "                ] for bounding_box_dict in\n",
    "                IMAGE_PATHS_TO_BOUNDING_BOXES[image_path.numpy()]\n",
    "            ],\n",
    "            dtype=DATA_TYPE_FOR_OUTPUTS\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def load_sample_and_get_model_outputs(image_path: Tensor) -> Tuple[\n",
    "        Tensor, Tensor\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the sample and get the label - representing model outputs - of the\n",
    "    image represented by the input path.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        decode_jpeg(\n",
    "            contents=read_file(\n",
    "                filename=image_path\n",
    "            )\n",
    "        ),\n",
    "        convert_to_tensor(\n",
    "            # bounding boxes as network output values:\n",
    "            value=IMAGE_PATHS_TO_MODEL_OUTPUTS[image_path.numpy()],\n",
    "            dtype=DATA_TYPE_FOR_OUTPUTS\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set: Dataset\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Split the input dataset into a training set and a validation set, both\n",
    "    already divided into mini-batches.\n",
    "    \"\"\"\n",
    "    n_samples_in_validation_set = int(\n",
    "        VALIDATION_SET_PORTION_OF_DATA * N_TRAINING_PLUS_VALIDATION_SAMPLES\n",
    "    )\n",
    "    n_samples_in_training_set = (\n",
    "        N_TRAINING_PLUS_VALIDATION_SAMPLES - n_samples_in_validation_set\n",
    "    )\n",
    "\n",
    "    training_set = (\n",
    "        training_plus_validation_set\n",
    "        # selecting only the training samples and labels:\n",
    "        .take(count=n_samples_in_training_set)\n",
    "        # creating mini-batches:\n",
    "        .batch(\n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            drop_remainder=False,\n",
    "            num_parallel_calls=AUTOTUNE,  # TODO\n",
    "            deterministic=True\n",
    "        )\n",
    "    )\n",
    "    validation_set = (\n",
    "        training_plus_validation_set\n",
    "        # selecting only the validation samples and labels:\n",
    "        .skip(count=n_samples_in_training_set)\n",
    "        .take(count=n_samples_in_validation_set)\n",
    "        # creating mini-batches:\n",
    "        .batch(\n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            drop_remainder=False,\n",
    "            num_parallel_calls=AUTOTUNE,  # TODO\n",
    "            deterministic=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return (training_set, validation_set)\n",
    "\n",
    "\n",
    "def show_dataset_as_movie(\n",
    "        ordered_samples_and_labels: Dataset,\n",
    "        bounding_boxes_or_model_outputs: str = 'bounding_boxes'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Show the dataset images frame by frame, reconstructing the video\n",
    "    sequences, with boundinx boxes contained displayed over the respective\n",
    "    sample/frame.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        bounding_boxes_or_model_outputs in ('bounding_boxes', 'model_outputs')\n",
    "    ), \"Invalid 'bounding_boxes_or_model_outputs' input.\"\n",
    "\n",
    "    _, axes = subplots(1, 1)\n",
    "\n",
    "    # for each sample-label pair, a frame fusing them together is shown:\n",
    "    for index, sample_and_label in enumerate(ordered_samples_and_labels):\n",
    "        if index % 1000 == 0:\n",
    "            print(f\"{index} frames shown\")\n",
    "\n",
    "        # clearing axes from the previous frame information:\n",
    "        axes.clear()\n",
    "\n",
    "        # showing the image:\n",
    "        axes.imshow(sample_and_label[0].numpy())\n",
    "\n",
    "        # showing labels...\n",
    "\n",
    "        # ... either as bounding boxes:\n",
    "        if bounding_boxes_or_model_outputs == 'bounding_boxes':\n",
    "            # for each bounding box:\n",
    "            for bounding_box in sample_and_label[1].numpy().tolist():\n",
    "                # drawing the bounding box over the frame image:\n",
    "                axes.add_patch(\n",
    "                    p=Rectangle(\n",
    "                        xy=(bounding_box[0], bounding_box[1]),\n",
    "                        width=bounding_box[2],\n",
    "                        height=bounding_box[3],\n",
    "                        linewidth=2,\n",
    "                        edgecolor='#00ff00',\n",
    "                        facecolor='none'\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # ... or as model output grid cells:\n",
    "        elif bounding_boxes_or_model_outputs == 'model_outputs':\n",
    "            # for each model output grid cell whose label contains anchors:\n",
    "            for cell_row_index in range(OUTPUT_GRID_N_ROWS):\n",
    "                for cell_column_index in range(OUTPUT_GRID_N_COLUMNS):\n",
    "                    # filtering out grid cells not containing any anchor:\n",
    "                    if (\n",
    "                        sample_and_label[1][\n",
    "                            cell_row_index,\n",
    "                            cell_column_index,\n",
    "                            :,\n",
    "                            :\n",
    "                        ].numpy() == zeros(\n",
    "                            shape=(OUTPUT_GRID_CELL_N_ANCHORS, N_OUTPUTS_PER_ANCHOR)\n",
    "                        )\n",
    "                    ).all():\n",
    "                        continue\n",
    "\n",
    "                    # highlighting the full cell over the frame image:\n",
    "                    axes.add_patch(\n",
    "                        p=Rectangle(\n",
    "                            xy=(\n",
    "                                OUTPUT_GRID_CELL_CORNERS_XY_COORDS[\n",
    "                                    cell_row_index,\n",
    "                                    cell_column_index\n",
    "                                ]\n",
    "                            ),\n",
    "                            width=OUTPUT_GRID_CELL_N_COLUMNS,\n",
    "                            height=OUTPUT_GRID_CELL_N_ROWS,\n",
    "                            linewidth=2,\n",
    "                            edgecolor='#00ff00',\n",
    "                            facecolor='none'\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Ill-conceived code.\")\n",
    "\n",
    "        # making the plot go adeah with the next frame after a small pause for\n",
    "        # better observation:\n",
    "        plt_show(block=False)\n",
    "        plt_pause(interval=0.000001)\n",
    "\n",
    "\n",
    "def turn_bounding_boxes_to_model_outputs(\n",
    "        raw_bounding_boxes: List[Dict[str, int]]\n",
    ") -> Dict[str, List[List[Tuple[int, int, int, int]]]]:\n",
    "    \"\"\"\n",
    "    Turn the input, raw list of bounding boxes' position information into the\n",
    "    equivalent information from the model outputs' perspective, as direct\n",
    "    supervision labels.\n",
    "    \"\"\"\n",
    "    labels = zeros(\n",
    "        shape=(\n",
    "            OUTPUT_GRID_N_ROWS,\n",
    "            OUTPUT_GRID_N_COLUMNS,\n",
    "            OUTPUT_GRID_CELL_N_ANCHORS,\n",
    "            N_OUTPUTS_PER_ANCHOR\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for bounding_box in raw_bounding_boxes:\n",
    "        (\n",
    "            cell_row_index,\n",
    "            cell_column_index,\n",
    "            cell_x_coord,\n",
    "            cell_y_coord\n",
    "        ) = get_cell_containing_bounding_box_center(\n",
    "            center_absolute_x_and_y_coords=(\n",
    "                bounding_box['x'] + (bounding_box['width'] / 2),\n",
    "                bounding_box['y'] + (bounding_box['height'] / 2)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        relative_x_coord = (\n",
    "            (bounding_box['x'] - cell_x_coord) / OUTPUT_GRID_CELL_N_COLUMNS\n",
    "        )\n",
    "        relative_y_coord = (\n",
    "            bounding_box['y'] - cell_y_coord / OUTPUT_GRID_CELL_N_ROWS\n",
    "        )\n",
    "        relative_width = bounding_box['width'] / IMAGE_N_COLUMNS\n",
    "        relative_height = bounding_box['width'] / IMAGE_N_ROWS\n",
    "\n",
    "        # FIXME: introduce anchors' similarity to choose which anchor\n",
    "        label_associated_to_some_anchor = False\n",
    "        for anchor_index in range(OUTPUT_GRID_CELL_N_ANCHORS):\n",
    "            is_this_ancor_already_full = (\n",
    "                labels[cell_row_index, cell_column_index, anchor_index, :] !=\n",
    "                [.0] * N_OUTPUTS_PER_ANCHOR\n",
    "            ).any()\n",
    "            if is_this_ancor_already_full:\n",
    "                continue\n",
    "\n",
    "            labels[cell_row_index, cell_column_index, anchor_index, :] = [\n",
    "                1.0,  # FIXME: is this supposed to be just an objectiveness score or an IoU?\n",
    "                relative_x_coord,\n",
    "                relative_y_coord,\n",
    "                relative_width,\n",
    "                relative_height]\n",
    "\n",
    "            label_associated_to_some_anchor = True\n",
    "            break\n",
    "\n",
    "        if not label_associated_to_some_anchor:\n",
    "            raise Exception(\n",
    "                f\"Either more than {OUTPUT_GRID_CELL_N_ANCHORS} anchors or \" +\n",
    "                \"a better output resolution are required, as more bounding \" +\n",
    "                \"boxes than the set number of anchors are falling within \" +\n",
    "                \"the same output cell in this sample.\"\n",
    "            )\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "(\n",
    "    IMAGE_PATHS_TO_BOUNDING_BOXES,\n",
    "    IMAGE_PATHS_TO_MODEL_OUTPUTS\n",
    ") = load_labels_as_paths_to_bounding_boxes_and_model_outputs_dicts()\n",
    "\n",
    "N_TRAINING_PLUS_VALIDATION_SAMPLES = len(IMAGE_PATHS_TO_BOUNDING_BOXES)\n",
    "\n",
    "\n",
    "# TODO: .cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# TODO: .map() to preprocess samples vs preprocessing layer in the network?\n",
    "# TODO: fix determinism for reproducibility\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if SHOW_BOUNDING_BOXES_STATISTICS:\n",
    "        inspect_bounding_boxes_statistics_on_training_n_validation_set()\n",
    "\n",
    "    samples_n_bounding_boxes_dataset = dataset_of_samples_and_bounding_boxes()\n",
    "\n",
    "    if SHOW_DATASET_MOVIES:\n",
    "        show_dataset_as_movie(\n",
    "            ordered_samples_and_labels=samples_n_bounding_boxes_dataset,\n",
    "            bounding_boxes_or_model_outputs='bounding_boxes'\n",
    "        )\n",
    "\n",
    "    samples_n_model_outputs_dataset = dataset_of_samples_and_model_outputs(\n",
    "        # not shuffling when needing adjacent frames for showing the movie:\n",
    "        shuffle=(not SHOW_DATASET_MOVIES)\n",
    "    )\n",
    "\n",
    "    if SHOW_DATASET_MOVIES:\n",
    "        show_dataset_as_movie(\n",
    "            ordered_samples_and_labels=samples_n_model_outputs_dataset,\n",
    "            bounding_boxes_or_model_outputs='model_outputs'\n",
    "        )\n",
    "\n",
    "    (\n",
    "        training_set, validation_set\n",
    "    ) = split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set=samples_n_model_outputs_dataset\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d99044",
   "metadata": {},
   "source": [
    "#### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model architecture definition.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# pylint: disable=import-error\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Convolution2D,\n",
    "    LeakyReLU,\n",
    "    MaxPooling2D,\n",
    "    Reshape\n",
    ")\n",
    "# pylint: enable=import-error\n",
    "\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from common_constants import (\n",
    "        DOWNSAMPLING_STEPS,\n",
    "        IMAGE_N_CHANNELS,\n",
    "        IMAGE_N_COLUMNS,\n",
    "        IMAGE_N_ROWS,\n",
    "        N_OUTPUTS_PER_ANCHOR,\n",
    "        OUTPUT_GRID_CELL_N_ANCHORS,\n",
    "        OUTPUT_GRID_N_COLUMNS,\n",
    "        OUTPUT_GRID_N_ROWS\n",
    "    )\n",
    "\n",
    "\n",
    "CONVOLUTIONAL_LAYERS_COMMON_KWARGS = {\n",
    "    'kernel_size': (3, 3),\n",
    "    'strides': (1, 1),\n",
    "    'padding': 'same',  # TODO\n",
    "    'data_format': 'channels_last',\n",
    "    'dilation_rate': (1, 1),\n",
    "    'groups': 1,\n",
    "    'activation': None,\n",
    "    'use_bias': True\n",
    "}\n",
    "FIRST_LAYER_N_CONVOLUTIONAL_FILTERS = 64\n",
    "LEAKY_RELU_NEGATIVE_SLOPE = 0.1\n",
    "N_CONVOLUTIONS_AT_SAME_RESOLUTION = 3\n",
    "POOLING_LAYERS_COMMON_KWARGS = {\n",
    "    'pool_size': (2, 2),\n",
    "    'strides': (2, 2),\n",
    "    'padding': 'valid',\n",
    "    'data_format': 'channels_last',\n",
    "}\n",
    "\n",
    "\n",
    "class YOLOv3Variant(Model):  # noqa: E501 pylint: disable=abstract-method, too-many-ancestors\n",
    "    \"\"\"\n",
    "    Customized architecture variant of YOLOv3.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_plus_norm_plus_activation(\n",
    "            n_of_filters: int\n",
    "    ) -> Sequential:\n",
    "        \"\"\"\n",
    "        Return an instance of an enriched convolutional layer block composed,\n",
    "        going from inputs to outputs, of:\n",
    "        - a 2D convolutional layer without any non-linearity;\n",
    "        - a batch-normalization layer;\n",
    "        - a leaky rectified linear unit activation function.\n",
    "        \"\"\"\n",
    "        return Sequential(\n",
    "            [\n",
    "                Convolution2D(\n",
    "                    filters=n_of_filters,\n",
    "                    **CONVOLUTIONAL_LAYERS_COMMON_KWARGS\n",
    "                ),\n",
    "                BatchNormalization(),\n",
    "                LeakyReLU(\n",
    "                    alpha=LEAKY_RELU_NEGATIVE_SLOPE\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_fully_convolutional_yolov3_architecture() -> Model:\n",
    "        \"\"\"\n",
    "        Return an instance of the herein defined YOLOv3 model architecture\n",
    "        that represents its fully-convolutional part, that is excluding\n",
    "        bounding boxes' postprocessing (filtering & aggregation).\n",
    "        \"\"\"\n",
    "        inputs = Input(\n",
    "            shape=(IMAGE_N_ROWS, IMAGE_N_COLUMNS, IMAGE_N_CHANNELS)\n",
    "        )\n",
    "\n",
    "        outputs = inputs\n",
    "        current_n_of_filters = FIRST_LAYER_N_CONVOLUTIONAL_FILTERS\n",
    "\n",
    "        # for each iso-resolution block of convolutional processing ended by a\n",
    "        # downsampling:\n",
    "        for _ in range(DOWNSAMPLING_STEPS):\n",
    "            # for each enriched convolutional layer in the current\n",
    "            # iso-resolution block:\n",
    "            for _ in range(N_CONVOLUTIONS_AT_SAME_RESOLUTION):\n",
    "                outputs = YOLOv3Variant.conv_plus_norm_plus_activation(\n",
    "                    n_of_filters=current_n_of_filters\n",
    "                )(outputs)\n",
    "\n",
    "            # downsampling, ending the iso-resolution block:\n",
    "            outputs = MaxPooling2D(**POOLING_LAYERS_COMMON_KWARGS)(outputs)\n",
    "\n",
    "            # updating the number of filters for the next iso-resolution\n",
    "            # convolutional layers (by doubling them):\n",
    "            current_n_of_filters *= 2\n",
    "\n",
    "        # final 1x1 convolutions to predict bounding boxes' attributes from\n",
    "        # grid anchors' feature maps:\n",
    "        outputs = Convolution2D(\n",
    "            filters=(N_OUTPUTS_PER_ANCHOR * OUTPUT_GRID_CELL_N_ANCHORS),\n",
    "            **(\n",
    "                dict(CONVOLUTIONAL_LAYERS_COMMON_KWARGS, kernel_size=(1, 1))\n",
    "            )\n",
    "        )(outputs)\n",
    "        # NOTE: now bounding boxes' attributes respect the order of meaning\n",
    "        # (object centered probability, x, y, width, height)\n",
    "\n",
    "        # asserting the correctness of the outputs' shape:\n",
    "        assert (\n",
    "            outputs.shape[1:] == (\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                OUTPUT_GRID_CELL_N_ANCHORS * N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "        ), \"Unmatched expectations between outputs and labels shape.\"\n",
    "\n",
    "        # reshaping the last output dimension to split anchors and their\n",
    "        # features along two separate dimensions:\n",
    "        outputs = Reshape(\n",
    "            target_shape=(\n",
    "                OUTPUT_GRID_N_ROWS,\n",
    "                OUTPUT_GRID_N_COLUMNS,\n",
    "                OUTPUT_GRID_CELL_N_ANCHORS,\n",
    "                N_OUTPUTS_PER_ANCHOR\n",
    "            )\n",
    "        )(outputs)\n",
    "\n",
    "        # applying an element-wise sigmoidal activation function as all 5\n",
    "        # bounding boxes' output attributes must belong to [0;1] range,\n",
    "        # since they are either probabilities of a single class (the first\n",
    "        # attribute) or relative coordinates (the second and third one) or\n",
    "        # relative sizes (the fourth and fifth one):\n",
    "        outputs = sigmoid(outputs)\n",
    "        # FIXME: can these sigmoidal computations be carried out together with\n",
    "        # the loss to achieve better gradients during training?\n",
    "\n",
    "        return Model(\n",
    "            inputs=inputs,\n",
    "            outputs=outputs\n",
    "        )\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(YOLOv3Variant, self).__init__()\n",
    "        self.yolov3_fcn = self.build_fully_convolutional_yolov3_architecture()\n",
    "\n",
    "    def call(self, inputs: Tensor, training: bool = False) -> Tensor:  # noqa: E501 pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Forward propagation definition.\n",
    "        \"\"\"\n",
    "        # passing the inputs through the fully-convolutional network:\n",
    "        fcn_outputs = self.yolov3_fcn(\n",
    "            inputs=inputs,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        if not training:\n",
    "            # post-processing the bounding boxes outputs to return only the\n",
    "            # final, filtered and aggregated ones:\n",
    "            raise NotImplementedError\n",
    "            # TODO:\n",
    "            # tf.image.generate_bounding_box_proposals\n",
    "            # tf.image.combined_non_max_suppression\n",
    "            # tf.image.non_max_suppression\n",
    "                # tf.image.non_max_suppression_overlaps\n",
    "                # tf.image.non_max_suppression_padded\n",
    "                # tf.image.non_max_suppression_with_scores\n",
    "\n",
    "        return fcn_outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = YOLOv3Variant()\n",
    "\n",
    "    model.yolov3_fcn.summary()\n",
    "    # TODO: model.plot_model(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1e81a",
   "metadata": {},
   "source": [
    "#### Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definitions of the employed loss function and metrics.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# pylint: disable=import-error\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.math import reduce_sum\n",
    "# pylint: enable=import-error\n",
    "\n",
    "\n",
    "def iou_threshold_averaged_f2_score():\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def yolov3_variant_loss(y_true: Tensor, y_pred: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Loss function minimized to train the defined YOLOv3 variant.\n",
    "    \"\"\"\n",
    "    return reduce_sum((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b15a5",
   "metadata": {},
   "source": [
    "#### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ba7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execution of the defined model training and validation on the respective\n",
    "preprocessed dataset splits, optimizing the defined loss and monitoring the\n",
    "metrics of interest.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# pylint: disable=import-error\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# pylint: enable=import-error\n",
    "\n",
    "if __name__ != 'main_by_mattia':\n",
    "    from loss_and_metrics import (\n",
    "        iou_threshold_averaged_f2_score, yolov3_variant_loss\n",
    "    )\n",
    "    from model_architecture import YOLOv3Variant\n",
    "    from samples_and_labels import (\n",
    "        dataset_of_samples_and_model_outputs,\n",
    "        split_dataset_into_batched_training_and_validation_sets\n",
    "    )\n",
    "\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "N_EPOCHS = 10\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        model_instance: Model,\n",
    "        training_set: Dataset,\n",
    "        validation_set: Dataset\n",
    ") -> str:  # TODO: output dtype\n",
    "    \"\"\"\n",
    "    Compile (in TensorFlow's language acception, i.e. associate optimizer,\n",
    "    loss function and metrics to the model instance) the input model instance\n",
    "    first and then train it on the input dataset.\n",
    "    \"\"\"\n",
    "    model_instance.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=yolov3_variant_loss,\n",
    "        metrics=[]\n",
    "    )\n",
    "\n",
    "    training_history = model_instance.fit(\n",
    "        x=training_set,\n",
    "        epochs=N_EPOCHS,\n",
    "        validation_data=validation_set\n",
    "    )\n",
    "\n",
    "    return training_history\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (\n",
    "        training_samples_and_labels, validation_samples_and_labels\n",
    "    ) = split_dataset_into_batched_training_and_validation_sets(\n",
    "        training_plus_validation_set=dataset_of_samples_and_model_outputs()\n",
    "    )\n",
    "    model = YOLOv3Variant()\n",
    "\n",
    "    training_history = train_model(\n",
    "        model_instance=model,\n",
    "        training_set=training_samples_and_labels,\n",
    "        validation_set=validation_samples_and_labels,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
